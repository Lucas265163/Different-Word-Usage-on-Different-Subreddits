{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d0c47d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading global bigram model from models/bigram/political_bigram_1.phr\n",
      "Loaded 1000000 comments from processed_comments_1/democrats\\democrats_batch1.pkl\n",
      "Loaded 933011 comments from processed_comments_1/democrats\\democrats_batch2.pkl\n",
      "Processing chunk of 1000000 comments for 2021_2024\n",
      "2021_2024 vocabulary size: 28457\n",
      "Processing final 127161 comments for before_2016\n",
      "before_2016 vocabulary size: 11534\n",
      "Processing final 472651 comments for 2017_2020\n",
      "2017_2020 vocabulary size: 19525\n",
      "Processing final 333199 comments for 2021_2024\n",
      "2021_2024 vocabulary size: 28704\n",
      "Model saved to models/chunk_1\n",
      "Completed building models for democrats\n",
      "Loading global bigram model from models/bigram/political_bigram_1.phr\n",
      "Loaded 1000000 comments from processed_comments_1/republican\\republican_batch1.pkl\n",
      "Loaded 290701 comments from processed_comments_1/republican\\republican_batch2.pkl\n",
      "Processing final 263564 comments for before_2016\n",
      "before_2016 vocabulary size: 17932\n",
      "Processing final 414712 comments for 2017_2020\n",
      "2017_2020 vocabulary size: 19067\n",
      "Processing final 612425 comments for 2021_2024\n",
      "2021_2024 vocabulary size: 21754\n",
      "Model saved to models/chunk_1\n",
      "Completed building models for republican\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "def get_date_from_comment(comment):\n",
    "    \"\"\"Extract date from a comment dictionary\"\"\"\n",
    "    try:\n",
    "        return datetime.datetime.strptime(comment[\"date\"], \"%Y-%m-%d\").date()\n",
    "    except (KeyError, ValueError):\n",
    "        try:\n",
    "            return datetime.datetime.fromtimestamp(int(comment[\"timestamp\"])).date()\n",
    "        except (KeyError, ValueError):\n",
    "            return None\n",
    "\n",
    "def get_period(date):\n",
    "    \"\"\"Determine which time period a date belongs to\"\"\"\n",
    "    if date is None:\n",
    "        return None\n",
    "    year = date.year\n",
    "    if year <= 2016:\n",
    "        return \"before_2016\"\n",
    "    elif 2017 <= year <= 2020:\n",
    "        return \"2017_2020\"\n",
    "    elif 2021 <= year <= 2024:\n",
    "        return \"2021_2024\"\n",
    "    return None\n",
    "\n",
    "def build_bigram_model(comments):\n",
    "    \"\"\"Build a bigram model for the given comments\"\"\"\n",
    "    sentences = []\n",
    "    for comment in comments:\n",
    "        if \"processed_text\" in comment:\n",
    "            sentences.append(comment[\"processed_text\"])\n",
    "    phrases = Phrases(sentences, min_count=10, threshold=10)\n",
    "    return Phraser(phrases)\n",
    "\n",
    "def apply_bigrams(comments, bigram_model):\n",
    "    \"\"\"Apply bigram model to comments\"\"\"\n",
    "    processed = []\n",
    "    for comment in comments:\n",
    "        if \"processed_text\" in comment:\n",
    "            processed.append(bigram_model[comment[\"processed_text\"]])\n",
    "    return processed\n",
    "\n",
    "def create_or_update_model(period, comments, vector_size, window, min_count, workers, sg, epochs, existing_model=None):\n",
    "    \"\"\"Create a new model or update an existing one\"\"\"\n",
    "    if existing_model is None:\n",
    "        model = Word2Vec(\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=min_count,\n",
    "            workers=workers,\n",
    "            sg=sg,\n",
    "            seed=23\n",
    "        )\n",
    "        model.build_vocab(comments)\n",
    "        print(f\"{period} vocabulary size: {len(model.wv.index_to_key)}\")\n",
    "    else:\n",
    "        model = existing_model\n",
    "        model.build_vocab(comments, update=True)\n",
    "        print(f\"{period} vocabulary size: {len(model.wv.index_to_key)}\")\n",
    "    model.train(comments, total_examples=len(comments), epochs=epochs)\n",
    "    return model\n",
    "\n",
    "def save_model(model, subreddit, period, model_dir, is_interim=False):\n",
    "    \"\"\"Save model to disk\"\"\"\n",
    "    if is_interim:\n",
    "        path = f\"{model_dir}/interim/{subreddit}_{period}_interim.model\"\n",
    "    else:\n",
    "        path = f\"{model_dir}/{subreddit}_{period}.model\"\n",
    "    model.save(path)\n",
    "\n",
    "def build_models_for_subreddit(\n",
    "    subreddit,\n",
    "    base_data_dir,\n",
    "    model_dir,\n",
    "    vector_size=300,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    epochs=5,\n",
    "    workers=16,\n",
    "    sg=0,\n",
    "    min_comments_to_train=10000,\n",
    "    chunk_size=1000000,\n",
    "    global_bigram_path=None\n",
    "):\n",
    "\n",
    "    time_periods = [\"before_2016\", \"2017_2020\", \"2021_2024\"]\n",
    "    models = {period: None for period in time_periods}\n",
    "    bigram_models = {period: None for period in time_periods}\n",
    "    \n",
    "    # Load global bigram model if exists\n",
    "    global_bigram_path = global_bigram_path\n",
    "    if os.path.exists(global_bigram_path):\n",
    "        print(f\"Loading global bigram model from {global_bigram_path}\")\n",
    "        global_bigram_model = Phraser.load(global_bigram_path)\n",
    "    else:\n",
    "        print(f\"Global bigram model not found at {global_bigram_path}, will train on each chunk.\")\n",
    "        global_bigram_model = None\n",
    "        return\n",
    "\n",
    "    # Find all pickle files\n",
    "    pattern = f\"{base_data_dir}/{subreddit}/{subreddit}_batch*.pkl\"\n",
    "    pickle_files = sorted(glob.glob(pattern))\n",
    "    if not pickle_files:\n",
    "        print(f\"No pickle files found for {subreddit} in {base_data_dir}/{subreddit}/\")\n",
    "        return\n",
    "\n",
    "    comments_by_period = {period: [] for period in time_periods}\n",
    "\n",
    "    for file_path in pickle_files:\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                comments = pickle.load(f)\n",
    "            print(f\"Loaded {len(comments)} comments from {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for comment in comments:\n",
    "            date = get_date_from_comment(comment)\n",
    "            period = get_period(date)\n",
    "            if period:\n",
    "                comments_by_period[period].append(comment)\n",
    "\n",
    "        for period in time_periods:\n",
    "            period_comments = comments_by_period[period]\n",
    "            while len(period_comments) >= chunk_size:\n",
    "                print(f\"Processing chunk of {chunk_size} comments for {period}\")\n",
    "                chunk = period_comments[:chunk_size]\n",
    "                period_comments = period_comments[chunk_size:]\n",
    "\n",
    "                # Use global bigram model if exists, otherwise train on each chunk\n",
    "                if global_bigram_model is not None:\n",
    "                    bigram_model = global_bigram_model\n",
    "                else:\n",
    "                    bigram_model = build_bigram_model(chunk)\n",
    "                bigram_models[period] = bigram_model\n",
    "                processed_chunk = apply_bigrams(chunk, bigram_model)\n",
    "\n",
    "                if len(processed_chunk) > min_comments_to_train:\n",
    "                    model = create_or_update_model(\n",
    "                        period, processed_chunk, vector_size, window, min_count, workers, sg, epochs, models[period]\n",
    "                    )\n",
    "                    models[period] = model\n",
    "                    save_model(model, subreddit, period, model_dir, is_interim=True)\n",
    "            comments_by_period[period] = period_comments\n",
    "\n",
    "    # Process any remaining comments\n",
    "    for period, remaining_comments in comments_by_period.items():\n",
    "        if len(remaining_comments) > min_comments_to_train:\n",
    "            print(f\"Processing final {len(remaining_comments)} comments for {period}\")\n",
    "            if global_bigram_model is not None:\n",
    "                bigram_model = global_bigram_model\n",
    "            else:\n",
    "                bigram_model = build_bigram_model(remaining_comments)\n",
    "            bigram_models[period] = bigram_model\n",
    "            processed_chunk = apply_bigrams(remaining_comments, bigram_model)\n",
    "            model = create_or_update_model(\n",
    "                period, processed_chunk, vector_size, window, min_count, workers, sg, epochs, models[period]\n",
    "            )\n",
    "            models[period] = model\n",
    "            save_model(model, subreddit, period, model_dir, is_interim=False)\n",
    "        else:\n",
    "            print(f\"Skipping final {len(remaining_comments)} comments for {period} (less than minimum required)\")\n",
    "\n",
    "    # Save final models\n",
    "    for period, model in models.items():\n",
    "        if model is not None:\n",
    "            save_model(model, subreddit, period, model_dir, is_interim=False)\n",
    "    print(f\"Model saved to {model_dir}\")\n",
    "    print(f\"Completed building models for {subreddit}\")\n",
    "\n",
    "def main():\n",
    "    model_dir = \"models/chunk_1\"\n",
    "    global_bigram_path = \"models/bigram/political_bigram_1.phr\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(f\"{model_dir}/interim\", exist_ok=True)\n",
    "    random.seed(23)\n",
    "    np.random.seed(23)\n",
    "    subreddits = [\"democrats\", \"republican\"]\n",
    "    for subreddit in subreddits:\n",
    "        build_models_for_subreddit(\n",
    "            subreddit,\n",
    "            base_data_dir=\"processed_comments_1\",\n",
    "            model_dir=model_dir,\n",
    "            vector_size=300,\n",
    "            window=5,\n",
    "            min_count=10,\n",
    "            epochs=5,\n",
    "            workers=16,\n",
    "            sg=0,\n",
    "            min_comments_to_train=10000,\n",
    "            chunk_size=1000000,\n",
    "            global_bigram_path=global_bigram_path\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Changes made:\n",
    "# Using global bigram model, set min_count=10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

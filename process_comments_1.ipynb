{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9a8624a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing subreddit: democrats\n",
      "Processing file: datasets/democrats_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing democrats comments: 1035748it [00:33, 36947.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 1...\n",
      "Saved 1000000 comments to processed_comments_2/democrats/democrats_batch1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing democrats comments: 2011525it [01:02, 32007.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving remaining 941514 comments...\n",
      "Saved 941514 comments to processed_comments_2/democrats/democrats_batch2.pkl\n",
      "\n",
      "Completed processing democrats comments!\n",
      "Total comments saved: 1941514\n",
      "\n",
      "Processing subreddit: republican\n",
      "Processing file: datasets/Republican_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing republican comments: 1071593it [00:33, 37639.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing republican comments: 1082696it [00:36, 8237.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/republican/republican_batch1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing republican comments: 1405486it [00:44, 31463.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving remaining 296609 comments...\n",
      "Saved 296609 comments to processed_comments_2/republican/republican_batch2.pkl\n",
      "\n",
      "Completed processing republican comments!\n",
      "Total comments saved: 1296609\n",
      "\n",
      "Processing subreddit: conservative\n",
      "Processing file: datasets/Conservative_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 1006762it [00:35, 36803.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 1014581it [00:38, 5820.53it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 2017516it [01:12, 33795.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 2...\n",
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch2.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 3027286it [01:46, 38016.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 3031119it [01:48, 4961.89it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch3.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 4035096it [02:19, 31260.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 4040739it [02:22, 5201.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch4.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 5041290it [02:56, 35423.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 5048101it [02:58, 5489.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch5.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 6056257it [03:30, 38948.72it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 6...\n",
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch6.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 7062498it [04:02, 35291.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 7...\n",
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch7.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 8074131it [04:32, 36454.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 8081473it [04:34, 6967.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch8.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 9083618it [05:03, 40876.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 9091379it [05:05, 7210.58it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch9.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 10094490it [05:36, 33497.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 10100945it [05:38, 5879.85it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch10.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 11103686it [06:09, 37609.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 11111381it [06:11, 7087.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch11.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 12109823it [06:41, 37346.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 12...\n",
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch12.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 13115643it [07:14, 38794.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 13...\n",
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch13.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 14120897it [07:44, 51417.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 14...\n",
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch14.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 15125693it [08:14, 45443.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 15134435it [08:16, 8667.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch15.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 16132434it [08:44, 37425.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 16139613it [08:46, 7006.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch16.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 17140972it [09:13, 44113.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 17...\n",
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch17.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 18144421it [09:42, 48903.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 18...\n",
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch18.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 18984143it [10:02, 31485.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving remaining 832387 comments...\n",
      "Saved 832387 comments to processed_comments_2/conservative/conservative_batch19.pkl\n",
      "\n",
      "Completed processing conservative comments!\n",
      "Total comments saved: 18832387\n",
      "\n",
      "Processing subreddit: liberal\n",
      "Processing file: datasets/Liberal_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing liberal comments: 497079it [00:18, 27418.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving remaining 492131 comments...\n",
      "Saved 492131 comments to processed_comments_2/liberal/liberal_batch1.pkl\n",
      "\n",
      "Completed processing liberal comments!\n",
      "Total comments saved: 492131\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "import html\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from fileStreams import getFileJsonStream\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "# Initialize global resources once\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "# POS tag cache to avoid redundant tagging\n",
    "POS_CACHE = {}\n",
    "# Lemma cache to avoid redundant lemmatization\n",
    "LEMMA_CACHE = {}\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Convert NLTK POS tag to WordNet POS tag\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # verb\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'  # noun\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # adverb\n",
    "    else:\n",
    "        return 'n'  # default as noun\n",
    "\n",
    "def preprocess_text(text, lemmatize=True, without_stopwords=True):\n",
    "    \"\"\"Preprocess Reddit text content with optimized NLTK operations\"\"\"\n",
    "    # Handle HTML entities\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Unicode normalization\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Remove URLs and Markdown formatting\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)\n",
    "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)\n",
    "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)\n",
    "    \n",
    "    # Remove subreddit and user references\n",
    "    text = re.sub(r'/r/\\w+', '', text)\n",
    "    text = re.sub(r'r/\\w+', '', text)\n",
    "    text = re.sub(r'/u/\\w+', '', text)\n",
    "    text = re.sub(r'u/\\w+', '', text)\n",
    "    \n",
    "    # Basic text cleaning\n",
    "    text = re.sub(\"[^A-Za-z]+\", ' ', text).lower()\n",
    "    \n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "    \n",
    "    # Lemmatization first\n",
    "    if lemmatize:\n",
    "        # POS tagging (with cache)\n",
    "        uncached_words = [w for w in words if w not in POS_CACHE]\n",
    "        if uncached_words:\n",
    "            tagged_uncached = nltk.pos_tag(uncached_words)\n",
    "            for word, tag in tagged_uncached:\n",
    "                POS_CACHE[word] = tag\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            tag = POS_CACHE[word]\n",
    "            wordnet_pos = get_wordnet_pos(tag)\n",
    "            lemma_key = (word, wordnet_pos)\n",
    "            if lemma_key in LEMMA_CACHE:\n",
    "                lemma = LEMMA_CACHE[lemma_key]\n",
    "            else:\n",
    "                lemma = LEMMATIZER.lemmatize(word, pos=wordnet_pos)\n",
    "                LEMMA_CACHE[lemma_key] = lemma\n",
    "            processed_words.append(lemma)\n",
    "    else:\n",
    "        processed_words = words\n",
    "\n",
    "    # Remove stopwords after lemmatization\n",
    "    if without_stopwords:\n",
    "        processed_words = [w for w in processed_words if w not in STOP_WORDS]\n",
    "\n",
    "    # Remove all words with length <= 2 and > 15\n",
    "    processed_words = [w for w in processed_words if len(w) > 2 and len(w) <= 15]\n",
    "\n",
    "    return processed_words\n",
    "\n",
    "\n",
    "def process_and_save_comments(path, subreddit, output_dir, without_stopwords=True, batch_size=1000000):\n",
    "    \"\"\"Process comments and save in batches\"\"\"\n",
    "    print(f\"Processing file: {path}\")\n",
    "    \n",
    "    # Batch processing counters\n",
    "    batch_count = 0\n",
    "    batch_number = 1\n",
    "    total_count = 0\n",
    "    \n",
    "    # Create data structure for comments\n",
    "    comments_batch = []\n",
    "\n",
    "    with open(path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Unable to read file {path}\")\n",
    "            return\n",
    "        \n",
    "        for row in tqdm(jsonStream, desc=f\"Processing {subreddit} comments\"):\n",
    "            if \"body\" not in row or \"created_utc\" not in row or \"author\" not in row or \"id\" not in row:\n",
    "                continue\n",
    "                \n",
    "            author = row[\"author\"]\n",
    "            if author in {\"AutoModerator\", \"election_info_bot\"}:\n",
    "                continue\n",
    "            \n",
    "            comment_id = row[\"id\"]\n",
    "            text = row[\"body\"]\n",
    "            created_timestamp = row[\"created_utc\"]\n",
    "            date = datetime.datetime.fromtimestamp(int(created_timestamp))\n",
    "            \n",
    "            # Process text with optimized functions\n",
    "            processed_words = preprocess_text(text, lemmatize=True, without_stopwords=without_stopwords)\n",
    "            \n",
    "            if processed_words:\n",
    "                # Save processed comment with metadata\n",
    "                comment_data = {\n",
    "                    \"comment_id\": comment_id,\n",
    "                    \"author\": author,\n",
    "                    \"date\": date.strftime(\"%Y-%m-%d\"),\n",
    "                    \"timestamp\": created_timestamp,\n",
    "                    \"processed_text\": processed_words,  # Original order preserved\n",
    "                    \"original\": text\n",
    "                }\n",
    "                \n",
    "                comments_batch.append(comment_data)\n",
    "                batch_count += 1\n",
    "                \n",
    "            # Check if we need to save the current batch\n",
    "            if batch_count >= batch_size:\n",
    "                print(f\"\\nReached {batch_size} comments, saving batch {batch_number}...\")\n",
    "                \n",
    "                # Save batch directly without filtering\n",
    "                save_path = f\"{output_dir}/{subreddit}_batch{batch_number}.pkl\"\n",
    "                with open(save_path, \"wb\") as out_file:\n",
    "                    pickle.dump(comments_batch, out_file)\n",
    "                \n",
    "                print(f\"Saved {len(comments_batch)} comments to {save_path}\")\n",
    "                \n",
    "                # Reset batch data\n",
    "                comments_batch = []\n",
    "                batch_count = 0\n",
    "                batch_number += 1\n",
    "                total_count += batch_size\n",
    "    \n",
    "    # Process any remaining comments\n",
    "    if batch_count > 0:\n",
    "        print(f\"\\nSaving remaining {batch_count} comments...\")\n",
    "        \n",
    "        # Save batch\n",
    "        save_path = f\"{output_dir}/{subreddit}_batch{batch_number}.pkl\"\n",
    "        with open(save_path, \"wb\") as out_file:\n",
    "            pickle.dump(comments_batch, out_file)\n",
    "        \n",
    "        print(f\"Saved {len(comments_batch)} comments to {save_path}\")\n",
    "        total_count += batch_count\n",
    "    \n",
    "    print(f\"\\nCompleted processing {subreddit} comments!\")\n",
    "    print(f\"Total comments saved: {total_count}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    random.seed(23)\n",
    "    np.random.seed(23)\n",
    "    \n",
    "    # Define data file paths\n",
    "    files = {\n",
    "        \"democrats\": r\"datasets/democrats_comments.zst\",\n",
    "        \"republican\": r\"datasets/Republican_comments.zst\",\n",
    "        \"conservative\": r\"datasets/Conservative_comments.zst\",\n",
    "        \"liberal\": r\"datasets/Liberal_comments.zst\"\n",
    "        # \"vagabond\": r\"datasets/vagabond_comments.zst\",\n",
    "        # \"backpacking\": r\"datasets/backpacking_comments.zst\"\n",
    "        # \"cooking\": r\"datasets/Cooking_comments.zst\",\n",
    "        # \"travel\": r\"datasets/travel_comments.zst\",\n",
    "        # \"books\": r\"datasets/books_comments.zst\",\n",
    "        # \"gaming\": r\"datasets/gaming_comments.zst\",\n",
    "        # \"movies\": r\"datasets/movies_comments.zst\",\n",
    "        # \"technology\": r\"datasets/technology_comments.zst\",\n",
    "        # \"personalfinance\": r\"datasets/personalfinance_comments.zst\"\n",
    "    }\n",
    "    \n",
    "    # List of subreddits to process (process all by default)\n",
    "    subreddits_to_process = list(files.keys())\n",
    "\n",
    "    for subreddit in subreddits_to_process:\n",
    "        output_dir = f\"processed_comments_2/{subreddit}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"\\nProcessing subreddit: {subreddit}\")\n",
    "        process_and_save_comments(\n",
    "            files[subreddit],\n",
    "            subreddit,\n",
    "            output_dir,\n",
    "            without_stopwords=False,\n",
    "            batch_size=1000000\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

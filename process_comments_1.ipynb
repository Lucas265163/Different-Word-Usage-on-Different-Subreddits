{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a8624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "import html\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from fileStreams import getFileJsonStream\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "# Initialize global resources once\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "# POS tag cache to avoid redundant tagging\n",
    "POS_CACHE = {}\n",
    "# Lemma cache to avoid redundant lemmatization\n",
    "LEMMA_CACHE = {}\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Convert NLTK POS tag to WordNet POS tag\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # verb\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'  # noun\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # adverb\n",
    "    else:\n",
    "        return 'n'  # default as noun\n",
    "\n",
    "def preprocess_text(text, lemmatize=True, without_stopwords=True):\n",
    "    \"\"\"Preprocess Reddit text content with optimized NLTK operations\"\"\"\n",
    "    # Handle HTML entities\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Unicode normalization\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Remove URLs and Markdown formatting\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)\n",
    "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)\n",
    "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)\n",
    "    \n",
    "    # Remove subreddit and user references\n",
    "    text = re.sub(r'/r/\\w+', '', text)\n",
    "    text = re.sub(r'r/\\w+', '', text)\n",
    "    text = re.sub(r'/u/\\w+', '', text)\n",
    "    text = re.sub(r'u/\\w+', '', text)\n",
    "    \n",
    "    # Basic text cleaning\n",
    "    text = re.sub(\"[^A-Za-z]+\", ' ', text).lower()\n",
    "    \n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "    \n",
    "    # Lemmatization first\n",
    "    if lemmatize:\n",
    "        # POS tagging (with cache)\n",
    "        uncached_words = [w for w in words if w not in POS_CACHE]\n",
    "        if uncached_words:\n",
    "            tagged_uncached = nltk.pos_tag(uncached_words)\n",
    "            for word, tag in tagged_uncached:\n",
    "                POS_CACHE[word] = tag\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            tag = POS_CACHE[word]\n",
    "            wordnet_pos = get_wordnet_pos(tag)\n",
    "            lemma_key = (word, wordnet_pos)\n",
    "            if lemma_key in LEMMA_CACHE:\n",
    "                lemma = LEMMA_CACHE[lemma_key]\n",
    "            else:\n",
    "                lemma = LEMMATIZER.lemmatize(word, pos=wordnet_pos)\n",
    "                LEMMA_CACHE[lemma_key] = lemma\n",
    "            processed_words.append(lemma)\n",
    "    else:\n",
    "        processed_words = words\n",
    "\n",
    "    # Remove stopwords after lemmatization\n",
    "    if without_stopwords:\n",
    "        processed_words = [w for w in processed_words if w not in STOP_WORDS]\n",
    "\n",
    "    # Remove all words with length <= 2\n",
    "    processed_words = [w for w in processed_words if len(w) > 2]\n",
    "\n",
    "    return processed_words\n",
    "\n",
    "\n",
    "def process_and_save_comments(path, subreddit, output_dir, without_stopwords=True, batch_size=1000000):\n",
    "    \"\"\"Process comments and save in batches\"\"\"\n",
    "    print(f\"Processing file: {path}\")\n",
    "    \n",
    "    # Batch processing counters\n",
    "    batch_count = 0\n",
    "    batch_number = 1\n",
    "    total_count = 0\n",
    "    \n",
    "    # Create data structure for comments\n",
    "    comments_batch = []\n",
    "\n",
    "    with open(path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Unable to read file {path}\")\n",
    "            return\n",
    "        \n",
    "        for row in tqdm(jsonStream, desc=f\"Processing {subreddit} comments\"):\n",
    "            if \"body\" not in row or \"created_utc\" not in row or \"author\" not in row or \"id\" not in row:\n",
    "                continue\n",
    "                \n",
    "            author = row[\"author\"]\n",
    "            if author in {\"AutoModerator\", \"election_info_bot\"}:\n",
    "                continue\n",
    "            \n",
    "            comment_id = row[\"id\"]\n",
    "            text = row[\"body\"]\n",
    "            created_timestamp = row[\"created_utc\"]\n",
    "            date = datetime.datetime.fromtimestamp(int(created_timestamp))\n",
    "            \n",
    "            # Process text with optimized functions\n",
    "            processed_words = preprocess_text(text, lemmatize=True, without_stopwords=without_stopwords)\n",
    "            \n",
    "            if processed_words:\n",
    "                # Save processed comment with metadata\n",
    "                comment_data = {\n",
    "                    \"comment_id\": comment_id,\n",
    "                    \"author\": author,\n",
    "                    \"date\": date.strftime(\"%Y-%m-%d\"),\n",
    "                    \"timestamp\": created_timestamp,\n",
    "                    \"processed_text\": processed_words,  # Original order preserved\n",
    "                    \"original\": text\n",
    "                }\n",
    "                \n",
    "                comments_batch.append(comment_data)\n",
    "                batch_count += 1\n",
    "                \n",
    "            # Check if we need to save the current batch\n",
    "            if batch_count >= batch_size:\n",
    "                print(f\"\\nReached {batch_size} comments, saving batch {batch_number}...\")\n",
    "                \n",
    "                # Save batch directly without filtering\n",
    "                save_path = f\"{output_dir}/{subreddit}_batch{batch_number}.pkl\"\n",
    "                with open(save_path, \"wb\") as out_file:\n",
    "                    pickle.dump(comments_batch, out_file)\n",
    "                \n",
    "                print(f\"Saved {len(comments_batch)} comments to {save_path}\")\n",
    "                \n",
    "                # Reset batch data\n",
    "                comments_batch = []\n",
    "                batch_count = 0\n",
    "                batch_number += 1\n",
    "                total_count += batch_size\n",
    "    \n",
    "    # Process any remaining comments\n",
    "    if batch_count > 0:\n",
    "        print(f\"\\nSaving remaining {batch_count} comments...\")\n",
    "        \n",
    "        # Save batch\n",
    "        save_path = f\"{output_dir}/{subreddit}_batch{batch_number}.pkl\"\n",
    "        with open(save_path, \"wb\") as out_file:\n",
    "            pickle.dump(comments_batch, out_file)\n",
    "        \n",
    "        print(f\"Saved {len(comments_batch)} comments to {save_path}\")\n",
    "        total_count += batch_count\n",
    "    \n",
    "    print(f\"\\nCompleted processing {subreddit} comments!\")\n",
    "    print(f\"Total comments saved: {total_count}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    random.seed(23)\n",
    "    np.random.seed(23)\n",
    "    \n",
    "    # Define data file paths\n",
    "    files = {\n",
    "        \"democrats\": r\"datasets/democrats_comments.zst\",\n",
    "        \"republican\": r\"datasets/Republican_comments.zst\",\n",
    "        \"conservative\": r\"datasets/Conservative_comments.zst\",\n",
    "        \"liberal\": r\"datasets/Liberal_comments.zst\",\n",
    "        \"vagabond\": r\"datasets/vagabond_comments.zst\",\n",
    "        \"backpacking\": r\"datasets/backpacking_comments.zst\"\n",
    "    }\n",
    "    \n",
    "    # List of subreddits to process (process all by default)\n",
    "    subreddits_to_process = list(files.keys())\n",
    "\n",
    "    for subreddit in subreddits_to_process:\n",
    "        output_dir = f\"processed_comments_1/{subreddit}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"\\nProcessing subreddit: {subreddit}\")\n",
    "        process_and_save_comments(\n",
    "            files[subreddit],\n",
    "            subreddit,\n",
    "            output_dir,\n",
    "            without_stopwords=True,\n",
    "            batch_size=1000000\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c8e6006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for 50 words from dem_10_10_list.csv\n",
      "Words to search for: ['j_fzim', 'vobu_q', 'aug_pm', 'pm_edt', 'loosen_arctic', 'drilling_restriction', 'webster_bbc', 'ukraine_merriam', 'catherine_cortez', 'masto', 'aug_pm', 'pm_pm', 'pm_edt', 'pm_pm', 'aldarondo', 'misla', 'lukens', 'buz', 'immoral_mob', 'insecure_pathological', 'dr_martin', 'luther_king', 'fri_aug', 'edt_tue', 'fri_aug', 'pm_pm', 'jerry_moran', 'john_hoeven', 'j_fzim', 'xt', 'o_bstruct', 'p_roject', 'agedlikemilk', 'noshitsherlock', 'noshitsherlock', 'lostredditors', 'paywall_workaround', 'firewall_workaround', 'ecuador_indict', 'alleged_bribery', 'cynthia', 'lummis', 'p_roject', 'g_aslight', 'climate_resiliency', 'coastal_resiliency', 'vobu_q', 'xt', 'edt_tue', 'pm_pm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments: 1670395it [13:06, 2124.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comments Containing Target Words ===\n",
      "\n",
      "\n",
      "## Word: j_fzim ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: ![gif](giphy|J8FZIm9VoBU6Q)\n",
      "Processed: gif giphy j fzim vobu q\n",
      "Author: fatjumboshrimp\n",
      "\n",
      "Example 2:\n",
      "Original: ![gif](giphy|J8FZIm9VoBU6Q)\n",
      "Processed: gif giphy j fzim vobu q\n",
      "Author: fatjumboshrimp\n",
      "\n",
      "Example 3:\n",
      "Original: ![gif](giphy|J8FZIm9VoBU6Q)\n",
      "Processed: gif giphy j fzim vobu q\n",
      "Author: NemoLeeGreen\n",
      "\n",
      "## Word: vobu_q ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: ![gif](giphy|J8FZIm9VoBU6Q)\n",
      "Processed: gif giphy j fzim vobu q\n",
      "Author: fatjumboshrimp\n",
      "\n",
      "Example 2:\n",
      "Original: ![gif](giphy|J8FZIm9VoBU6Q)\n",
      "Processed: gif giphy j fzim vobu q\n",
      "Author: fatjumboshrimp\n",
      "\n",
      "Example 3:\n",
      "Original: ![gif](giphy|J8FZIm9VoBU6Q)\n",
      "Processed: gif giphy j fzim vobu q\n",
      "Author: NemoLeeGreen\n",
      "\n",
      "## Word: aug_pm ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: What is so stunning is that in violence we may see it end in a slaughter and just keep on trying. This is a faith element in some way. Such faith is being demonstrated in the two wars right now. Furth...\n",
      "Processed: what be so stunning be that in violence we may see it end in a slaughter and just keep on try this be a faith element in some way such faith be be demonstrate in the two war right now far to understan...\n",
      "Author: ravia\n",
      "\n",
      "Example 2:\n",
      "Original: What is so stunning is that in violence we may see it end in a slaughter and just keep on trying. This is a faith element in some way. Such faith is being demonstrated in the two wars right now. Furth...\n",
      "Processed: what be so stunning be that in violence we may see it end in a slaughter and just keep on try this be a faith element in some way such faith be be demonstrate in the two war right now far to understan...\n",
      "Author: ravia\n",
      "\n",
      "Example 3:\n",
      "Original: This video is a ridiculous attempt at a smear job from immediately after the election in 2008. \n",
      "\n",
      "\n",
      "&gt;1) I opposed this war *(the Iraq war)* from the beginning, and as President, I will end it\n",
      "\n",
      "&gt;I ...\n",
      "Processed: this video be a ridiculous attempt at a smear job from immediately after the election in gt i oppose this war the iraq war from the beginning and a president i will end it gt i will make the fight aga...\n",
      "Author: zilacove\n",
      "\n",
      "## Word: pm_edt ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: Unfortunately before the Clinton winning debate, 538 of course: \n",
      "\n",
      "UPDATED 5:15 PM EDT | APR 15, 2016\n",
      "\n",
      "According to our latest polls-plus forecast, Hillary Clinton has a 99% chance of winning the New Y...\n",
      "Processed: unfortunately before the clinton win debate of course update pm edt apr accord to our late poll plus forecast hillary clinton have a chance of win the new york primary clinton sander if you believe in...\n",
      "Author: michaelconfoy\n",
      "\n",
      "Example 2:\n",
      "Original: Unfortunately before the Clinton winning debate, 538 of course: \n",
      "\n",
      "UPDATED 5:15 PM EDT | APR 15, 2016\n",
      "\n",
      "According to our latest polls-plus forecast, Hillary Clinton has a 99% chance of winning the New Y...\n",
      "Processed: unfortunately before the clinton win debate of course update pm edt apr accord to our late poll plus forecast hillary clinton have a chance of win the new york primary clinton sander if you believe in...\n",
      "Author: michaelconfoy\n",
      "\n",
      "Example 3:\n",
      "Original: Baby Donald. It's the type of diminutive term that gets under his thin skin, and it is easy to use for snarky, low-effort retorts to anything he does in a way he can't defend against. E.g. saying some...\n",
      "Processed: baby donald it s the type of diminutive term that get under his thin skin and it be easy to use for snarky low effort retort to anything he do in a way he can t defend against e g say something like l...\n",
      "Author: sociotronics\n",
      "\n",
      "## Word: loosen_arctic ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: 40. Proposed opening most of America’s coastal waters to offshore oil and gas drilling but delayed the plan after a federal judge ruled that Mr. Trump’s reversal of an Obama-era ban on drilling in the...\n",
      "Processed: propose open most of america s coastal water to offshore oil and gas drilling but delay the plan after a federal judge rule that mr trump s reversal of an obama era ban on drilling in the atlantic and...\n",
      "Author: infinitywee\n",
      "\n",
      "Example 2:\n",
      "Original: Shapiro is a worthless hack.\n",
      "\n",
      "-----------------\n",
      "\n",
      "* Restored daily press briefings\n",
      "\n",
      "* Cancel Keystone Pipeline\n",
      "\n",
      "* Reverse Trump's Muslim ban\n",
      "\n",
      "* Require masks on federal property\n",
      "\n",
      "* Rejoin the Paris Cli...\n",
      "Processed: shapiro be a worthless hack restore daily press briefing cancel keystone pipeline reverse trump s muslim ban require mask on federal property rejoin the paris climate agreement extend student loan pay...\n",
      "Author: backpackwayne\n",
      "\n",
      "Example 3:\n",
      "Original: * Restored daily press briefings\n",
      "\n",
      "* Cancel Keystone Pipeline\n",
      "\n",
      "* Reverse Trump's Muslim ban\n",
      "\n",
      "* Require masks on federal property\n",
      "\n",
      "* Rejoin the Paris Climate agreement\n",
      "\n",
      "* Extend Student Loan payment fre...\n",
      "Processed: restore daily press briefing cancel keystone pipeline reverse trump s muslim ban require mask on federal property rejoin the paris climate agreement extend student loan payment freeze extend eviction ...\n",
      "Author: backpackwayne\n",
      "\n",
      "## Word: drilling_restriction ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: Just a few things here and there:\n",
      "\n",
      "He was handed the biggest mess since the great depression and accomplished the following all in the face of the most obstructive congress in history:\n",
      "\n",
      "•\t**Economy  g...\n",
      "Processed: just a few thing here and there he be hand the big mess since the great depression and accomplish the follow all in the face of the most obstructive congress in history economy grow in the first quart...\n",
      "Author: backpackwayne\n",
      "\n",
      "Example 2:\n",
      "Original: Old post, that would be because of restrictions on off-shore drilling and the pipeline, which would have helped current prices further.  \n",
      "Processed: old post that would be because of restriction on off shore drilling and the pipeline which would have help current price far\n",
      "Author: [deleted]\n",
      "\n",
      "Example 3:\n",
      "Original: #Not at all:\n",
      "\n",
      "•\t**Handed biggest mess since the great depression and accomplished the following all in the face of the most obstructive congress in history:**\n",
      "\n",
      "•\t**Outperformed Reagan on jobs, growth ...\n",
      "Processed: not at all hand big mess since the great depression and accomplish the follow all in the face of the most obstructive congress in history outperform reagan on job growth and invest straight month and ...\n",
      "Author: backpackwayne\n",
      "\n",
      "## Word: webster_bbc ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: It's 'Ukraine' and not 'the Ukraine'\n",
      "\n",
      "[[Merriam-Webster](https://www.merriam-webster.com/dictionary/Ukraine)] [[BBC Styleguide](https://www.bbc.co.uk/newsstyleguide/u)] [[Reuters Styleguide](https://h...\n",
      "Processed: it s ukraine and not the ukraine merriam webster bbc styleguide reuters styleguide beep boop i m a bot\n",
      "Author: UkraineWithoutTheBot\n",
      "\n",
      "Example 2:\n",
      "Original: It's 'Ukraine' and not 'the Ukraine'\n",
      "\n",
      "[[Merriam-Webster](https://www.merriam-webster.com/dictionary/Ukraine)] [[BBC Styleguide](https://www.bbc.co.uk/newsstyleguide/u)] [[Reuters Styleguide](https://h...\n",
      "Processed: it s ukraine and not the ukraine merriam webster bbc styleguide reuters styleguide beep boop i m a bot\n",
      "Author: UkraineWithoutTheBot\n",
      "\n",
      "Example 3:\n",
      "Original: It's 'Ukraine' and not 'the Ukraine'\n",
      "\n",
      "[[Merriam-Webster](https://www.merriam-webster.com/dictionary/Ukraine)] [[BBC Styleguide](https://www.bbc.co.uk/newsstyleguide/u)] [[Reuters Styleguide](https://h...\n",
      "Processed: it s ukraine and not the ukraine merriam webster bbc styleguide reuters styleguide beep boop i m a bot\n",
      "Author: UkraineWithoutTheBot\n",
      "\n",
      "## Word: ukraine_merriam ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: It's 'Ukraine' and not 'the Ukraine'\n",
      "\n",
      "[[Merriam-Webster](https://www.merriam-webster.com/dictionary/Ukraine)] [[BBC Styleguide](https://www.bbc.co.uk/newsstyleguide/u)] [[Reuters Styleguide](https://h...\n",
      "Processed: it s ukraine and not the ukraine merriam webster bbc styleguide reuters styleguide beep boop i m a bot\n",
      "Author: UkraineWithoutTheBot\n",
      "\n",
      "Example 2:\n",
      "Original: It's 'Ukraine' and not 'the Ukraine'\n",
      "\n",
      "[[Merriam-Webster](https://www.merriam-webster.com/dictionary/Ukraine)] [[BBC Styleguide](https://www.bbc.co.uk/newsstyleguide/u)] [[Reuters Styleguide](https://h...\n",
      "Processed: it s ukraine and not the ukraine merriam webster bbc styleguide reuters styleguide beep boop i m a bot\n",
      "Author: UkraineWithoutTheBot\n",
      "\n",
      "Example 3:\n",
      "Original: It's 'Ukraine' and not 'the Ukraine'\n",
      "\n",
      "[[Merriam-Webster](https://www.merriam-webster.com/dictionary/Ukraine)] [[BBC Styleguide](https://www.bbc.co.uk/newsstyleguide/u)] [[Reuters Styleguide](https://h...\n",
      "Processed: it s ukraine and not the ukraine merriam webster bbc styleguide reuters styleguide beep boop i m a bot\n",
      "Author: UkraineWithoutTheBot\n",
      "\n",
      "## Word: catherine_cortez ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: If this holds, four of the five new Democratic senators elected last night will be women: Kamala Harris in California; Catherine Cortez Masto in Nevada; Tammy Duckworth in Illinois; and Maggie Hassan ...\n",
      "Processed: if this hold four of the five new democratic senator elect last night will be woman kamala harris in california catherine cortez masto in nevada tammy duckworth in illinois and maggie hassan in new ha...\n",
      "Author: wenchette\n",
      "\n",
      "Example 2:\n",
      "Original: She's not weak. She survived!\n",
      "\n",
      "I personally don't see her as weak. My issue is that there are some voters that would equate it with weakness. Other then that she's very personable and charismatic. She...\n",
      "Processed: she s not weak she survive i personally don t see her a weak my issue be that there be some voter that would equate it with weakness other then that she s very personable and charismatic she would be ...\n",
      "Author: [deleted]\n",
      "\n",
      "Example 3:\n",
      "Original: I don’t agree with the premise Joe Biden’s campaign needs ‘saving’ but I think Catherine Cortez Masto looks great. I think VP should be a woman of colour. Black or Latino works for me. Democrat voters...\n",
      "Processed: i don t agree with the premise joe biden s campaign need save but i think catherine cortez masto look great i think vp should be a woman of colour black or latino work for me democrat voter have a his...\n",
      "Author: elisart\n",
      "\n",
      "## Word: masto ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: **1. Illinois — GOP Sen. Mark Kirk is running for reelection (Previous ranking: 1)**\n",
      "\n",
      "Would be nice to get some polling. Anyone local with insight?\n",
      "\n",
      "**2. Wisconsin — GOP Sen. Ron Johnson is running fo...\n",
      "Processed: illinois gop sen mark kirk be run for reelection previous ranking would be nice to get some poll anyone local with insight wisconsin gop sen ron johnson be run for reelection rj super genius be do it ...\n",
      "Author: michaelconfoy\n",
      "\n",
      "Example 2:\n",
      "Original: If this holds, four of the five new Democratic senators elected last night will be women: Kamala Harris in California; Catherine Cortez Masto in Nevada; Tammy Duckworth in Illinois; and Maggie Hassan ...\n",
      "Processed: if this hold four of the five new democratic senator elect last night will be woman kamala harris in california catherine cortez masto in nevada tammy duckworth in illinois and maggie hassan in new ha...\n",
      "Author: wenchette\n",
      "\n",
      "Example 3:\n",
      "Original: As a Nevadan, I sure as hell won't. Cortez Masto couldn't come sooner. \n",
      "Processed: a a nevadan i sure a hell win t cortez masto couldn t come sooner\n",
      "Author: GroriousNipponSteer\n",
      "\n",
      "## Word: aug_pm ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: What is so stunning is that in violence we may see it end in a slaughter and just keep on trying. This is a faith element in some way. Such faith is being demonstrated in the two wars right now. Furth...\n",
      "Processed: what be so stunning be that in violence we may see it end in a slaughter and just keep on try this be a faith element in some way such faith be be demonstrate in the two war right now far to understan...\n",
      "Author: ravia\n",
      "\n",
      "Example 2:\n",
      "Original: What is so stunning is that in violence we may see it end in a slaughter and just keep on trying. This is a faith element in some way. Such faith is being demonstrated in the two wars right now. Furth...\n",
      "Processed: what be so stunning be that in violence we may see it end in a slaughter and just keep on try this be a faith element in some way such faith be be demonstrate in the two war right now far to understan...\n",
      "Author: ravia\n",
      "\n",
      "Example 3:\n",
      "Original: This video is a ridiculous attempt at a smear job from immediately after the election in 2008. \n",
      "\n",
      "\n",
      "&gt;1) I opposed this war *(the Iraq war)* from the beginning, and as President, I will end it\n",
      "\n",
      "&gt;I ...\n",
      "Processed: this video be a ridiculous attempt at a smear job from immediately after the election in gt i oppose this war the iraq war from the beginning and a president i will end it gt i will make the fight aga...\n",
      "Author: zilacove\n",
      "\n",
      "## Word: pm_pm ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: This is regarding the latest developments in the Wisconsin protests and the senator walkout to prevent quorum on this \"budget\" bill which strips collective bargaining for workers:\n",
      "\n",
      "*\"\"We're going to b...\n",
      "Processed: this be regard the late development in the wisconsin protest and the senator walkout to prevent quorum on this budget bill which strip collective bargaining for worker we re go to be stay away until w...\n",
      "Author: tob_krean\n",
      "\n",
      "Example 2:\n",
      "Original: This is regarding the latest developments in the Wisconsin protests and the senator walkout to prevent quorum on this \"budget\" bill which strips collective bargaining for workers:\n",
      "\n",
      "*\"\"We're going to b...\n",
      "Processed: this be regard the late development in the wisconsin protest and the senator walkout to prevent quorum on this budget bill which strip collective bargaining for worker we re go to be stay away until w...\n",
      "Author: tob_krean\n",
      "\n",
      "Example 3:\n",
      "Original: This is regarding the latest developments in the Wisconsin protests and the senator walkout to prevent quorum on this \"budget\" bill which strips collective bargaining for workers:\n",
      "\n",
      "*\"\"We're going to b...\n",
      "Processed: this be regard the late development in the wisconsin protest and the senator walkout to prevent quorum on this budget bill which strip collective bargaining for worker we re go to be stay away until w...\n",
      "Author: tob_krean\n",
      "\n",
      "## Word: pm_edt ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: Unfortunately before the Clinton winning debate, 538 of course: \n",
      "\n",
      "UPDATED 5:15 PM EDT | APR 15, 2016\n",
      "\n",
      "According to our latest polls-plus forecast, Hillary Clinton has a 99% chance of winning the New Y...\n",
      "Processed: unfortunately before the clinton win debate of course update pm edt apr accord to our late poll plus forecast hillary clinton have a chance of win the new york primary clinton sander if you believe in...\n",
      "Author: michaelconfoy\n",
      "\n",
      "Example 2:\n",
      "Original: Unfortunately before the Clinton winning debate, 538 of course: \n",
      "\n",
      "UPDATED 5:15 PM EDT | APR 15, 2016\n",
      "\n",
      "According to our latest polls-plus forecast, Hillary Clinton has a 99% chance of winning the New Y...\n",
      "Processed: unfortunately before the clinton win debate of course update pm edt apr accord to our late poll plus forecast hillary clinton have a chance of win the new york primary clinton sander if you believe in...\n",
      "Author: michaelconfoy\n",
      "\n",
      "Example 3:\n",
      "Original: Baby Donald. It's the type of diminutive term that gets under his thin skin, and it is easy to use for snarky, low-effort retorts to anything he does in a way he can't defend against. E.g. saying some...\n",
      "Processed: baby donald it s the type of diminutive term that get under his thin skin and it be easy to use for snarky low effort retort to anything he do in a way he can t defend against e g say something like l...\n",
      "Author: sociotronics\n",
      "\n",
      "## Word: pm_pm ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: This is regarding the latest developments in the Wisconsin protests and the senator walkout to prevent quorum on this \"budget\" bill which strips collective bargaining for workers:\n",
      "\n",
      "*\"\"We're going to b...\n",
      "Processed: this be regard the late development in the wisconsin protest and the senator walkout to prevent quorum on this budget bill which strip collective bargaining for worker we re go to be stay away until w...\n",
      "Author: tob_krean\n",
      "\n",
      "Example 2:\n",
      "Original: This is regarding the latest developments in the Wisconsin protests and the senator walkout to prevent quorum on this \"budget\" bill which strips collective bargaining for workers:\n",
      "\n",
      "*\"\"We're going to b...\n",
      "Processed: this be regard the late development in the wisconsin protest and the senator walkout to prevent quorum on this budget bill which strip collective bargaining for worker we re go to be stay away until w...\n",
      "Author: tob_krean\n",
      "\n",
      "Example 3:\n",
      "Original: This is regarding the latest developments in the Wisconsin protests and the senator walkout to prevent quorum on this \"budget\" bill which strips collective bargaining for workers:\n",
      "\n",
      "*\"\"We're going to b...\n",
      "Processed: this be regard the late development in the wisconsin protest and the senator walkout to prevent quorum on this budget bill which strip collective bargaining for worker we re go to be stay away until w...\n",
      "Author: tob_krean\n",
      "\n",
      "## Word: aldarondo ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: * Republican anti-abortion activist Howard Scott Heldreth is a convicted child rapist in Florida.\n",
      "* Republican County Commissioner David Swartz pleaded guilty to molesting two girls under the age of 1...\n",
      "Processed: republican anti abortion activist howard scott heldreth be a convicted child rapist in florida republican county commissioner david swartz plead guilty to molest two girl under the age of and be sente...\n",
      "Author: dingoselfies\n",
      "\n",
      "Example 2:\n",
      "Original: Please, tell me again how Kavanaugh, Moore, and Trump hold the moral high ground, and are perfectly suited for office?\n",
      "\n",
      "Or how about these fine Republicans?\n",
      "\n",
      "TL;DR: Clean up your own glass house befor...\n",
      "Processed: please tell me again how kavanaugh moore and trump hold the moral high ground and be perfectly suit for office or how about these fine republican tl dr clean up your own glass house before throw stone...\n",
      "Author: PraxisLD\n",
      "\n",
      "Example 3:\n",
      "Original:  Republican anti-abortion activist Howard Scott Heldreth is a convicted child rapist in Florida.\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      " Republican County Commissioner David Swartz pleaded guilty to molesting two girls under the age ...\n",
      "Processed: republican anti abortion activist howard scott heldreth be a convicted child rapist in florida republican county commissioner david swartz plead guilty to molest two girl under the age of and be sente...\n",
      "Author: dingoselfies\n",
      "\n",
      "## Word: misla ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: I do not approve.  When you haphazardly toss the word \"terrorist\" around it begins to lose its meaning.  A lot of different people already can be labeled  \"terrorists\" in order to strengthen an argume...\n",
      "Processed: i do not approve when you haphazardly toss the word terrorist around it begin to lose it mean a lot of different people already can be label terrorist in order to strengthen an argument for an end be ...\n",
      "Author: otter111a\n",
      "\n",
      "Example 2:\n",
      "Original: So literally the only difference is a full auto mode?  People get way too outraged over the mislabeling of the AR15 if that's the only difference.\n",
      "Processed: so literally the only difference be a full auto mode people get way too outrage over the mislabeling of the ar if that s the only difference\n",
      "Author: organoleptic-leper\n",
      "\n",
      "Example 3:\n",
      "Original: Mislabeling racism and divisive politics as populism is not a good thing.  It's Orwellian.  FDR and MLK were populists.\n",
      "Processed: mislabeling racism and divisive politics a populism be not a good thing it s orwellian fdr and mlk be populist\n",
      "Author: KubrickIsMyCopilot\n",
      "\n",
      "## Word: lukens ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: * Republican anti-abortion activist Howard Scott Heldreth is a convicted child rapist in Florida.\n",
      "* Republican County Commissioner David Swartz pleaded guilty to molesting two girls under the age of 1...\n",
      "Processed: republican anti abortion activist howard scott heldreth be a convicted child rapist in florida republican county commissioner david swartz plead guilty to molest two girl under the age of and be sente...\n",
      "Author: dingoselfies\n",
      "\n",
      "Example 2:\n",
      "Original: Please, tell me again how Kavanaugh, Moore, and Trump hold the moral high ground, and are perfectly suited for office?\n",
      "\n",
      "Or how about these fine Republicans?\n",
      "\n",
      "TL;DR: Clean up your own glass house befor...\n",
      "Processed: please tell me again how kavanaugh moore and trump hold the moral high ground and be perfectly suit for office or how about these fine republican tl dr clean up your own glass house before throw stone...\n",
      "Author: PraxisLD\n",
      "\n",
      "Example 3:\n",
      "Original:  Republican anti-abortion activist Howard Scott Heldreth is a convicted child rapist in Florida.\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      " Republican County Commissioner David Swartz pleaded guilty to molesting two girls under the age ...\n",
      "Processed: republican anti abortion activist howard scott heldreth be a convicted child rapist in florida republican county commissioner david swartz plead guilty to molest two girl under the age of and be sente...\n",
      "Author: dingoselfies\n",
      "\n",
      "## Word: buz ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: Same money - kochs et al - he had in the first place. It is their agenda on the line and he is the wisconsin face of it. The fact that the legislative recall earlier this year failed to net more than ...\n",
      "Processed: same money koch et al he have in the first place it be their agenda on the line and he be the wisconsin face of it the fact that the legislative recall earlier this year fail to net more than two of t...\n",
      "Author: [deleted]\n",
      "\n",
      "Example 2:\n",
      "Original: Not to be a buzzkill, but isn't the Wisconsin recall effort looking [REALLY bleak?](http://www.washingtonpost.com/blogs/the-fix/post/scott-walker-leads-in-wisconsin-recall-poll/2012/01/25/gIQAbtkfQQ_b...\n",
      "Processed: not to be a buzzkill but isn t the wisconsin recall effort look really bleak walker be already win before a dem have even get the nomination i m not sure i like our odds once you consider that anger t...\n",
      "Author: silverpaw1786\n",
      "\n",
      "Example 3:\n",
      "Original: People are angry but they don't know why they are angry. They hear buzz words and loose concepts of the issues at hand and regurgitate it without any understanding. This is how politicians get elected...\n",
      "Processed: people be angry but they don t know why they be angry they hear buzz word and loose concept of the issue at hand and regurgitate it without any understanding this be how politician get elect base on s...\n",
      "Author: revolvingdoor\n",
      "\n",
      "## Word: immoral_mob ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: Dude I honestly don’t know who you’re talking to that is demonizing you that way for owning a firearm. That sort of fascistic adherence to group think is antithetical to liberalism, imo. I have to ass...\n",
      "Processed: dude i honestly don t know who you re talk to that be demonize you that way for own a firearm that sort of fascistic adherence to group think be antithetical to liberalism imo i have to assume that it...\n",
      "Author: pops_secret\n",
      "\n",
      "Example 2:\n",
      "Original: King Donald is a comedy of errors. Evil intent with no fear of consequence. A gang of faithful, loyal, and criminal idiots orchestrated by an insecure, pathological, and immoral mob boss. If there are...\n",
      "Processed: king donald be a comedy of error evil intent with no fear of consequence a gang of faithful loyal and criminal idiot orchestrate by an insecure pathological and immoral mob bos if there be some decent...\n",
      "Author: PeteLarsen\n",
      "\n",
      "Example 3:\n",
      "Original: Donnie is a comedy of errors. Evil intent with no fear of consequence. A gang of faithful, loyal, and criminal idiots orchestrated by an insecure, pathological, and immoral mob boss.\n",
      "Processed: donnie be a comedy of error evil intent with no fear of consequence a gang of faithful loyal and criminal idiot orchestrate by an insecure pathological and immoral mob bos\n",
      "Author: PeteLarsen\n",
      "\n",
      "## Word: insecure_pathological ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: King Donald is a comedy of errors. Evil intent with no fear of consequence. A gang of faithful, loyal, and criminal idiots orchestrated by an insecure, pathological, and immoral mob boss. If there are...\n",
      "Processed: king donald be a comedy of error evil intent with no fear of consequence a gang of faithful loyal and criminal idiot orchestrate by an insecure pathological and immoral mob bos if there be some decent...\n",
      "Author: PeteLarsen\n",
      "\n",
      "Example 2:\n",
      "Original: Donnie is a comedy of errors. Evil intent with no fear of consequence. A gang of faithful, loyal, and criminal idiots orchestrated by an insecure, pathological, and immoral mob boss.\n",
      "Processed: donnie be a comedy of error evil intent with no fear of consequence a gang of faithful loyal and criminal idiot orchestrate by an insecure pathological and immoral mob bos\n",
      "Author: PeteLarsen\n",
      "\n",
      "Example 3:\n",
      "Original: King Donald is a comedy of errors. Evil intent with no fear. A gang of faithful, loyal, and criminal idiots orchestrated by an insecure, pathological, and immoral mob boss.\n",
      "Processed: king donald be a comedy of error evil intent with no fear a gang of faithful loyal and criminal idiot orchestrate by an insecure pathological and immoral mob bos\n",
      "Author: PeteLarsen\n",
      "\n",
      "## Word: dr_martin ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: &gt;    Joe Biden - Vice Presidents usually make a run for President when they can, but Biden didn't fare quite so well in the 2008 Primary, and there's not much enthusiasm in the base. He does have a...\n",
      "Processed: gt joe biden vice president usually make a run for president when they can but biden didn t fare quite so well in the primary and there s not much enthusiasm in the base he do have a broad appeal if n...\n",
      "Author: Anticipator1234\n",
      "\n",
      "Example 2:\n",
      "Original: Don't forget that while the Republican party encompasses the majority of the racist white end of the political population today, they didn't prior to 60 years ago.  Lincoln was a Republican, John C. C...\n",
      "Processed: don t forget that while the republican party encompass the majority of the racist white end of the political population today they didn t prior to year ago lincoln be a republican john c calhoun be a ...\n",
      "Author: river-wind\n",
      "\n",
      "Example 3:\n",
      "Original: I'm sure that Ted Cruz, Walter E. Williams, Marco Rubio, Mia Love, Susana Martinez, Deneen Borelli, Alfonzo Rachel, Mychal Massie, Dr. Thomas Sowell, Niger Innis, Crystal Wright, Herman Cain, Stacy Da...\n",
      "Processed: i m sure that ted cruz walter e williams marco rubio mia love susana martinez deneen borelli alfonzo rachel mychal massie dr thomas sowell niger innis crystal wright herman cain stacy dash allen west ...\n",
      "Author: TreeGrowsInBrooklyn\n",
      "\n",
      "## Word: luther_king ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: It is meant to say slavery  was bad and your doing the same thing all over again. grant it slavery was much worse but is still taking away peoples civil rights. were would we be with Martin Luther Kin...\n",
      "Processed: it be meant to say slavery be bad and your do the same thing all over again grant it slavery be much bad but be still take away people civil right be would we be with martin luther king\n",
      "Author: johnson-johnson\n",
      "\n",
      "Example 2:\n",
      "Original: http://www.dockersunion.net/vb/showthread.php?632-JFK-Assassination-Nutshell .. What does Carl have to say about this - The Moorman shot taken Dallas Tx, 22 November 1963, at the assassination of Pres...\n",
      "Processed: what do carl have to say about this the moorman shot take dallas tx november at the assassination of president kennedy show martin luther amp coretta king together in one of two sniper nest on the gra...\n",
      "Author: RoyLuhza\n",
      "\n",
      "Example 3:\n",
      "Original: &gt;The Moorman shot taken Dallas Tx, 22 November 1963, at the assassination of President Kennedy, shows Martin Luther &amp; Coretta﻿ King together in one of two snipers nests on﻿ the Grassy Knoll\n",
      "\n",
      "Wh...\n",
      "Processed: gt the moorman shot take dallas tx november at the assassination of president kennedy show martin luther amp coretta king together in one of two sniper nest on the grassy knoll what do i read this cor...\n",
      "Author: oddmanout\n",
      "\n",
      "## Word: fri_aug ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: I'm honestly unsure about how this disenfranchises voters though. Of all the friends I know (those who are US citizens), everyone has some sort of valid ID on them. I should know after many drunken pa...\n",
      "Processed: i m honestly unsure about how this disenfranchise voter though of all the friend i know those who be u citizens everyone have some sort of valid id on them i should know after many drunken party of la...\n",
      "Author: green_boy\n",
      "\n",
      "Example 2:\n",
      "Original: I'm honestly unsure about how this disenfranchises voters though. Of all the friends I know (those who are US citizens), everyone has some sort of valid ID on them. I should know after many drunken pa...\n",
      "Processed: i m honestly unsure about how this disenfranchise voter though of all the friend i know those who be u citizens everyone have some sort of valid id on them i should know after many drunken party of la...\n",
      "Author: green_boy\n",
      "\n",
      "Example 3:\n",
      "Original: This is the problem with republicans. Cut the budget, cut the deficit UNLESS it effects us, then SPEND SPEND SPEND. \n",
      "\n",
      "EVERY FREAKING TIME!\n",
      "\n",
      "It makes me so angry. Typical republican crap (from actual b...\n",
      "Processed: this be the problem with republican cut the budget cut the deficit unless it effect u then spend spend spend every freaking time it make me so angry typical republican crap from actual big name politi...\n",
      "Author: alienzx\n",
      "\n",
      "## Word: edt_tue ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: Was Mainstream news 3 years ago. \n",
      "\n",
      "“More signs point to Mark Zuckerberg possibly running for president in 2020\n",
      "Published Tue, Aug 15 2017 1:51 PM EDT\n",
      "\n",
      "Zuckerberg and his wife Priscilla Chan have hired...\n",
      "Processed: be mainstream news year ago more sign point to mark zuckerberg possibly run for president in publish tue aug pm edt zuckerberg and his wife priscilla chan have hire joel benenson a democratic pollster...\n",
      "Author: WestFast\n",
      "\n",
      "Example 2:\n",
      "Original: Was Mainstream news 3 years ago. \n",
      "\n",
      "“More signs point to Mark Zuckerberg possibly running for president in 2020\n",
      "Published Tue, Aug 15 2017 1:51 PM EDT\n",
      "\n",
      "Zuckerberg and his wife Priscilla Chan have hired...\n",
      "Processed: be mainstream news year ago more sign point to mark zuckerberg possibly run for president in publish tue aug pm edt zuckerberg and his wife priscilla chan have hire joel benenson a democratic pollster...\n",
      "Author: WestFast\n",
      "\n",
      "Example 3:\n",
      "Original: This is why republicans are using southwest as example - As long as there is a hint and possibility, it’s good enough for politics. \n",
      "\n",
      "https://www.cnbc.com/amp/2021/10/12/covid-vaccine-southwest-ceo-ga...\n",
      "Processed: this be why republican be use southwest as example as long a there be a hint and possibility it s good enough for politics summary southwest ceo say he never want a covid vaccine mandate but biden for...\n",
      "Author: No_Dark9287\n",
      "\n",
      "## Word: fri_aug ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: I'm honestly unsure about how this disenfranchises voters though. Of all the friends I know (those who are US citizens), everyone has some sort of valid ID on them. I should know after many drunken pa...\n",
      "Processed: i m honestly unsure about how this disenfranchise voter though of all the friend i know those who be u citizens everyone have some sort of valid id on them i should know after many drunken party of la...\n",
      "Author: green_boy\n",
      "\n",
      "Example 2:\n",
      "Original: I'm honestly unsure about how this disenfranchises voters though. Of all the friends I know (those who are US citizens), everyone has some sort of valid ID on them. I should know after many drunken pa...\n",
      "Processed: i m honestly unsure about how this disenfranchise voter though of all the friend i know those who be u citizens everyone have some sort of valid id on them i should know after many drunken party of la...\n",
      "Author: green_boy\n",
      "\n",
      "Example 3:\n",
      "Original: This is the problem with republicans. Cut the budget, cut the deficit UNLESS it effects us, then SPEND SPEND SPEND. \n",
      "\n",
      "EVERY FREAKING TIME!\n",
      "\n",
      "It makes me so angry. Typical republican crap (from actual b...\n",
      "Processed: this be the problem with republican cut the budget cut the deficit unless it effect u then spend spend spend every freaking time it make me so angry typical republican crap from actual big name politi...\n",
      "Author: alienzx\n",
      "\n",
      "## Word: pm_pm ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: This is regarding the latest developments in the Wisconsin protests and the senator walkout to prevent quorum on this \"budget\" bill which strips collective bargaining for workers:\n",
      "\n",
      "*\"\"We're going to b...\n",
      "Processed: this be regard the late development in the wisconsin protest and the senator walkout to prevent quorum on this budget bill which strip collective bargaining for worker we re go to be stay away until w...\n",
      "Author: tob_krean\n",
      "\n",
      "Example 2:\n",
      "Original: This is regarding the latest developments in the Wisconsin protests and the senator walkout to prevent quorum on this \"budget\" bill which strips collective bargaining for workers:\n",
      "\n",
      "*\"\"We're going to b...\n",
      "Processed: this be regard the late development in the wisconsin protest and the senator walkout to prevent quorum on this budget bill which strip collective bargaining for worker we re go to be stay away until w...\n",
      "Author: tob_krean\n",
      "\n",
      "Example 3:\n",
      "Original: This is regarding the latest developments in the Wisconsin protests and the senator walkout to prevent quorum on this \"budget\" bill which strips collective bargaining for workers:\n",
      "\n",
      "*\"\"We're going to b...\n",
      "Processed: this be regard the late development in the wisconsin protest and the senator walkout to prevent quorum on this budget bill which strip collective bargaining for worker we re go to be stay away until w...\n",
      "Author: tob_krean\n",
      "\n",
      "## Word: jerry_moran ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: So I live in Kansas, and as you know, most of my neighbors love their conservative politicians. They love them so much so Jerry Moran, Pat Roberts, and the entirety of the GOP know they have the KS vo...\n",
      "Processed: so i live in kansa and a you know most of my neighbor love their conservative politician they love them so much so jerry moran pat robert and the entirety of the gop know they have the k vote what s m...\n",
      "Author: sidneyaks\n",
      "\n",
      "Example 2:\n",
      "Original: &gt; WASHINGTON — The Senate voted 59-41 on Thursday to cancel President Donald Trump's national security declaration to fund a wall on the border, picking up the support of 12 Republicans to put the ...\n",
      "Processed: gt washington the senate vote on thursday to cancel president donald trump s national security declaration to fund a wall on the border pick up the support of republican to put the measure over the to...\n",
      "Author: progress18\n",
      "\n",
      "Example 3:\n",
      "Original: This time let’s find out why Russia is corresponding with Ron Johnson. This is such bullshit. These tea party types are at it once again.\n",
      "\n",
      "Eight Republicans pick the worst possible place to celebrate ...\n",
      "Processed: this time let s find out why russia be correspond with ron johnson this be such bullshit these tea party type be at it once again eight republican pick the bad possible place to celebrate july in mosc...\n",
      "Author: dakandy\n",
      "\n",
      "## Word: john_hoeven ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: This time let’s find out why Russia is corresponding with Ron Johnson. This is such bullshit. These tea party types are at it once again.\n",
      "\n",
      "Eight Republicans pick the worst possible place to celebrate ...\n",
      "Processed: this time let s find out why russia be correspond with ron johnson this be such bullshit these tea party type be at it once again eight republican pick the bad possible place to celebrate july in mosc...\n",
      "Author: dakandy\n",
      "\n",
      "Example 2:\n",
      "Original: • ⁠Multiple Republican Congressmen went to Russia on July 4th, 2018.\n",
      "\n",
      "Sen. Richard C. Shelby (R-Ala.) told Russia’s foreign minister that while Russia and the United States were competitors, “we don’t...\n",
      "Processed: multiple republican congressman go to russia on july th sen richard c shelby r ala tell russia s foreign minister that while russia and the united state be competitor we don t necessarily need to be a...\n",
      "Author: NORDLAN\n",
      "\n",
      "Example 3:\n",
      "Original: GOP senator Ron Johnson implicated in Rudy Giuliani plot to smear Biden with help from Russian spy\n",
      "https://www.rawstory.com/2020/09/gop-senator-implicated-in-rudy-giuliani-plot-to-smear-biden-with-hel...\n",
      "Processed: gop senator ron johnson implicate in rudy giuliani plot to smear biden with help from russian spy multiple republican congressman go to russia on july th sen richard c shelby r ala who chair the senat...\n",
      "Author: NORDLAN\n",
      "\n",
      "## Word: j_fzim ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: ![gif](giphy|J8FZIm9VoBU6Q)\n",
      "Processed: gif giphy j fzim vobu q\n",
      "Author: fatjumboshrimp\n",
      "\n",
      "Example 2:\n",
      "Original: ![gif](giphy|J8FZIm9VoBU6Q)\n",
      "Processed: gif giphy j fzim vobu q\n",
      "Author: fatjumboshrimp\n",
      "\n",
      "Example 3:\n",
      "Original: ![gif](giphy|J8FZIm9VoBU6Q)\n",
      "Processed: gif giphy j fzim vobu q\n",
      "Author: NemoLeeGreen\n",
      "\n",
      "## Word: xt ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: I definitely don't believe that Obama's disapproval ratings are based primarily on his race (as some leftists lazily accuse), and I do agree that the primary approval ratings of blacks in particular a...\n",
      "Processed: i definitely don t believe that obama s disapproval rating be base primarily on his race a some leftist lazily accuse and i do agree that the primary approval rating of black in particular be probably...\n",
      "Author: Clumpy\n",
      "\n",
      "Example 2:\n",
      "Original: I definitely don't believe that Obama's disapproval ratings are based primarily on his race (as some leftists lazily accuse), and I do agree that the primary approval ratings of blacks in particular a...\n",
      "Processed: i definitely don t believe that obama s disapproval rating be base primarily on his race a some leftist lazily accuse and i do agree that the primary approval rating of black in particular be probably...\n",
      "Author: Clumpy\n",
      "\n",
      "Example 3:\n",
      "Original: \"Kathleen Kennedy Townsend, a former Maryland lieutenant governor and the eldest of Robert F. Kennedy’s 11 children, has agreed to serve as the chairwoman of the group, which will be called American B...\n",
      "Processed: kathleen kennedy townsend a former maryland lieutenant governor and the eldest of robert f kennedy s child have agree to serve a the chairwoman of the group which will be call american bridge lending ...\n",
      "Author: Koshari\n",
      "\n",
      "## Word: o_bstruct ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: And of those 42, many of them he tried to fulfill. Doing what he has in face of the most obstructive congress in history is quite a feat indeed.\n",
      "Processed: and of those many of them he try to fulfill do what he have in face of the most obstructive congress in history be quite a feat indeed\n",
      "Author: backpackwayne\n",
      "\n",
      "Example 2:\n",
      "Original: Most obstructive Congress? What about the [do-nothing Congress](http://en.wikipedia.org/wiki/80th_United_States_Congress) during Harry Truman's time?\n",
      "\n",
      "Edit: Fixed link\n",
      "Processed: most obstructive congress what about the do nothing congress during harry truman s time edit fix link\n",
      "Author: verbify\n",
      "\n",
      "Example 3:\n",
      "Original: Republican = Obstructionist\n",
      "Processed: republican obstructionist\n",
      "Author: raggamuffin10m\n",
      "\n",
      "## Word: p_roject ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: They need to heavily publicize things like this portion of the bill:\n",
      "\n",
      "&gt; “Notwithstanding ss. 13.48 (14) (am) and 16.705 (1), the department may sell any state-owned heating, cooling, and power plan...\n",
      "Processed: they need to heavily publicize thing like this portion of the bill gt notwithstanding ss be and the department may sell any state own heat cooling and power plant or may contract with a private entity...\n",
      "Author: JimmyHavok\n",
      "\n",
      "Example 2:\n",
      "Original: They need to heavily publicize things like this portion of the bill:\n",
      "\n",
      "&gt; “Notwithstanding ss. 13.48 (14) (am) and 16.705 (1), the department may sell any state-owned heating, cooling, and power plan...\n",
      "Processed: they need to heavily publicize thing like this portion of the bill gt notwithstanding ss be and the department may sell any state own heat cooling and power plant or may contract with a private entity...\n",
      "Author: JimmyHavok\n",
      "\n",
      "Example 3:\n",
      "Original: Nonviolence seeks to melt the hearts of oppressors, where possible, to actually win them over. This may not be easy, though it may be more possible than we often believe. Consider the people who did c...\n",
      "Processed: nonviolence seek to melt the heart of oppressor where possible to actually win them over this may not be easy though it may be more possible than we often believe consider the people who do come to th...\n",
      "Author: ravia\n",
      "\n",
      "## Word: agedlikemilk ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: /r/agedlikemilk\n",
      "Processed: r agedlikemilk\n",
      "Author: dongsuvious\n",
      "\n",
      "Example 2:\n",
      "Original: Please don't wind up in r/agedlikemilk I am begging you\n",
      "Processed: please don t wind up in r agedlikemilk i be beg you\n",
      "Author: HawlSera\n",
      "\n",
      "Example 3:\n",
      "Original: The majority in Holder v Shelby County called this “40 year-old facts having no logical relationship to the present day.”\n",
      "\n",
      "r/agedlikemilk\n",
      "Processed: the majority in holder v shelby county call this year old fact have no logical relationship to the present day r agedlikemilk\n",
      "Author: minus_minus\n",
      "\n",
      "## Word: noshitsherlock ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: /r/noshitsherlock\n",
      "Processed: r noshitsherlock\n",
      "Author: [deleted]\n",
      "\n",
      "Example 2:\n",
      "Original: /r/noshitsherlock\n",
      "Processed: r noshitsherlock\n",
      "Author: [deleted]\n",
      "\n",
      "Example 3:\n",
      "Original: Someone should send this over to /r/noshitsherlock\n",
      "Processed: someone should send this over to r noshitsherlock\n",
      "Author: PigSlam\n",
      "\n",
      "## Word: noshitsherlock ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: /r/noshitsherlock\n",
      "Processed: r noshitsherlock\n",
      "Author: [deleted]\n",
      "\n",
      "Example 2:\n",
      "Original: /r/noshitsherlock\n",
      "Processed: r noshitsherlock\n",
      "Author: [deleted]\n",
      "\n",
      "Example 3:\n",
      "Original: Someone should send this over to /r/noshitsherlock\n",
      "Processed: someone should send this over to r noshitsherlock\n",
      "Author: PigSlam\n",
      "\n",
      "## Word: lostredditors ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: This is some top tier /r/lostredditors.\n",
      "Processed: this be some top tier r lostredditors\n",
      "Author: dolphins3\n",
      "\n",
      "Example 2:\n",
      "Original: r/lostredditors\n",
      "Processed: r lostredditors\n",
      "Author: Musicianalyst\n",
      "\n",
      "Example 3:\n",
      "Original: I respect wanting facts/science, but this seems rather r/lostredditors to me\n",
      "Processed: i respect want fact science but this seem rather r lostredditors to me\n",
      "Author: madam_zeroni\n",
      "\n",
      "## Word: paywall_workaround ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: Hit the paywall. Is there a workaround? \n",
      "Processed: hit the paywall be there a workaround\n",
      "Author: grumpman\n",
      "\n",
      "Example 2:\n",
      "Original: Paywall workaround:\n",
      "\n",
      "https://outline.com/BeEtmb\n",
      "Processed: paywall workaround\n",
      "Author: wenchette\n",
      "\n",
      "Example 3:\n",
      "Original: Free paywall workaround for this article:\n",
      "\n",
      "http://archive.is/FhnR2\n",
      "Processed: free paywall workaround for this article\n",
      "Author: wenchette\n",
      "\n",
      "## Word: firewall_workaround ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: Free firewall workaround:\n",
      "\n",
      "https://archive.is/ct4H5\n",
      "Processed: free firewall workaround\n",
      "Author: wenchette\n",
      "\n",
      "Example 2:\n",
      "Original: Free firewall workaround:\n",
      "\n",
      "https://archive.is/GXogy\n",
      "Processed: free firewall workaround\n",
      "Author: wenchette\n",
      "\n",
      "Example 3:\n",
      "Original: Free firewall workaround:\n",
      "\n",
      "https://archive.is/zJH8R\n",
      "Processed: free firewall workaround\n",
      "Author: wenchette\n",
      "\n",
      "## Word: ecuador_indict ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: The 101 plus contacts that were all lied about were all about the cost of vodka, right? The campaign data Trump's campaign manager, and no one does anything without clearing it with Trump, gave to som...\n",
      "Processed: the plus contact that be all lie about be all about the cost of vodka right the campaign data trump s campaign manager and no one do anything without clear it with trump give to someone from russian i...\n",
      "Author: therecordcorrected\n",
      "\n",
      "Example 2:\n",
      "Original: Hi there, imll99. Thank you for participating on /r/democrats. Unfortunately, your post has been automatically removed because we do not permit anyone to post more than 5 times over a 24 hour timespan...\n",
      "Processed: hi there imll thank you for participate on r democrat unfortunately your post have be automatically remove because we do not permit anyone to post more than time over a hour timespan your previous pos...\n",
      "Author: FloodgatesBot\n",
      "\n",
      "Example 3:\n",
      "Original: Hi there, imll99. Thank you for participating on /r/democrats. Unfortunately, your post has been automatically removed because we do not permit anyone to post more than 5 times over a 24 hour timespan...\n",
      "Processed: hi there imll thank you for participate on r democrat unfortunately your post have be automatically remove because we do not permit anyone to post more than time over a hour timespan your previous pos...\n",
      "Author: FloodgatesBot\n",
      "\n",
      "## Word: alleged_bribery ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: Hi there, imll99. Thank you for participating on /r/democrats. Unfortunately, your post has been automatically removed because we do not permit anyone to post more than 5 times over a 24 hour timespan...\n",
      "Processed: hi there imll thank you for participate on r democrat unfortunately your post have be automatically remove because we do not permit anyone to post more than time over a hour timespan your previous pos...\n",
      "Author: FloodgatesBot\n",
      "\n",
      "Example 2:\n",
      "Original: Hi there, imll99. Thank you for participating on /r/democrats. Unfortunately, your post has been automatically removed because we do not permit anyone to post more than 5 times over a 24 hour timespan...\n",
      "Processed: hi there imll thank you for participate on r democrat unfortunately your post have be automatically remove because we do not permit anyone to post more than time over a hour timespan your previous pos...\n",
      "Author: FloodgatesBot\n",
      "\n",
      "Example 3:\n",
      "Original: Hi there, imll99. Thank you for participating on /r/democrats. Unfortunately, your post has been automatically removed because we do not permit anyone to post more than 5 times over a 24 hour timespan...\n",
      "Processed: hi there imll thank you for participate on r democrat unfortunately your post have be automatically remove because we do not permit anyone to post more than time over a hour timespan your previous pos...\n",
      "Author: FloodgatesBot\n",
      "\n",
      "## Word: cynthia ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: \n",
      "\"Democrats choosing Obama over Hillary was another failure, IMO. Obama is the best we've got at this point. Dems had a chance though\"\n",
      "\n",
      "Wrong, democrats choosing a puppet like Obama over principled Li...\n",
      "Processed: democrat choose obama over hillary be another failure imo obama be the best we ve get at this point dems have a chance though wrong democrat choose a puppet like obama over principled liberal like den...\n",
      "Author: y2quest\n",
      "\n",
      "Example 2:\n",
      "Original: It's not corruption, so I kinda have to reject the premise of your question. \n",
      "\n",
      "And you must be either gallingly egotistical or just very new to politics if you think the liberal wing (they're not the ...\n",
      "Processed: it s not corruption so i kinda have to reject the premise of your question and you must be either gallingly egotistical or just very new to politics if you think the liberal wing they re not the democ...\n",
      "Author: [deleted]\n",
      "\n",
      "Example 3:\n",
      "Original: Fucking lets go Cynthia.\n",
      "Processed: fuck let go cynthia\n",
      "Author: 71017\n",
      "\n",
      "## Word: lummis ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: Hope ur right and this shitshow ends. Sweet mother of God. Put them in Prison. An add these Texas Sen. Ted Cruz\n",
      "Missouri Sen. Josh Hawley\n",
      "Wyoming Sen. Cynthia Lummis\n",
      "Kansas Sen. Roger Marshall\n",
      "Florida...\n",
      "Processed: hope ur right and this shitshow end sweet mother of god put them in prison an add these texas sen ted cruz missouri sen josh hawley wyoming sen cynthia lummis kansa sen roger marshall florida sen rick...\n",
      "Author: spindoctor2020\n",
      "\n",
      "Example 2:\n",
      "Original: &gt;Manchin and Sinema aren't even doing what their constituents ask of them\n",
      "  \n",
      "Sinema sure, Manchin is in one of the most Republican states in the nation so some of the things that we as liberals don...\n",
      "Processed: gt manchin and sinema aren t even do what their constituent ask of them sinema sure manchin be in one of the most republican state in the nation so some of the thing that we a liberal don t like be su...\n",
      "Author: SaintArkweather\n",
      "\n",
      "Example 3:\n",
      "Original: Yup. Here's a possible mockup of who would be in the senate.\n",
      "\n",
      "Angus King-I   \n",
      "Susan Collins-R\n",
      "\n",
      "Jeanne Shaheen-D   \n",
      "Maggie Hassan-D\n",
      "\n",
      "Patrick Leahy-D   \n",
      "Bernie Sanders-I\n",
      "\n",
      "Elizabeth Warren-D   \n",
      "Ed Markey...\n",
      "Processed: yup here s a possible mockup of who would be in the senate angus king i susan collins r jeanne shaheen d maggie hassan d patrick leahy d bernie sander i elizabeth warren d ed markey d jack reed d shel...\n",
      "Author: MondaleforPresident\n",
      "\n",
      "## Word: p_roject ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: They need to heavily publicize things like this portion of the bill:\n",
      "\n",
      "&gt; “Notwithstanding ss. 13.48 (14) (am) and 16.705 (1), the department may sell any state-owned heating, cooling, and power plan...\n",
      "Processed: they need to heavily publicize thing like this portion of the bill gt notwithstanding ss be and the department may sell any state own heat cooling and power plant or may contract with a private entity...\n",
      "Author: JimmyHavok\n",
      "\n",
      "Example 2:\n",
      "Original: They need to heavily publicize things like this portion of the bill:\n",
      "\n",
      "&gt; “Notwithstanding ss. 13.48 (14) (am) and 16.705 (1), the department may sell any state-owned heating, cooling, and power plan...\n",
      "Processed: they need to heavily publicize thing like this portion of the bill gt notwithstanding ss be and the department may sell any state own heat cooling and power plant or may contract with a private entity...\n",
      "Author: JimmyHavok\n",
      "\n",
      "Example 3:\n",
      "Original: Nonviolence seeks to melt the hearts of oppressors, where possible, to actually win them over. This may not be easy, though it may be more possible than we often believe. Consider the people who did c...\n",
      "Processed: nonviolence seek to melt the heart of oppressor where possible to actually win them over this may not be easy though it may be more possible than we often believe consider the people who do come to th...\n",
      "Author: ravia\n",
      "\n",
      "## Word: g_aslight ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: You imply emotion where none exists, a gaslight attempt to gain a high ground through logic fallacy. Nice try, but it's both transparent and juvenile-- not to mention dead wrong. And the tone you use ...\n",
      "Processed: you imply emotion where none exist a gaslight attempt to gain a high ground through logic fallacy nice try but it s both transparent and juvenile not to mention dead wrong and the tone you use to ask ...\n",
      "Author: OkToBeTakei\n",
      "\n",
      "Example 2:\n",
      "Original: This is the best tl;dr I could make, [original](http://www.salon.com/2015/11/01/gop_hates_the_media_because_they_are_liars_the_damning_evil_history_of_the_rights_war_on_a_free_press/) reduced by 93%. ...\n",
      "Processed: this be the best tl dr i could make original reduce by i m a bot gt movement conservative jump on the idea that a cabal of liberal in the medium be undermine their political success gt in the reagan y...\n",
      "Author: autotldr\n",
      "\n",
      "Example 3:\n",
      "Original: Personally I think Clinton would make an OK president, not that she's my first choice.\n",
      "\n",
      "But I think this headline points to a real issue: she's been the target of character assassination for so long, ...\n",
      "Processed: personally i think clinton would make an ok president not that she s my first choice but i think this headline point to a real issue she s be the target of character assassination for so long even som...\n",
      "Author: DronedAgain\n",
      "\n",
      "## Word: climate_resiliency ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: \"As leaders at all levels of US government concluded after Sandy, microgrids are shaping up to be key components to improving “energy resiliency”, and to mitigating the crippling impact of disasters f...\n",
      "Processed: a leader at all level of u government conclude after sandy microgrids be shape up to be key component to improve energy resiliency and to mitigate the crippling impact of disaster fuel by climate chan...\n",
      "Author: RedTurnsBlue\n",
      "\n",
      "Example 2:\n",
      "Original: (Part 4, more in depth)\n",
      "\n",
      "From local Behance founding member [Dave Stein](https://twitter.com/dave_stein?s=21)\n",
      "\n",
      "&gt; What are you reading?\n",
      "Yang: Says he's a nerd for reading AI\n",
      "Iscol: Green Lights\n",
      "Mora...\n",
      "Processed: part more in depth from local behance found member dave stein gt what be you read yang say he s a nerd for read ai iscol green light morales the black friend on be a good white person ray profile of b...\n",
      "Author: EagleFly_5\n",
      "\n",
      "Example 3:\n",
      "Original: The Infrastructure Investment and Jobs Act is a huge accomplishment that we can add on to the many Biden and his fellow Democrats have already seen this year.\n",
      "\n",
      "Keep in mind what this success means goi...\n",
      "Processed: the infrastructure investment and job act be a huge accomplishment that we can add on to the many biden and his fellow democrat have already see this year keep in mind what this success mean go forwar...\n",
      "Author: Zexapher\n",
      "\n",
      "## Word: coastal_resiliency ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: The Infrastructure Investment and Jobs Act is a huge accomplishment that we can add on to the many Biden and his fellow Democrats have already seen this year.\n",
      "\n",
      "Keep in mind what this success means goi...\n",
      "Processed: the infrastructure investment and job act be a huge accomplishment that we can add on to the many biden and his fellow democrat have already see this year keep in mind what this success mean go forwar...\n",
      "Author: Zexapher\n",
      "\n",
      "Example 2:\n",
      "Original: The article makes a decent point that messaging needs to improve, and combining that with your idea I think we need to get better at acknowledging successes. A good platform is great, I'm sure it help...\n",
      "Processed: the article make a decent point that message need to improve and combine that with your idea i think we need to get good at acknowledge success a good platform be great i m sure it help biden a ton bu...\n",
      "Author: Zexapher\n",
      "\n",
      "Example 3:\n",
      "Original: &gt;Presidents don’t make or destroy job s\n",
      "\n",
      " They do if they lie a lot\n",
      "\n",
      "**Trump lied,**\n",
      "\n",
      "the Market crashed,\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "**Trump lied,**\n",
      "\n",
      "the economy died, \n",
      "\n",
      "--------------------...\n",
      "Processed: gt president don t make or destroy job s they do if they lie a lot trump lie the market crash trump lie the economy die trump lie business die trump lie job die trump lie over thousand american die ot...\n",
      "Author: Old_Fart_1948\n",
      "\n",
      "## Word: vobu_q ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: ![gif](giphy|J8FZIm9VoBU6Q)\n",
      "Processed: gif giphy j fzim vobu q\n",
      "Author: fatjumboshrimp\n",
      "\n",
      "Example 2:\n",
      "Original: ![gif](giphy|J8FZIm9VoBU6Q)\n",
      "Processed: gif giphy j fzim vobu q\n",
      "Author: fatjumboshrimp\n",
      "\n",
      "Example 3:\n",
      "Original: ![gif](giphy|J8FZIm9VoBU6Q)\n",
      "Processed: gif giphy j fzim vobu q\n",
      "Author: NemoLeeGreen\n",
      "\n",
      "## Word: xt ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: I definitely don't believe that Obama's disapproval ratings are based primarily on his race (as some leftists lazily accuse), and I do agree that the primary approval ratings of blacks in particular a...\n",
      "Processed: i definitely don t believe that obama s disapproval rating be base primarily on his race a some leftist lazily accuse and i do agree that the primary approval rating of black in particular be probably...\n",
      "Author: Clumpy\n",
      "\n",
      "Example 2:\n",
      "Original: I definitely don't believe that Obama's disapproval ratings are based primarily on his race (as some leftists lazily accuse), and I do agree that the primary approval ratings of blacks in particular a...\n",
      "Processed: i definitely don t believe that obama s disapproval rating be base primarily on his race a some leftist lazily accuse and i do agree that the primary approval rating of black in particular be probably...\n",
      "Author: Clumpy\n",
      "\n",
      "Example 3:\n",
      "Original: \"Kathleen Kennedy Townsend, a former Maryland lieutenant governor and the eldest of Robert F. Kennedy’s 11 children, has agreed to serve as the chairwoman of the group, which will be called American B...\n",
      "Processed: kathleen kennedy townsend a former maryland lieutenant governor and the eldest of robert f kennedy s child have agree to serve a the chairwoman of the group which will be call american bridge lending ...\n",
      "Author: Koshari\n",
      "\n",
      "## Word: edt_tue ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: Was Mainstream news 3 years ago. \n",
      "\n",
      "“More signs point to Mark Zuckerberg possibly running for president in 2020\n",
      "Published Tue, Aug 15 2017 1:51 PM EDT\n",
      "\n",
      "Zuckerberg and his wife Priscilla Chan have hired...\n",
      "Processed: be mainstream news year ago more sign point to mark zuckerberg possibly run for president in publish tue aug pm edt zuckerberg and his wife priscilla chan have hire joel benenson a democratic pollster...\n",
      "Author: WestFast\n",
      "\n",
      "Example 2:\n",
      "Original: Was Mainstream news 3 years ago. \n",
      "\n",
      "“More signs point to Mark Zuckerberg possibly running for president in 2020\n",
      "Published Tue, Aug 15 2017 1:51 PM EDT\n",
      "\n",
      "Zuckerberg and his wife Priscilla Chan have hired...\n",
      "Processed: be mainstream news year ago more sign point to mark zuckerberg possibly run for president in publish tue aug pm edt zuckerberg and his wife priscilla chan have hire joel benenson a democratic pollster...\n",
      "Author: WestFast\n",
      "\n",
      "Example 3:\n",
      "Original: This is why republicans are using southwest as example - As long as there is a hint and possibility, it’s good enough for politics. \n",
      "\n",
      "https://www.cnbc.com/amp/2021/10/12/covid-vaccine-southwest-ceo-ga...\n",
      "Processed: this be why republican be use southwest as example as long a there be a hint and possibility it s good enough for politics summary southwest ceo say he never want a covid vaccine mandate but biden for...\n",
      "Author: No_Dark9287\n",
      "\n",
      "## Word: pm_pm ##\n",
      "\n",
      "\n",
      "Example 1:\n",
      "Original: This is regarding the latest developments in the Wisconsin protests and the senator walkout to prevent quorum on this \"budget\" bill which strips collective bargaining for workers:\n",
      "\n",
      "*\"\"We're going to b...\n",
      "Processed: this be regard the late development in the wisconsin protest and the senator walkout to prevent quorum on this budget bill which strip collective bargaining for worker we re go to be stay away until w...\n",
      "Author: tob_krean\n",
      "\n",
      "Example 2:\n",
      "Original: This is regarding the latest developments in the Wisconsin protests and the senator walkout to prevent quorum on this \"budget\" bill which strips collective bargaining for workers:\n",
      "\n",
      "*\"\"We're going to b...\n",
      "Processed: this be regard the late development in the wisconsin protest and the senator walkout to prevent quorum on this budget bill which strip collective bargaining for worker we re go to be stay away until w...\n",
      "Author: tob_krean\n",
      "\n",
      "Example 3:\n",
      "Original: This is regarding the latest developments in the Wisconsin protests and the senator walkout to prevent quorum on this \"budget\" bill which strips collective bargaining for workers:\n",
      "\n",
      "*\"\"We're going to b...\n",
      "Processed: this be regard the late development in the wisconsin protest and the senator walkout to prevent quorum on this budget bill which strip collective bargaining for worker we re go to be stay away until w...\n",
      "Author: tob_krean\n",
      "\n",
      "Found examples for 50 out of 50 words\n",
      "\n",
      "Results saved to output/top50_word_comments.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from fileStreams import getFileJsonStream\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# Initialize lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocess text the same way as in model training\n",
    "def preprocess_text(text, without_stopwords=False):\n",
    "    # Remove URLs\n",
    "    txt = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    txt = re.sub(\"[^A-Za-z']+\", ' ', txt).lower()\n",
    "    \n",
    "    # Replace special characters with spaces\n",
    "    txt = re.sub(r\"['\\-_]\", ' ', txt)\n",
    "    \n",
    "    # Tokenize\n",
    "    words = txt.split()\n",
    "    \n",
    "    # Tag words with parts of speech for better lemmatization\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    processed_words = []\n",
    "    \n",
    "    for word, tag in tagged_words:\n",
    "        if without_stopwords and word in stop_words:\n",
    "            continue\n",
    "        \n",
    "        # Convert Penn Treebank tag to WordNet tag\n",
    "        if tag.startswith('J'):\n",
    "            wordnet_pos = 'a'  # adjective\n",
    "        elif tag.startswith('V'):\n",
    "            wordnet_pos = 'v'  # verb\n",
    "        elif tag.startswith('N'):\n",
    "            wordnet_pos = 'n'  # noun\n",
    "        elif tag.startswith('R'):\n",
    "            wordnet_pos = 'r'  # adverb\n",
    "        else:\n",
    "            wordnet_pos = 'n'  # default to noun\n",
    "                \n",
    "        # Lemmatize with the correct POS\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "        processed_words.append(lemma)\n",
    "    \n",
    "    return processed_words\n",
    "\n",
    "def find_comments_with_single_word(data_path, target_words, max_results=3):\n",
    "    \"\"\"Find comments containing a single target word after preprocessing\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    for word in target_words:\n",
    "        results[word] = []\n",
    "    \n",
    "    # Process comments\n",
    "    with open(data_path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(data_path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Skipping unknown file {data_path}\")\n",
    "            return results\n",
    "        \n",
    "        for row in tqdm(jsonStream, desc=\"Processing comments\"):\n",
    "            if \"body\" not in row or \"created_utc\" not in row:\n",
    "                continue\n",
    "                \n",
    "            # Check if we have enough results for all words\n",
    "            if all(len(results[word]) >= max_results for word in target_words):\n",
    "                break\n",
    "            \n",
    "            # Get the comment text\n",
    "            text = row[\"body\"]\n",
    "            \n",
    "            # Process text\n",
    "            processed_words = preprocess_text(text)\n",
    "            processed_text = \" \".join(processed_words)\n",
    "            \n",
    "            # Check for each target word\n",
    "            for word in target_words:\n",
    "                # Skip if we already have enough examples for this word\n",
    "                if len(results[word]) >= max_results:\n",
    "                    continue\n",
    "                \n",
    "                # For words with underscores, check both versions\n",
    "                search_variations = [word, word.replace('_', ' ')]\n",
    "                \n",
    "                # Also check for component parts (for bigrams)\n",
    "                components = word.split('_')\n",
    "                components_present = all(comp in processed_text for comp in components)\n",
    "                \n",
    "                # Check if the word is in the processed text or all components are present\n",
    "                if any(var in processed_text for var in search_variations) or components_present:\n",
    "                    results[word].append({\n",
    "                        'original_comment': text,\n",
    "                        'processed_comment': processed_text,\n",
    "                        'author': row.get('author', 'unknown'),\n",
    "                        'created_utc': row.get('created_utc', 'unknown')\n",
    "                    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "def find_single_words():\n",
    "    # Load the top 50 words from dem_10_10_list.csv\n",
    "    df = pd.read_csv('output/dem_10_10_list.csv')\n",
    "    \n",
    "    # Get the top 50 source words (preserving original order)\n",
    "    target_words = df['source'].head(50).tolist()\n",
    "    \n",
    "    print(f\"Searching for {len(target_words)} words from dem_10_10_list.csv\")\n",
    "    print(f\"Words to search for: {target_words}\")\n",
    "    \n",
    "    # Find comments containing these words\n",
    "    data_path = \"datasets/democrats_comments.zst\"\n",
    "    results = find_comments_with_single_word(data_path, target_words, max_results=3)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n=== Comments Containing Target Words ===\\n\")\n",
    "    \n",
    "    found_count = 0\n",
    "    for word in target_words:  # Iterate in original order\n",
    "        comments = results[word]\n",
    "        print(f\"\\n## Word: {word} ##\\n\")\n",
    "        \n",
    "        if not comments:\n",
    "            print(\"No comments found with this word.\")\n",
    "            continue\n",
    "        \n",
    "        found_count += 1\n",
    "        for i, comment in enumerate(comments):\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            print(f\"Original: {comment['original_comment'][:200]}...\" if len(comment['original_comment']) > 200 \n",
    "                  else f\"Original: {comment['original_comment']}\")\n",
    "            print(f\"Processed: {comment['processed_comment'][:200]}...\" if len(comment['processed_comment']) > 200\n",
    "                  else f\"Processed: {comment['processed_comment']}\")\n",
    "            print(f\"Author: {comment['author']}\")\n",
    "    \n",
    "    print(f\"\\nFound examples for {found_count} out of {len(target_words)} words\")\n",
    "    \n",
    "    # Save results to file\n",
    "    with open(\"output/top50_word_comments.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(\"\\nResults saved to output/top50_word_comments.json\")\n",
    "\n",
    "# Run the single word search\n",
    "find_single_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b63348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_word2vect_model(path, party, without_stopwords=True, phrases_min_count=5, word2vec_min_count=5):\n",
    "    print(f\"Processing file {path}\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # For each period, track comments and user-word usage\n",
    "    chunks = {\n",
    "        \"before_2016\": [],\n",
    "        \"2017_2020\": [],\n",
    "        \"2021_2024\": [],\n",
    "    }\n",
    "    user_words = {\n",
    "        \"before_2016\": defaultdict(set),\n",
    "        \"2017_2020\": defaultdict(set),\n",
    "        \"2021_2024\": defaultdict(set),\n",
    "    }\n",
    "    user_comments = {\n",
    "        \"before_2016\": defaultdict(list),\n",
    "        \"2017_2020\": defaultdict(list),\n",
    "        \"2021_2024\": defaultdict(list),\n",
    "    }\n",
    "    counts = {period: 0 for period in chunks.keys()}\n",
    "\n",
    "    with open(path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Skipping unknown file {path}\")\n",
    "            return\n",
    "        for row in tqdm(jsonStream, desc=\"Processing comments\"):\n",
    "            if \"body\" not in row or \"created_utc\" not in row or \"author\" not in row:\n",
    "                continue\n",
    "            author = row[\"author\"]\n",
    "            if author in {\"AutoModerator\", \"election_info_bot\"}:\n",
    "                continue\n",
    "            text = row[\"body\"]\n",
    "            created_timestamp = row[\"created_utc\"]\n",
    "            year = datetime.datetime.fromtimestamp(int(created_timestamp)).year\n",
    "            # if year <= 2016:\n",
    "            #     chunk_key = \"before_2016\"\n",
    "            # elif 2017 <= year <= 2020:\n",
    "            #     chunk_key = \"2017_2020\"\n",
    "            # elif 2021 <= year <= 2024:\n",
    "            #     chunk_key = \"2021_2024\"\n",
    "            # else:\n",
    "            #     continue\n",
    "            if 2021 <= year <= 2024:\n",
    "                chunk_key = \"2021_2024\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            txt = re.sub(r'http\\S+', '', text)\n",
    "            txt = re.sub(\"[^A-Za-z']+\", ' ', txt).lower()\n",
    "            txt = re.sub(r\"['\\-_]\", ' ', txt)\n",
    "            words = txt.split()\n",
    "            if not words:\n",
    "                continue\n",
    "            tagged_words = nltk.pos_tag(words)\n",
    "            processed_words = []\n",
    "            for word, tag in tagged_words:\n",
    "                if without_stopwords and word in stop_words:\n",
    "                    continue\n",
    "                if tag.startswith('J'):\n",
    "                    wordnet_pos = 'a'\n",
    "                elif tag.startswith('V'):\n",
    "                    wordnet_pos = 'v'\n",
    "                elif tag.startswith('N'):\n",
    "                    wordnet_pos = 'n'\n",
    "                elif tag.startswith('R'):\n",
    "                    wordnet_pos = 'r'\n",
    "                else:\n",
    "                    wordnet_pos = 'n'\n",
    "                lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "                processed_words.append(lemma)\n",
    "                user_words[chunk_key][lemma].add(author)\n",
    "            if processed_words:\n",
    "                user_comments[chunk_key][author].append(processed_words)\n",
    "                counts[chunk_key] += 1\n",
    "\n",
    "    print(\"\\n=== Comment Counts by Period ===\")\n",
    "    for period, count in counts.items():\n",
    "        print(f\"{period}: {count} comments\")\n",
    "\n",
    "    # Filter words by user count and rebuild comments for each period\n",
    "    for period in chunks.keys():\n",
    "        valid_words = {w for w, users in user_words[period].items() if len(users) >= 5}\n",
    "        filtered_comments = []\n",
    "        for comments in user_comments[period].values():\n",
    "            for comment in comments:\n",
    "                filtered = [w for w in comment if w in valid_words]\n",
    "                if filtered:\n",
    "                    filtered_comments.append(filtered)\n",
    "        print(f\"{period}: {len(filtered_comments)} comments after filtering words by user count\")\n",
    "        if filtered_comments:\n",
    "            print(f\"\\nExtracting bigrams for {period}...\")\n",
    "            phrases = Phrases(filtered_comments, \n",
    "                              min_count=phrases_min_count, \n",
    "                              threshold=100)\n",
    "            bigram_model = Phraser(phrases)\n",
    "            bigrammed_comments = [bigram_model[comment] for comment in filtered_comments]\n",
    "            chunks[period] = bigrammed_comments\n",
    "        else:\n",
    "            chunks[period] = []\n",
    "\n",
    "    # Train a Word2Vec model for each time period\n",
    "    for period, comments in chunks.items():\n",
    "        if len(comments) > 0:\n",
    "            print(f\"\\n=== Training Word2Vec for {period} ({len(comments)} comments) ===\")\n",
    "            model = Word2Vec(\n",
    "                vector_size=300,\n",
    "                window=5,\n",
    "                min_count=word2vec_min_count,\n",
    "                workers=16\n",
    "            )\n",
    "            model.build_vocab(comments)\n",
    "            print(f\"Vocabulary size: {len(model.wv.index_to_key)}\")\n",
    "            model.train(\n",
    "                comments,\n",
    "                total_examples=len(comments),\n",
    "                epochs=5\n",
    "            )\n",
    "            model_path = f\"models/reddit_word2vec_{phrases_min_count}_{word2vec_min_count}_filterd_{party}_{period}.model\"\n",
    "            model.save(model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def main():\n",
    "    build_word2vect_model(filePathforDemocrats, \"democrats\", without_stopwords=False, \n",
    "                          phrases_min_count=10, word2vec_min_count=10)\n",
    "    print(\"Done :>\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51c8e614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from models/reddit_word2vec_10_10_filterd_democrats_2021_2024.model\n",
      "Analyzing 50 words from dem_10_10_list.csv\n",
      "Words in model: 50 out of 50\n",
      "Processing comments from datasets/democrats_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments: 100001it [01:00, 1640.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Results saved to output/gibberish_analysis/\n",
      "Words with examples found: 17 out of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from fileStreams import getFileJsonStream\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import os\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# Initialize lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"output/gibberish_analysis\", exist_ok=True)\n",
    "\n",
    "def preprocess_text(text, without_stopwords=False):\n",
    "    \"\"\"Process text the same way as in model training\"\"\"\n",
    "    # Remove URLs\n",
    "    txt = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    txt = re.sub(\"[^A-Za-z']+\", ' ', txt).lower()\n",
    "    \n",
    "    # Replace special characters with spaces\n",
    "    txt = re.sub(r\"['\\-_]\", ' ', txt)\n",
    "    \n",
    "    # Tokenize\n",
    "    words = txt.split()\n",
    "    \n",
    "    # Tag words with parts of speech for better lemmatization\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    processed_words = []\n",
    "    \n",
    "    for word, tag in tagged_words:\n",
    "        if without_stopwords and word in stop_words:\n",
    "            continue\n",
    "        \n",
    "        # Convert Penn Treebank tag to WordNet tag\n",
    "        if tag.startswith('J'):\n",
    "            wordnet_pos = 'a'  # adjective\n",
    "        elif tag.startswith('V'):\n",
    "            wordnet_pos = 'v'  # verb\n",
    "        elif tag.startswith('N'):\n",
    "            wordnet_pos = 'n'  # noun\n",
    "        elif tag.startswith('R'):\n",
    "            wordnet_pos = 'r'  # adverb\n",
    "        else:\n",
    "            wordnet_pos = 'n'  # default to noun\n",
    "                \n",
    "        # Lemmatize with the correct POS\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "        processed_words.append(lemma)\n",
    "    \n",
    "    return processed_words\n",
    "\n",
    "def analyze_gibberish_words():\n",
    "    # Load the model to check vocabulary\n",
    "    model_path = \"models/reddit_word2vec_10_10_filterd_democrats_2021_2024.model\"\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    model = Word2Vec.load(model_path)\n",
    "    \n",
    "    # Load the top 50 source words\n",
    "    df = pd.read_csv('output/dem_10_10_list.csv')\n",
    "    target_words = df['source'].head(50).tolist()\n",
    "    \n",
    "    print(f\"Analyzing {len(target_words)} words from dem_10_10_list.csv\")\n",
    "    \n",
    "    # Check which words are in the model vocabulary\n",
    "    words_in_model = []\n",
    "    words_not_in_model = []\n",
    "    \n",
    "    for word in target_words:\n",
    "        if word in model.wv:\n",
    "            words_in_model.append(word)\n",
    "        else:\n",
    "            words_not_in_model.append(word)\n",
    "    \n",
    "    print(f\"Words in model: {len(words_in_model)} out of {len(target_words)}\")\n",
    "    if words_not_in_model:\n",
    "        print(f\"Words not found in model: {words_not_in_model}\")\n",
    "    \n",
    "    # Process sample comments to create bigrams\n",
    "    data_path = \"datasets/democrats_comments.zst\"\n",
    "    print(f\"Processing comments from {data_path}\")\n",
    "    \n",
    "    # Store comments and their processing stages\n",
    "    all_comments = []\n",
    "    word_occurrences = {word: [] for word in target_words}\n",
    "    bigram_components = {}\n",
    "    \n",
    "    # For multi-word terms, track their components\n",
    "    for word in target_words:\n",
    "        if '_' in word:\n",
    "            components = word.split('_')\n",
    "            bigram_components[word] = components\n",
    "    \n",
    "    # Process comments\n",
    "    with open(data_path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(data_path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Skipping unknown file {data_path}\")\n",
    "            return\n",
    "        \n",
    "        # Process up to 100,000 comments to find examples\n",
    "        for idx, row in enumerate(tqdm(jsonStream, desc=\"Processing comments\")):\n",
    "            if idx > 100000:  # Limit for performance\n",
    "                break\n",
    "                \n",
    "            if \"body\" not in row or \"created_utc\" not in row or \"author\" not in row:\n",
    "                continue\n",
    "                \n",
    "            # Get the comment text\n",
    "            text = row[\"body\"]\n",
    "            author = row.get('author', 'unknown')\n",
    "            created_utc = row.get('created_utc', 'unknown')\n",
    "            \n",
    "            # 1. Original text\n",
    "            original_text = text\n",
    "            \n",
    "            # 2. After URL removal and lowercase\n",
    "            cleaned_text = re.sub(r'http\\S+', '', text)\n",
    "            cleaned_text = re.sub(\"[^A-Za-z']+\", ' ', cleaned_text).lower()\n",
    "            cleaned_text = re.sub(r\"['\\-_]\", ' ', cleaned_text)\n",
    "            \n",
    "            # 3. After lemmatization\n",
    "            processed_words = preprocess_text(text, without_stopwords=False)\n",
    "            lemmatized_text = \" \".join(processed_words)\n",
    "            \n",
    "            # Create a comment record\n",
    "            comment_record = {\n",
    "                \"original_text\": original_text,\n",
    "                \"cleaned_text\": cleaned_text,\n",
    "                \"lemmatized_text\": lemmatized_text,\n",
    "                \"processed_words\": processed_words,\n",
    "                \"author\": author,\n",
    "                \"created_utc\": created_utc\n",
    "            }\n",
    "            \n",
    "            # Check if any target word components are in this comment\n",
    "            for word, components in bigram_components.items():\n",
    "                # Check if all components appear in the lemmatized text\n",
    "                if all(comp.lower() in lemmatized_text for comp in components):\n",
    "                    if len(word_occurrences[word]) < 5:  # Limit to 5 examples per word\n",
    "                        word_occurrences[word].append(comment_record)\n",
    "            \n",
    "            # Also check for exact word matches\n",
    "            for word in target_words:\n",
    "                # Skip if we already have enough examples\n",
    "                if len(word_occurrences[word]) >= 5:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if the word is in the lemmatized text\n",
    "                if word in lemmatized_text or word.replace('_', ' ') in lemmatized_text:\n",
    "                    word_occurrences[word].append(comment_record)\n",
    "            \n",
    "            # Keep track of all comments for further analysis\n",
    "            all_comments.append(comment_record)\n",
    "            \n",
    "            # Break if we found examples for all words\n",
    "            if all(len(examples) >= 5 for examples in word_occurrences.values()):\n",
    "                break\n",
    "    \n",
    "    # Prepare results for output\n",
    "    results = {\n",
    "        \"model_info\": {\n",
    "            \"path\": model_path,\n",
    "            \"vocabulary_size\": len(model.wv.index_to_key),\n",
    "            \"target_words_in_model\": words_in_model,\n",
    "            \"target_words_not_in_model\": words_not_in_model\n",
    "        },\n",
    "        \"word_examples\": {}\n",
    "    }\n",
    "    \n",
    "    # For each target word, show the comments and processing steps\n",
    "    for word in target_words:\n",
    "        examples = word_occurrences[word]\n",
    "        \n",
    "        word_results = {\n",
    "            \"word\": word,\n",
    "            \"is_bigram\": '_' in word,\n",
    "            \"components\": word.split('_') if '_' in word else [word],\n",
    "            \"found_in_comments\": len(examples) > 0,\n",
    "            \"examples\": []\n",
    "        }\n",
    "        \n",
    "        for example in examples:\n",
    "            # Find where the word components appear in the processed words\n",
    "            if '_' in word:\n",
    "                components = word.split('_')\n",
    "                component_indices = []\n",
    "                \n",
    "                for comp in components:\n",
    "                    indices = [i for i, w in enumerate(example[\"processed_words\"]) if w == comp.lower()]\n",
    "                    if indices:\n",
    "                        component_indices.append((comp, indices))\n",
    "                \n",
    "                word_results[\"examples\"].append({\n",
    "                    \"original_text\": example[\"original_text\"],\n",
    "                    \"cleaned_text\": example[\"cleaned_text\"],\n",
    "                    \"lemmatized_text\": example[\"lemmatized_text\"],\n",
    "                    \"component_locations\": component_indices,\n",
    "                    \"author\": example[\"author\"],\n",
    "                    \"created_utc\": example[\"created_utc\"]\n",
    "                })\n",
    "            else:\n",
    "                # For single words\n",
    "                indices = [i for i, w in enumerate(example[\"processed_words\"]) if w == word.lower()]\n",
    "                \n",
    "                word_results[\"examples\"].append({\n",
    "                    \"original_text\": example[\"original_text\"],\n",
    "                    \"cleaned_text\": example[\"cleaned_text\"],\n",
    "                    \"lemmatized_text\": example[\"lemmatized_text\"],\n",
    "                    \"word_locations\": indices,\n",
    "                    \"author\": example[\"author\"],\n",
    "                    \"created_utc\": example[\"created_utc\"]\n",
    "                })\n",
    "        \n",
    "        results[\"word_examples\"][word] = word_results\n",
    "    \n",
    "    # Save the analysis results\n",
    "    with open(\"output/gibberish_analysis/gibberish_word_analysis.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Generate a readable report\n",
    "    with open(\"output/gibberish_analysis/gibberish_word_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"ANALYSIS OF GIBBERISH WORDS\\n\")\n",
    "        f.write(\"==========================\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Model path: {model_path}\\n\")\n",
    "        f.write(f\"Vocabulary size: {len(model.wv.index_to_key)}\\n\")\n",
    "        f.write(f\"Target words in model: {len(words_in_model)} out of {len(target_words)}\\n\")\n",
    "        if words_not_in_model:\n",
    "            f.write(f\"Words not found in model: {', '.join(words_not_in_model)}\\n\\n\")\n",
    "        \n",
    "        # Create a summary of findings\n",
    "        words_with_examples = sum(1 for w in results[\"word_examples\"].values() if w[\"found_in_comments\"])\n",
    "        f.write(f\"Words with examples found: {words_with_examples} out of {len(target_words)}\\n\\n\")\n",
    "        \n",
    "        # Report on each word\n",
    "        for word, word_data in results[\"word_examples\"].items():\n",
    "            f.write(f\"WORD: {word}\\n\")\n",
    "            f.write(f\"{'=' * (len(word) + 6)}\\n\")\n",
    "            f.write(f\"Is bigram: {word_data['is_bigram']}\\n\")\n",
    "            f.write(f\"Components: {', '.join(word_data['components'])}\\n\")\n",
    "            f.write(f\"Found in comments: {word_data['found_in_comments']}\\n\")\n",
    "            f.write(f\"Number of examples: {len(word_data['examples'])}\\n\\n\")\n",
    "            \n",
    "            # Show examples\n",
    "            for i, example in enumerate(word_data[\"examples\"]):\n",
    "                f.write(f\"Example {i+1}:\\n\")\n",
    "                f.write(f\"Original text: {example['original_text'][:200]}...\\n\")\n",
    "                f.write(f\"Cleaned text: {example['cleaned_text'][:200]}...\\n\")\n",
    "                f.write(f\"Lemmatized text: {example['lemmatized_text'][:200]}...\\n\")\n",
    "                \n",
    "                if word_data[\"is_bigram\"]:\n",
    "                    f.write(\"Component locations:\\n\")\n",
    "                    for comp, indices in example.get(\"component_locations\", []):\n",
    "                        f.write(f\"  - {comp}: positions {indices}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"Word locations: positions {example.get('word_locations', [])}\\n\")\n",
    "                \n",
    "                f.write(f\"Author: {example['author']}\\n\")\n",
    "                f.write(f\"Timestamp: {example['created_utc']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"\\n\" + \"-\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"Analysis complete. Results saved to output/gibberish_analysis/\")\n",
    "    print(f\"Words with examples found: {words_with_examples} out of {len(target_words)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_gibberish_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14ef4020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from models/reddit_word2vec_10_10_filterd_democrats_2021_2024.model\n",
      "Analyzing 50 words from dem_10_10_list.csv\n",
      "Processing comments from datasets/democrats_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments: 500000it [04:15, 1959.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with exact matches: 23 out of 50\n",
      "Words with only partial matches: 16 out of 50\n",
      "Analysis complete. Results saved to output/exact_word_matches/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from fileStreams import getFileJsonStream\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# Initialize lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"output/exact_word_matches\", exist_ok=True)\n",
    "\n",
    "def preprocess_text(text, without_stopwords=True):\n",
    "    \"\"\"Process text the same way as in model training\"\"\"\n",
    "    # Remove URLs\n",
    "    txt = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    txt = re.sub(\"[^A-Za-z']+\", ' ', txt).lower()\n",
    "    \n",
    "    # Replace special characters with spaces\n",
    "    txt = re.sub(r\"['\\-_]\", ' ', txt)\n",
    "    \n",
    "    # Tokenize\n",
    "    words = txt.split()\n",
    "    \n",
    "    # Tag words with parts of speech for better lemmatization\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    processed_words = []\n",
    "    \n",
    "    for word, tag in tagged_words:\n",
    "        if without_stopwords and word in stop_words:\n",
    "            continue\n",
    "        \n",
    "        # Convert Penn Treebank tag to WordNet tag\n",
    "        if tag.startswith('J'):\n",
    "            wordnet_pos = 'a'  # adjective\n",
    "        elif tag.startswith('V'):\n",
    "            wordnet_pos = 'v'  # verb\n",
    "        elif tag.startswith('N'):\n",
    "            wordnet_pos = 'n'  # noun\n",
    "        elif tag.startswith('R'):\n",
    "            wordnet_pos = 'r'  # adverb\n",
    "        else:\n",
    "            wordnet_pos = 'n'  # default to noun\n",
    "                \n",
    "        # Lemmatize with the correct POS\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "        processed_words.append(lemma)\n",
    "    \n",
    "    return processed_words\n",
    "\n",
    "def check_exact_word_match(word, processed_words):\n",
    "    \"\"\"Check if a word appears exactly in processed words (not as part of another word)\"\"\"\n",
    "    if '_' in word:\n",
    "        # For bigrams, check if components appear as separate words\n",
    "        components = word.split('_')\n",
    "        return all(comp in processed_words for comp in components)\n",
    "    else:\n",
    "        # For single words, check for exact match\n",
    "        return word in processed_words\n",
    "\n",
    "def find_exact_word_matches():\n",
    "    # Load the model to check vocabulary\n",
    "    model_path = \"models/reddit_word2vec_10_10_filterd_democrats_2021_2024.model\"\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    model = Word2Vec.load(model_path)\n",
    "    \n",
    "    # Load the top 50 source words\n",
    "    df = pd.read_csv('output/dem_10_10_list.csv')\n",
    "    target_words = df['source'].head(50).tolist()\n",
    "    \n",
    "    print(f\"Analyzing {len(target_words)} words from dem_10_10_list.csv\")\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {\n",
    "        \"model_info\": {\n",
    "            \"path\": model_path,\n",
    "            \"vocabulary_size\": len(model.wv.index_to_key),\n",
    "            \"top50_words\": target_words\n",
    "        },\n",
    "        \"word_analysis\": {}\n",
    "    }\n",
    "    \n",
    "    # Initialize analysis for each word\n",
    "    for word in target_words:\n",
    "        results[\"word_analysis\"][word] = {\n",
    "            \"word\": word,\n",
    "            \"in_model_vocab\": word in model.wv,\n",
    "            \"exact_matches\": [],\n",
    "            \"partial_matches\": [],\n",
    "            \"components\": word.split('_') if '_' in word else [word]\n",
    "        }\n",
    "    \n",
    "    # Process comments to find exact and partial matches\n",
    "    data_path = \"datasets/democrats_comments.zst\"\n",
    "    print(f\"Processing comments from {data_path}\")\n",
    "    \n",
    "    # Set maximum comments to process and examples to find per word\n",
    "    max_comments = 500000\n",
    "    max_examples = 5\n",
    "    \n",
    "    # Track if we have found enough examples for all words\n",
    "    all_found_exact = False\n",
    "    \n",
    "    # Process comments\n",
    "    with open(data_path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(data_path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Skipping unknown file {data_path}\")\n",
    "            return\n",
    "        \n",
    "        for idx, row in enumerate(tqdm(jsonStream, desc=\"Processing comments\")):\n",
    "            if idx >= max_comments:\n",
    "                break\n",
    "                \n",
    "            if \"body\" not in row or \"created_utc\" not in row or \"author\" not in row:\n",
    "                continue\n",
    "                \n",
    "            # Get the comment text\n",
    "            text = row[\"body\"]\n",
    "            author = row.get('author', 'unknown')\n",
    "            created_utc = row.get('created_utc', 'unknown')\n",
    "            year = datetime.datetime.fromtimestamp(int(created_utc)).year if isinstance(created_utc, (int, float)) else \"unknown\"\n",
    "            \n",
    "            # Process text\n",
    "            processed_words = preprocess_text(text, without_stopwords=False)\n",
    "            processed_text = \" \".join(processed_words)\n",
    "            \n",
    "            # Check for each target word\n",
    "            for word in target_words:\n",
    "                word_data = results[\"word_analysis\"][word]\n",
    "                \n",
    "                # Check for exact match as a whole token\n",
    "                if len(word_data[\"exact_matches\"]) < max_examples:\n",
    "                    exact_match = check_exact_word_match(word, processed_words)\n",
    "                    if exact_match:\n",
    "                        word_data[\"exact_matches\"].append({\n",
    "                            \"original_text\": text,\n",
    "                            \"processed_text\": processed_text,\n",
    "                            \"processed_words\": processed_words,\n",
    "                            \"author\": author,\n",
    "                            \"created_utc\": created_utc,\n",
    "                            \"year\": year\n",
    "                        })\n",
    "                \n",
    "                # Check for partial match (substring)\n",
    "                if len(word_data[\"partial_matches\"]) < max_examples:\n",
    "                    # For words with underscores, check each component separately\n",
    "                    if '_' in word:\n",
    "                        components = word.split('_')\n",
    "                        partial_match = all(any(comp in w for w in processed_words) for comp in components)\n",
    "                    else:\n",
    "                        # For single words, check if it's a substring of any processed word\n",
    "                        partial_match = any(word in w and word != w for w in processed_words)\n",
    "                    \n",
    "                    if partial_match and not exact_match:  # Only count as partial if not already an exact match\n",
    "                        word_data[\"partial_matches\"].append({\n",
    "                            \"original_text\": text,\n",
    "                            \"processed_text\": processed_text,\n",
    "                            \"processed_words\": processed_words,\n",
    "                            \"author\": author,\n",
    "                            \"created_utc\": created_utc,\n",
    "                            \"year\": year\n",
    "                        })\n",
    "            \n",
    "            # Check if we have enough examples for all words\n",
    "            all_found_exact = all(len(results[\"word_analysis\"][word][\"exact_matches\"]) >= max_examples for word in target_words)\n",
    "            \n",
    "            # If we have found enough examples for all words, stop processing\n",
    "            if all_found_exact:\n",
    "                print(f\"Found enough exact matches for all words after processing {idx+1} comments.\")\n",
    "                break\n",
    "    \n",
    "    # Analyze and summarize results\n",
    "    exact_match_counts = {word: len(data[\"exact_matches\"]) for word, data in results[\"word_analysis\"].items()}\n",
    "    partial_match_counts = {word: len(data[\"partial_matches\"]) for word, data in results[\"word_analysis\"].items()}\n",
    "    \n",
    "    words_with_exact_matches = sum(1 for count in exact_match_counts.values() if count > 0)\n",
    "    words_with_partial_matches = sum(1 for count in partial_match_counts.values() if count > 0)\n",
    "    \n",
    "    print(f\"Words with exact matches: {words_with_exact_matches} out of {len(target_words)}\")\n",
    "    print(f\"Words with only partial matches: {words_with_partial_matches} out of {len(target_words)}\")\n",
    "    \n",
    "    # Add summary to results\n",
    "    results[\"summary\"] = {\n",
    "        \"words_with_exact_matches\": words_with_exact_matches,\n",
    "        \"words_with_partial_matches\": words_with_partial_matches,\n",
    "        \"words_with_no_matches\": len(target_words) - words_with_exact_matches - words_with_partial_matches\n",
    "    }\n",
    "    \n",
    "    # Save results as JSON\n",
    "    with open(\"output/exact_word_matches/word_match_analysis.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Generate a readable report\n",
    "    with open(\"output/exact_word_matches/word_match_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"ANALYSIS OF TOP 50 WORDS - EXACT VS PARTIAL MATCHES\\n\")\n",
    "        f.write(\"=================================================\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Model path: {model_path}\\n\")\n",
    "        f.write(f\"Vocabulary size: {len(model.wv.index_to_key)}\\n\\n\")\n",
    "        \n",
    "        f.write(\"SUMMARY\\n\")\n",
    "        f.write(\"=======\\n\")\n",
    "        f.write(f\"Words with exact matches: {words_with_exact_matches} out of {len(target_words)}\\n\")\n",
    "        f.write(f\"Words with only partial matches: {words_with_partial_matches} out of {len(target_words)}\\n\")\n",
    "        f.write(f\"Words with no matches: {len(target_words) - words_with_exact_matches - words_with_partial_matches} out of {len(target_words)}\\n\\n\")\n",
    "        \n",
    "        # Words without any matches\n",
    "        no_match_words = [word for word in target_words if len(results[\"word_analysis\"][word][\"exact_matches\"]) == 0 and \n",
    "                           len(results[\"word_analysis\"][word][\"partial_matches\"]) == 0]\n",
    "        if no_match_words:\n",
    "            f.write(\"WORDS WITH NO MATCHES\\n\")\n",
    "            f.write(\"====================\\n\")\n",
    "            for word in no_match_words:\n",
    "                f.write(f\"- {word}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Words that appear only as part of other words\n",
    "        partial_only_words = [word for word in target_words if len(results[\"word_analysis\"][word][\"exact_matches\"]) == 0 and \n",
    "                               len(results[\"word_analysis\"][word][\"partial_matches\"]) > 0]\n",
    "        if partial_only_words:\n",
    "            f.write(\"WORDS APPEARING ONLY AS PART OF OTHER WORDS\\n\")\n",
    "            f.write(\"========================================\\n\")\n",
    "            for word in partial_only_words:\n",
    "                f.write(f\"- {word}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Words with exact matches\n",
    "        exact_match_words = [word for word in target_words if len(results[\"word_analysis\"][word][\"exact_matches\"]) > 0]\n",
    "        if exact_match_words:\n",
    "            f.write(\"WORDS WITH EXACT MATCHES\\n\")\n",
    "            f.write(\"=======================\\n\")\n",
    "            for word in exact_match_words:\n",
    "                f.write(f\"- {word} ({len(results['word_analysis'][word]['exact_matches'])} examples)\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Detailed examples for each word\n",
    "        f.write(\"DETAILED EXAMPLES\\n\")\n",
    "        f.write(\"================\\n\\n\")\n",
    "        \n",
    "        for word in target_words:\n",
    "            word_data = results[\"word_analysis\"][word]\n",
    "            f.write(f\"WORD: {word}\\n\")\n",
    "            f.write(f\"{'=' * (len(word) + 6)}\\n\")\n",
    "            f.write(f\"In model vocabulary: {word_data['in_model_vocab']}\\n\")\n",
    "            f.write(f\"Components: {', '.join(word_data['components'])}\\n\")\n",
    "            f.write(f\"Exact matches: {len(word_data['exact_matches'])}\\n\")\n",
    "            f.write(f\"Partial matches: {len(word_data['partial_matches'])}\\n\\n\")\n",
    "            \n",
    "            # Show exact match examples\n",
    "            if word_data[\"exact_matches\"]:\n",
    "                f.write(\"EXACT MATCHES:\\n\")\n",
    "                f.write(\"-------------\\n\")\n",
    "                for i, example in enumerate(word_data[\"exact_matches\"]):\n",
    "                    f.write(f\"Example {i+1} (Year: {example['year']}):\\n\")\n",
    "                    f.write(f\"Original: {example['original_text'][:200]}...\\n\" if len(example['original_text']) > 200 \n",
    "                           else f\"Original: {example['original_text']}\\n\")\n",
    "                    f.write(f\"Processed: {example['processed_text'][:200]}...\\n\" if len(example['processed_text']) > 200 \n",
    "                           else f\"Processed: {example['processed_text']}\\n\")\n",
    "                    f.write(f\"Tokens: {example['processed_words']}\\n\")\n",
    "                    f.write(f\"Author: {example['author']}\\n\\n\")\n",
    "            \n",
    "            # Show partial match examples\n",
    "            if word_data[\"partial_matches\"]:\n",
    "                f.write(\"PARTIAL MATCHES:\\n\")\n",
    "                f.write(\"---------------\\n\")\n",
    "                for i, example in enumerate(word_data[\"partial_matches\"]):\n",
    "                    f.write(f\"Example {i+1} (Year: {example['year']}):\\n\")\n",
    "                    f.write(f\"Original: {example['original_text'][:200]}...\\n\" if len(example['original_text']) > 200 \n",
    "                           else f\"Original: {example['original_text']}\\n\")\n",
    "                    f.write(f\"Processed: {example['processed_text'][:200]}...\\n\" if len(example['processed_text']) > 200 \n",
    "                           else f\"Processed: {example['processed_text']}\\n\")\n",
    "                    f.write(f\"Tokens: {example['processed_words']}\\n\")\n",
    "                    f.write(f\"Author: {example['author']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"\\n\" + \"-\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"Analysis complete. Results saved to output/exact_word_matches/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    find_exact_word_matches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4c7b206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from models/model_v3/reddit_word2vec_10_10_democrats_2021_2024.model\n",
      "Analyzing 50 words from dem_10_10_list.csv\n",
      "Processing comments from datasets/democrats_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments: 500000it [04:28, 1862.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with exact matches: 30 out of 50\n",
      "Words with only partial matches: 19 out of 50\n",
      "Analysis complete. Results saved to output/exact_word_matches/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from fileStreams import getFileJsonStream\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import datetime\n",
    "import html\n",
    "import unicodedata\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# Initialize lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"output/exact_word_matches\", exist_ok=True)\n",
    "\n",
    "def preprocess_reddit_text(text):\n",
    "    \"\"\"Specialized preprocessing for Reddit content before regular text processing\"\"\"\n",
    "    # Handle HTML entities\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Handle Unicode normalization\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Remove all URLs and images\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)\n",
    "    \n",
    "    # Handle Reddit's link format\n",
    "    text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)\n",
    "    \n",
    "    # Handle markdown formatting (bold, italics)\n",
    "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)\n",
    "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)\n",
    "    \n",
    "    # Handle Reddit single character bold/emphasis that causes g_aslight issue\n",
    "    text = re.sub(r'\\*\\*([A-Za-z])\\*\\*([A-Za-z]+)', r'\\1\\2', text)\n",
    "    \n",
    "    # Handle subreddit and user references\n",
    "    text = re.sub(r'/r/(\\w+)', r'subreddit_\\1', text)\n",
    "    text = re.sub(r'/u/(\\w+)', r'user_\\1', text)\n",
    "    \n",
    "    # Handle quote markers and other Reddit-specific formatting\n",
    "    text = re.sub(r'&gt;', ' ', text)  # Quote marker\n",
    "    text = re.sub(r'\\^', ' ', text)     # Superscript marker\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_text(text, without_stopwords=True):\n",
    "    \"\"\"Process text the same way as in model training\"\"\"\n",
    "    # Apply Reddit-specific preprocessing first\n",
    "    txt = preprocess_reddit_text(text)\n",
    "    \n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    txt = re.sub(\"[^A-Za-z']+\", ' ', txt).lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    words = txt.split()\n",
    "    \n",
    "    # Tag words with parts of speech for better lemmatization\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    processed_words = []\n",
    "    \n",
    "    for word, tag in tagged_words:\n",
    "        if without_stopwords and word in stop_words:\n",
    "            continue\n",
    "        \n",
    "        # Convert Penn Treebank tag to WordNet tag\n",
    "        if tag.startswith('J'):\n",
    "            wordnet_pos = 'a'  # adjective\n",
    "        elif tag.startswith('V'):\n",
    "            wordnet_pos = 'v'  # verb\n",
    "        elif tag.startswith('N'):\n",
    "            wordnet_pos = 'n'  # noun\n",
    "        elif tag.startswith('R'):\n",
    "            wordnet_pos = 'r'  # adverb\n",
    "        else:\n",
    "            wordnet_pos = 'n'  # default to noun\n",
    "                \n",
    "        # Lemmatize with the correct POS\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "        processed_words.append(lemma)\n",
    "    \n",
    "    return processed_words\n",
    "\n",
    "def check_exact_word_match(word, processed_words):\n",
    "    \"\"\"Check if a word appears exactly in processed words (not as part of another word)\"\"\"\n",
    "    if '_' in word:\n",
    "        # For bigrams, check if components appear as separate words\n",
    "        components = word.split('_')\n",
    "        return all(comp in processed_words for comp in components)\n",
    "    else:\n",
    "        # For single words, check for exact match\n",
    "        return word in processed_words\n",
    "\n",
    "def find_exact_word_matches():\n",
    "    # Load the model to check vocabulary\n",
    "    model_path = \"models/model_v3/reddit_word2vec_10_10_democrats_2021_2024.model\"\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    model = Word2Vec.load(model_path)\n",
    "    \n",
    "    # Load the top 50 source words\n",
    "    df = pd.read_csv('output/v3/dem_10_10_v3_list.csv')\n",
    "    target_words = df['source'].head(50).tolist()\n",
    "    \n",
    "    print(f\"Analyzing {len(target_words)} words from dem_10_10_list.csv\")\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {\n",
    "        \"model_info\": {\n",
    "            \"path\": model_path,\n",
    "            \"vocabulary_size\": len(model.wv.index_to_key),\n",
    "            \"top50_words\": target_words\n",
    "        },\n",
    "        \"word_analysis\": {}\n",
    "    }\n",
    "    \n",
    "    # Initialize analysis for each word\n",
    "    for word in target_words:\n",
    "        results[\"word_analysis\"][word] = {\n",
    "            \"word\": word,\n",
    "            \"in_model_vocab\": word in model.wv,\n",
    "            \"exact_matches\": [],\n",
    "            \"partial_matches\": [],\n",
    "            \"components\": word.split('_') if '_' in word else [word]\n",
    "        }\n",
    "    \n",
    "    # Process comments to find exact and partial matches\n",
    "    data_path = \"datasets/democrats_comments.zst\"\n",
    "    print(f\"Processing comments from {data_path}\")\n",
    "    \n",
    "    # Set maximum comments to process and examples to find per word\n",
    "    max_comments = 500000\n",
    "    max_examples = 5\n",
    "    \n",
    "    # Track if we have found enough examples for all words\n",
    "    all_found_exact = False\n",
    "    \n",
    "    # Process comments\n",
    "    with open(data_path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(data_path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Skipping unknown file {data_path}\")\n",
    "            return\n",
    "        \n",
    "        for idx, row in enumerate(tqdm(jsonStream, desc=\"Processing comments\")):\n",
    "            if idx >= max_comments:\n",
    "                break\n",
    "                \n",
    "            if \"body\" not in row or \"created_utc\" not in row or \"author\" not in row:\n",
    "                continue\n",
    "                \n",
    "            # Get the comment text\n",
    "            text = row[\"body\"]\n",
    "            author = row.get('author', 'unknown')\n",
    "            created_utc = row.get('created_utc', 'unknown')\n",
    "            year = datetime.datetime.fromtimestamp(int(created_utc)).year if isinstance(created_utc, (int, float)) else \"unknown\"\n",
    "            \n",
    "            # Process text\n",
    "            processed_words = preprocess_text(text, without_stopwords=False)\n",
    "            processed_text = \" \".join(processed_words)\n",
    "            \n",
    "            # Check for each target word\n",
    "            for word in target_words:\n",
    "                word_data = results[\"word_analysis\"][word]\n",
    "                \n",
    "                # Check for exact match as a whole token\n",
    "                if len(word_data[\"exact_matches\"]) < max_examples:\n",
    "                    exact_match = check_exact_word_match(word, processed_words)\n",
    "                    if exact_match:\n",
    "                        word_data[\"exact_matches\"].append({\n",
    "                            \"original_text\": text,\n",
    "                            \"processed_text\": processed_text,\n",
    "                            \"processed_words\": processed_words,\n",
    "                            \"author\": author,\n",
    "                            \"created_utc\": created_utc,\n",
    "                            \"year\": year\n",
    "                        })\n",
    "                \n",
    "                # Check for partial match (substring)\n",
    "                if len(word_data[\"partial_matches\"]) < max_examples:\n",
    "                    # For words with underscores, check each component separately\n",
    "                    if '_' in word:\n",
    "                        components = word.split('_')\n",
    "                        partial_match = all(any(comp in w for w in processed_words) for comp in components)\n",
    "                    else:\n",
    "                        # For single words, check if it's a substring of any processed word\n",
    "                        partial_match = any(word in w and word != w for w in processed_words)\n",
    "                    \n",
    "                    if partial_match and not exact_match:  # Only count as partial if not already an exact match\n",
    "                        word_data[\"partial_matches\"].append({\n",
    "                            \"original_text\": text,\n",
    "                            \"processed_text\": processed_text,\n",
    "                            \"processed_words\": processed_words,\n",
    "                            \"author\": author,\n",
    "                            \"created_utc\": created_utc,\n",
    "                            \"year\": year\n",
    "                        })\n",
    "            \n",
    "            # Check if we have enough examples for all words\n",
    "            all_found_exact = all(len(results[\"word_analysis\"][word][\"exact_matches\"]) >= max_examples for word in target_words)\n",
    "            \n",
    "            # If we have found enough examples for all words, stop processing\n",
    "            if all_found_exact:\n",
    "                print(f\"Found enough exact matches for all words after processing {idx+1} comments.\")\n",
    "                break\n",
    "    \n",
    "    # Analyze and summarize results\n",
    "    exact_match_counts = {word: len(data[\"exact_matches\"]) for word, data in results[\"word_analysis\"].items()}\n",
    "    partial_match_counts = {word: len(data[\"partial_matches\"]) for word, data in results[\"word_analysis\"].items()}\n",
    "    \n",
    "    words_with_exact_matches = sum(1 for count in exact_match_counts.values() if count > 0)\n",
    "    words_with_partial_matches = sum(1 for count in partial_match_counts.values() if count > 0)\n",
    "    \n",
    "    print(f\"Words with exact matches: {words_with_exact_matches} out of {len(target_words)}\")\n",
    "    print(f\"Words with only partial matches: {words_with_partial_matches} out of {len(target_words)}\")\n",
    "    \n",
    "    # Add summary to results\n",
    "    results[\"summary\"] = {\n",
    "        \"words_with_exact_matches\": words_with_exact_matches,\n",
    "        \"words_with_partial_matches\": words_with_partial_matches,\n",
    "        \"words_with_no_matches\": len(target_words) - words_with_exact_matches - words_with_partial_matches\n",
    "    }\n",
    "    \n",
    "    # Save results as JSON\n",
    "    with open(\"output/exact_word_matches/word_match_analysis_v3.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Generate a readable report\n",
    "    with open(\"output/exact_word_matches/word_match_report_v3.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"ANALYSIS OF TOP 50 WORDS - EXACT VS PARTIAL MATCHES\\n\")\n",
    "        f.write(\"=================================================\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Model path: {model_path}\\n\")\n",
    "        f.write(f\"Vocabulary size: {len(model.wv.index_to_key)}\\n\\n\")\n",
    "        \n",
    "        f.write(\"SUMMARY\\n\")\n",
    "        f.write(\"=======\\n\")\n",
    "        f.write(f\"Words with exact matches: {words_with_exact_matches} out of {len(target_words)}\\n\")\n",
    "        f.write(f\"Words with only partial matches: {words_with_partial_matches} out of {len(target_words)}\\n\")\n",
    "        f.write(f\"Words with no matches: {len(target_words) - words_with_exact_matches - words_with_partial_matches} out of {len(target_words)}\\n\\n\")\n",
    "        \n",
    "        # Words without any matches\n",
    "        no_match_words = [word for word in target_words if len(results[\"word_analysis\"][word][\"exact_matches\"]) == 0 and \n",
    "                           len(results[\"word_analysis\"][word][\"partial_matches\"]) == 0]\n",
    "        if no_match_words:\n",
    "            f.write(\"WORDS WITH NO MATCHES\\n\")\n",
    "            f.write(\"====================\\n\")\n",
    "            for word in no_match_words:\n",
    "                f.write(f\"- {word}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Words that appear only as part of other words\n",
    "        partial_only_words = [word for word in target_words if len(results[\"word_analysis\"][word][\"exact_matches\"]) == 0 and \n",
    "                               len(results[\"word_analysis\"][word][\"partial_matches\"]) > 0]\n",
    "        if partial_only_words:\n",
    "            f.write(\"WORDS APPEARING ONLY AS PART OF OTHER WORDS\\n\")\n",
    "            f.write(\"========================================\\n\")\n",
    "            for word in partial_only_words:\n",
    "                f.write(f\"- {word}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Words with exact matches\n",
    "        exact_match_words = [word for word in target_words if len(results[\"word_analysis\"][word][\"exact_matches\"]) > 0]\n",
    "        if exact_match_words:\n",
    "            f.write(\"WORDS WITH EXACT MATCHES\\n\")\n",
    "            f.write(\"=======================\\n\")\n",
    "            for word in exact_match_words:\n",
    "                f.write(f\"- {word} ({len(results['word_analysis'][word]['exact_matches'])} examples)\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Detailed examples for each word\n",
    "        f.write(\"DETAILED EXAMPLES\\n\")\n",
    "        f.write(\"================\\n\\n\")\n",
    "        \n",
    "        for word in target_words:\n",
    "            word_data = results[\"word_analysis\"][word]\n",
    "            f.write(f\"WORD: {word}\\n\")\n",
    "            f.write(f\"{'=' * (len(word) + 6)}\\n\")\n",
    "            f.write(f\"In model vocabulary: {word_data['in_model_vocab']}\\n\")\n",
    "            f.write(f\"Components: {', '.join(word_data['components'])}\\n\")\n",
    "            f.write(f\"Exact matches: {len(word_data['exact_matches'])}\\n\")\n",
    "            f.write(f\"Partial matches: {len(word_data['partial_matches'])}\\n\\n\")\n",
    "            \n",
    "            # Show exact match examples\n",
    "            if word_data[\"exact_matches\"]:\n",
    "                f.write(\"EXACT MATCHES:\\n\")\n",
    "                f.write(\"-------------\\n\")\n",
    "                for i, example in enumerate(word_data[\"exact_matches\"]):\n",
    "                    f.write(f\"Example {i+1} (Year: {example['year']}):\\n\")\n",
    "                    f.write(f\"Original: {example['original_text'][:200]}...\\n\" if len(example['original_text']) > 200 \n",
    "                           else f\"Original: {example['original_text']}\\n\")\n",
    "                    f.write(f\"Processed: {example['processed_text'][:200]}...\\n\" if len(example['processed_text']) > 200 \n",
    "                           else f\"Processed: {example['processed_text']}\\n\")\n",
    "                    f.write(f\"Tokens: {example['processed_words']}\\n\")\n",
    "                    f.write(f\"Author: {example['author']}\\n\\n\")\n",
    "            \n",
    "            # Show partial match examples\n",
    "            if word_data[\"partial_matches\"]:\n",
    "                f.write(\"PARTIAL MATCHES:\\n\")\n",
    "                f.write(\"---------------\\n\")\n",
    "                for i, example in enumerate(word_data[\"partial_matches\"]):\n",
    "                    f.write(f\"Example {i+1} (Year: {example['year']}):\\n\")\n",
    "                    f.write(f\"Original: {example['original_text'][:200]}...\\n\" if len(example['original_text']) > 200 \n",
    "                           else f\"Original: {example['original_text']}\\n\")\n",
    "                    f.write(f\"Processed: {example['processed_text'][:200]}...\\n\" if len(example['processed_text']) > 200 \n",
    "                           else f\"Processed: {example['processed_text']}\\n\")\n",
    "                    f.write(f\"Tokens: {example['processed_words']}\\n\")\n",
    "                    f.write(f\"Author: {example['author']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"\\n\" + \"-\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    print(f\"Analysis complete. Results saved to output/exact_word_matches/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    find_exact_word_matches()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

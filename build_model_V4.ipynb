{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a65927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For streaming\n",
    "import sys\n",
    "version = sys.version_info\n",
    "if version.major < 3 or (version.major == 3 and version.minor < 10):\n",
    "    raise RuntimeError(\"This script requires Python 3.10 or higher\")\n",
    "import os\n",
    "from typing import Iterable\n",
    "\n",
    "from fileStreams import getFileJsonStream\n",
    "from utils import FileProgressLog\n",
    "\n",
    "# For processing\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import html\n",
    "import unicodedata\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "    \n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "recursive = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4dc6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file datasets/Liberal_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments: 497079it [04:02, 2045.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Comment Counts by Period ===\n",
      "before_2016: 131482 comments\n",
      "2017_2020: 145788 comments\n",
      "2021_2024: 213308 comments\n",
      "\n",
      "Processing final batch of 131482 comments for before_2016...\n",
      "before_2016: Processing 131273 comments after filtering words by user count\n",
      "Extracting bigrams...\n",
      "Creating new Word2Vec model for before_2016\n",
      "Training model on 131273 comments\n",
      "Interim model saved to models/model_v4/interim/reddit_word2vec_10_10_liberal_before_2016_interim.model\n",
      "\n",
      "Processing final batch of 145788 comments for 2017_2020...\n",
      "2017_2020: Processing 145477 comments after filtering words by user count\n",
      "Extracting bigrams...\n",
      "Creating new Word2Vec model for 2017_2020\n",
      "Training model on 145477 comments\n",
      "Interim model saved to models/model_v4/interim/reddit_word2vec_10_10_liberal_2017_2020_interim.model\n",
      "\n",
      "Processing final batch of 213308 comments for 2021_2024...\n",
      "2021_2024: Processing 212844 comments after filtering words by user count\n",
      "Extracting bigrams...\n",
      "Creating new Word2Vec model for 2021_2024\n",
      "Training model on 212844 comments\n",
      "Interim model saved to models/model_v4/interim/reddit_word2vec_10_10_liberal_2021_2024_interim.model\n",
      "Done :>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_reddit_text(text):\n",
    "    # Handle HTML entities\n",
    "    text = html.unescape(text)                         # &amp; becomes &, etc.\n",
    "    \n",
    "    # Handle Unicode normalization\n",
    "    text = unicodedata.normalize('NFKD', text)         # converts café with a single combined \"é\" character to \"e\" with an accent\n",
    "    \n",
    "    # Remove all URLs\n",
    "    text = re.sub(r'http\\S+', '', text)                # Remove URLs\n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)        # Images/GIFs\n",
    "    \n",
    "    # Handle Reddit's link format\n",
    "    text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)    # [text](link) becomes text\n",
    "    \n",
    "    # Handle markdown formatting (bold, italics)\n",
    "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)       # **word** becomes word\n",
    "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)           # *word* becomes word\n",
    "    \n",
    "    # Handle subreddit and user references - both with and without leading slash\n",
    "    text = re.sub(r'/r/\\w+', '', text)                 # /r/politics becomes empty\n",
    "    text = re.sub(r'r/\\w+', '', text)                  # r/politics becomes empty\n",
    "    text = re.sub(r'/u/\\w+', '', text)                 # /u/username becomes empty\n",
    "    text = re.sub(r'u/\\w+', '', text)                  # u/username becomes empty\n",
    "    \n",
    "    # Remove time-related terms that create noise\n",
    "    # Time markers\n",
    "    text = re.sub(r'\\b(?:am|pm|AM|PM|a\\.m\\.|p\\.m\\.)\\b', '', text)\n",
    "    \n",
    "    # Days of week - both full and abbreviated forms\n",
    "    days_pattern = r'\\b(?:Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday|'\n",
    "    days_pattern += r'Mon|Tue|Tues|Wed|Thu|Thurs|Fri|Sat|Sun)\\b'\n",
    "    text = re.sub(days_pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Month names - both full and abbreviated forms\n",
    "    months_pattern = r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December|'\n",
    "    months_pattern += r'Jan|Feb|Mar|Apr|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)\\b'\n",
    "    text = re.sub(months_pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Time units that often appear in Reddit comments\n",
    "    time_units = r'\\b(?:second|minute|hour|day|week|month|year)s?\\b'\n",
    "    text = re.sub(time_units, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Basic text cleaning\n",
    "    text = re.sub(\"[^A-Za-z]+\", ' ', text).lower()\n",
    "\n",
    "    # Remove single letters except 'i'\n",
    "    text = re.sub(r'\\b([a-hj-z])\\b', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def build_word2vect_model(path, party, without_stopwords=True, phrases_min_count=5, word2vec_min_count=5, \n",
    "                          batch_size=1000000, save_interim=True):\n",
    "    print(f\"Processing file {path}\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # For each period, track comments and user-word usage\n",
    "    chunks = {\n",
    "        \"before_2016\": [],\n",
    "        \"2017_2020\": [],\n",
    "        \"2021_2024\": [],\n",
    "    }\n",
    "    user_words = {\n",
    "        \"before_2016\": defaultdict(set),\n",
    "        \"2017_2020\": defaultdict(set),\n",
    "        \"2021_2024\": defaultdict(set),\n",
    "    }\n",
    "    user_comments = {\n",
    "        \"before_2016\": defaultdict(list),\n",
    "        \"2017_2020\": defaultdict(list),\n",
    "        \"2021_2024\": defaultdict(list),\n",
    "    }\n",
    "    counts = {period: 0 for period in chunks.keys()}\n",
    "    \n",
    "    # Batch processing counters\n",
    "    batch_counts = {period: 0 for period in chunks.keys()}\n",
    "    models = {period: None for period in chunks.keys()}\n",
    "\n",
    "    with open(path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Skipping unknown file {path}\")\n",
    "            return\n",
    "            \n",
    "        for row in tqdm(jsonStream, desc=\"Processing comments\"):\n",
    "            if \"body\" not in row or \"created_utc\" not in row or \"author\" not in row:\n",
    "                continue\n",
    "            author = row[\"author\"]\n",
    "            if author in {\"AutoModerator\", \"election_info_bot\"}:\n",
    "                continue\n",
    "                \n",
    "            text = row[\"body\"]\n",
    "            created_timestamp = row[\"created_utc\"]\n",
    "            year = datetime.datetime.fromtimestamp(int(created_timestamp)).year\n",
    "            \n",
    "            if year <= 2016:\n",
    "                chunk_key = \"before_2016\"\n",
    "            elif 2017 <= year <= 2020:\n",
    "                chunk_key = \"2017_2020\"\n",
    "            elif 2021 <= year <= 2024:\n",
    "                chunk_key = \"2021_2024\"\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            text = preprocess_reddit_text(text)\n",
    "\n",
    "            words = text.split()\n",
    "            if not words:\n",
    "                continue\n",
    "                \n",
    "            tagged_words = nltk.pos_tag(words)\n",
    "            processed_words = []\n",
    "            \n",
    "            for word, tag in tagged_words:\n",
    "                if without_stopwords and word in stop_words:\n",
    "                    continue\n",
    "                if tag.startswith('J'):\n",
    "                    wordnet_pos = 'a'\n",
    "                elif tag.startswith('V'):\n",
    "                    wordnet_pos = 'v'\n",
    "                elif tag.startswith('N'):\n",
    "                    wordnet_pos = 'n'\n",
    "                elif tag.startswith('R'):\n",
    "                    wordnet_pos = 'r'\n",
    "                else:\n",
    "                    wordnet_pos = 'n'\n",
    "                lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "                processed_words.append(lemma)\n",
    "                user_words[chunk_key][lemma].add(author)\n",
    "                \n",
    "            if processed_words:\n",
    "                user_comments[chunk_key][author].append(processed_words)\n",
    "                counts[chunk_key] += 1\n",
    "                batch_counts[chunk_key] += 1\n",
    "            \n",
    "            # Process batch when we reach the batch size for any period\n",
    "            for period in chunks.keys():\n",
    "                if batch_counts[period] >= batch_size:\n",
    "                    print(f\"\\nReached {batch_size} comments for {period}, processing batch...\")\n",
    "                    process_batch(period, user_words[period], user_comments[period], \n",
    "                                 models[period], phrases_min_count, word2vec_min_count, party)\n",
    "                    \n",
    "                    # Clear batch data to free memory\n",
    "                    user_words[period] = defaultdict(set)\n",
    "                    user_comments[period] = defaultdict(list)\n",
    "                    batch_counts[period] = 0\n",
    "\n",
    "    print(\"\\n=== Final Comment Counts by Period ===\")\n",
    "    for period, count in counts.items():\n",
    "        print(f\"{period}: {count} comments\")\n",
    "    \n",
    "    # Process any remaining comments\n",
    "    for period in chunks.keys():\n",
    "        if batch_counts[period] > 0:\n",
    "            print(f\"\\nProcessing final batch of {batch_counts[period]} comments for {period}...\")\n",
    "            process_batch(period, user_words[period], user_comments[period], \n",
    "                         models[period], phrases_min_count, word2vec_min_count, party)\n",
    "\n",
    "    # Save final models\n",
    "    for period, model in models.items():\n",
    "        if model is not None:\n",
    "            final_path = f\"models/model_v4/reddit_word2vec_{phrases_min_count}_{word2vec_min_count}_{party}_{period}.model\"\n",
    "            model.save(final_path)\n",
    "            print(f\"Final model saved to {final_path}\")\n",
    "\n",
    "\n",
    "def process_batch(period, user_words_dict, user_comments_dict, existing_model, \n",
    "                 phrases_min_count, word2vec_min_count, party):\n",
    "    \"\"\"Process a batch of comments and update the model incrementally\"\"\"\n",
    "    \n",
    "    # Filter words by user count\n",
    "    valid_words = {w for w, users in user_words_dict.items() if len(users) >= 3}\n",
    "    filtered_comments = []\n",
    "    \n",
    "    for comments in user_comments_dict.values():\n",
    "        for comment in comments:\n",
    "            filtered = [w for w in comment if w in valid_words]\n",
    "            if filtered:\n",
    "                filtered_comments.append(filtered)\n",
    "                \n",
    "    print(f\"{period}: Processing {len(filtered_comments)} comments after filtering words by user count\")\n",
    "    \n",
    "    if not filtered_comments:\n",
    "        print(f\"No valid comments for {period} after filtering, skipping batch\")\n",
    "        return existing_model\n",
    "    \n",
    "    # Extract bigrams\n",
    "    print(f\"Extracting bigrams...\")\n",
    "    phrases = Phrases(filtered_comments, \n",
    "                      min_count=phrases_min_count, \n",
    "                      threshold=0.7,\n",
    "                      scoring='npmi')\n",
    "    bigram_model = Phraser(phrases)\n",
    "    bigrammed_comments = [bigram_model[comment] for comment in filtered_comments]\n",
    "    \n",
    "    # Either create a new model or update existing one\n",
    "    if existing_model is None:\n",
    "        print(f\"Creating new Word2Vec model for {period}\")\n",
    "        model = Word2Vec(\n",
    "            vector_size=300,\n",
    "            window=5,\n",
    "            min_count=word2vec_min_count,\n",
    "            workers=16\n",
    "        )\n",
    "        model.build_vocab(bigrammed_comments)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Updating existing Word2Vec model for {period}\")\n",
    "        model = existing_model\n",
    "        model.build_vocab(bigrammed_comments, update=True)\n",
    "    \n",
    "    print(f\"Training model on {len(bigrammed_comments)} comments\")\n",
    "    model.train(\n",
    "        bigrammed_comments,\n",
    "        total_examples=len(bigrammed_comments),\n",
    "        epochs=5\n",
    "    )\n",
    "    \n",
    "    # Save interim model\n",
    "    interim_path = f\"models/model_v4/interim/reddit_word2vec_{phrases_min_count}_{word2vec_min_count}_{party}_{period}_interim.model\"\n",
    "    os.makedirs(os.path.dirname(interim_path), exist_ok=True)\n",
    "    model.save(interim_path)\n",
    "    print(f\"Interim model saved to {interim_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Create output directories\n",
    "    os.makedirs(\"models/model_v4\", exist_ok=True)\n",
    "    os.makedirs(\"models/model_v4/interim\", exist_ok=True)\n",
    "    \n",
    "    filePathforDemocrats = r\"datasets/democrats_comments.zst\"\n",
    "    filePathforRepublican = r\"datasets/Republican_comments.zst\"\n",
    "    filePathforBackpacking = r\"datasets/backpacking_comments.zst\"\n",
    "    filePathforVagabond = r\"datasets/vagabond_comments.zst\"\n",
    "    filePathforConservative = r\"datasets/Conservative_comments.zst\"\n",
    "    filePathforLiberal = r\"datasets/Liberal_comments.zst\"\n",
    "\n",
    "    # Set batch size to 1 million comments\n",
    "    batch_size = 1000000\n",
    "    \n",
    "    # build_word2vect_model(filePathforDemocrats, \"democrats\", \n",
    "    #                       phrases_min_count=10, word2vec_min_count=10,\n",
    "    #                       batch_size=batch_size)\n",
    "    # build_word2vect_model(filePathforConservative, \"conservatives\", \n",
    "    #                       phrases_min_count=10, word2vec_min_count=10,\n",
    "    #                       batch_size=batch_size)\n",
    "    # build_word2vect_model(filePathforRepublican, \"republican\", \n",
    "    #                       phrases_min_count=10, word2vec_min_count=10,\n",
    "    #                       batch_size=batch_size)\n",
    "    # build_word2vect_model(filePathforBackpacking, \"backpacking\", \n",
    "    #                       phrases_min_count=10, word2vec_min_count=10,\n",
    "    #                       batch_size=batch_size)\n",
    "    # build_word2vect_model(filePathforVagabond, \"vagabond\", \n",
    "    #                       phrases_min_count=10, word2vec_min_count=10,\n",
    "    #                       batch_size=batch_size)\n",
    "    build_word2vect_model(filePathforLiberal, \"liberal\", \n",
    "                          phrases_min_count=10, word2vec_min_count=10,\n",
    "                          batch_size=batch_size)\n",
    "\n",
    "    print(\"Done :>\")\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ba2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reddit_text(text, lemmatize=True, without_stopwords=True):\n",
    "    # Handle HTML entities\n",
    "    text = html.unescape(text)                         # &amp; becomes &, etc.\n",
    "    \n",
    "    # Handle Unicode normalization\n",
    "    text = unicodedata.normalize('NFKD', text)         # converts café with a single combined \"é\" character to \"e\" with an accent\n",
    "    \n",
    "    # Remove all URLs\n",
    "    text = re.sub(r'http\\S+', '', text)                # Remove URLs\n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)        # Images/GIFs\n",
    "    \n",
    "    # Handle Reddit's link format\n",
    "    text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)    # [text](link) becomes text\n",
    "    \n",
    "    # Handle markdown formatting (bold, italics)\n",
    "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)       # **word** becomes word\n",
    "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)           # *word* becomes word\n",
    "    \n",
    "    # Handle subreddit and user references - both with and without leading slash\n",
    "    text = re.sub(r'/r/\\w+', '', text)                 # /r/politics becomes empty\n",
    "    text = re.sub(r'r/\\w+', '', text)                  # r/politics becomes empty\n",
    "    text = re.sub(r'/u/\\w+', '', text)                 # /u/username becomes empty\n",
    "    text = re.sub(r'u/\\w+', '', text)                  # u/username becomes empty\n",
    "    \n",
    "    # Remove time-related terms that create noise\n",
    "    # Time markers\n",
    "    text = re.sub(r'\\b(?:am|pm|AM|PM|a\\.m\\.|p\\.m\\.)\\b', '', text)\n",
    "    \n",
    "    # Days of week - both full and abbreviated forms\n",
    "    days_pattern = r'\\b(?:Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday|'\n",
    "    days_pattern += r'Mon|Tue|Tues|Wed|Thu|Thurs|Fri|Sat|Sun)\\b'\n",
    "    text = re.sub(days_pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Month names - both full and abbreviated forms\n",
    "    months_pattern = r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December|'\n",
    "    months_pattern += r'Jan|Feb|Mar|Apr|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)\\b'\n",
    "    text = re.sub(months_pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Time units that often appear in Reddit comments\n",
    "    time_units = r'\\b(?:second|minute|hour|day|week|month|year)s?\\b'\n",
    "    text = re.sub(time_units, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Basic text cleaning\n",
    "    text = re.sub(\"[^A-Za-z]+\", ' ', text).lower()\n",
    "\n",
    "    # Remove single letters except 'i'\n",
    "    text = re.sub(r'\\b([a-hj-z])\\b', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # If need lemmatization\n",
    "    if lemmatize:\n",
    "        stop_words = set(stopwords.words('english')) if without_stopwords else set()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        words = text.split()\n",
    "        if words:\n",
    "            tagged_words = nltk.pos_tag(words)\n",
    "            processed_words = []\n",
    "            \n",
    "            for word, tag in tagged_words:\n",
    "                if without_stopwords and word in stop_words:\n",
    "                    continue\n",
    "                    \n",
    "                if tag.startswith('J'):\n",
    "                    wordnet_pos = 'a'  # 形容词\n",
    "                elif tag.startswith('V'):\n",
    "                    wordnet_pos = 'v'  # 动词\n",
    "                elif tag.startswith('N'):\n",
    "                    wordnet_pos = 'n'  # 名词\n",
    "                elif tag.startswith('R'):\n",
    "                    wordnet_pos = 'r'  # 副词\n",
    "                else:\n",
    "                    wordnet_pos = 'n'  # 默认作为名词\n",
    "                    \n",
    "                lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "                processed_words.append(lemma)\n",
    "                \n",
    "            return ' '.join(processed_words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_store_comments(path, party, without_stopwords=True, min_users_per_word=3, batch_size=1000000):\n",
    "    \"\"\"\n",
    "    处理评论并按批次存储结果以供将来使用\n",
    "    \n",
    "    参数:\n",
    "    path - 评论数据路径\n",
    "    party - 子版块名称\n",
    "    without_stopwords - 是否移除停用词\n",
    "    min_users_per_word - 每个词至少要被多少用户使用\n",
    "    batch_size - 每批次处理的评论数量\n",
    "    \"\"\"\n",
    "    print(f\"处理文件 {path}\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "    \n",
    "    # 为每个时间段创建数据结构\n",
    "    periods = [\"before_2016\", \"2017_2020\", \"2021_2024\"]\n",
    "    \n",
    "    # 为每个批次设置计数器\n",
    "    batch_counts = {period: 0 for period in periods}\n",
    "    total_counts = {period: 0 for period in periods}\n",
    "    batch_number = {period: 1 for period in periods}\n",
    "    \n",
    "    # 创建输出目录\n",
    "    preprocessed_dir = \"preprocessed_data\"\n",
    "    os.makedirs(preprocessed_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"开始预处理来自 {path} 的评论...\")\n",
    "    with open(path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"跳过未知文件 {path}\")\n",
    "            return\n",
    "        \n",
    "        # 为当前批次创建数据结构\n",
    "        user_words = {period: defaultdict(set) for period in periods}\n",
    "        user_comments = {period: defaultdict(list) for period in periods}\n",
    "        \n",
    "        for row in tqdm(jsonStream, desc=\"处理评论\"):\n",
    "            if \"body\" not in row or \"created_utc\" not in row or \"author\" not in row:\n",
    "                continue\n",
    "            author = row[\"author\"]\n",
    "            if author in {\"AutoModerator\", \"election_info_bot\"}:\n",
    "                continue\n",
    "                \n",
    "            text = row[\"body\"]\n",
    "            created_timestamp = row[\"created_utc\"]\n",
    "            year = datetime.datetime.fromtimestamp(int(created_timestamp)).year\n",
    "            \n",
    "            # 确定时间段\n",
    "            if year <= 2016:\n",
    "                period = \"before_2016\"\n",
    "            elif 2017 <= year <= 2020:\n",
    "                period = \"2017_2020\"\n",
    "            elif 2021 <= year <= 2024:\n",
    "                period = \"2021_2024\"\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # 在这里直接调用预处理和词形还原函数\n",
    "            processed_text = preprocess_reddit_text(text, lemmatize=True, without_stopwords=without_stopwords)\n",
    "            processed_words = processed_text.split()\n",
    "            \n",
    "            if processed_words:\n",
    "                # 跟踪单词-用户关系\n",
    "                for word in processed_words:\n",
    "                    user_words[period][word].add(author)\n",
    "                \n",
    "                # 保存处理后的评论\n",
    "                user_comments[period][author].append(processed_words)\n",
    "                batch_counts[period] += 1\n",
    "                total_counts[period] += 1\n",
    "                \n",
    "            # 检查是否达到批处理阈值\n",
    "            for p in periods:\n",
    "                if batch_counts[p] >= batch_size:\n",
    "                    print(f\"\\n达到 {batch_size} 条 {p} 评论，处理批次 {batch_number[p]}...\")\n",
    "                    \n",
    "                    # 处理并保存这个批次\n",
    "                    _process_and_save_batch(\n",
    "                        p, user_words[p], user_comments[p], \n",
    "                        party, batch_number[p], min_users_per_word, preprocessed_dir\n",
    "                    )\n",
    "                    \n",
    "                    # 重置批次数据\n",
    "                    user_words[p] = defaultdict(set)\n",
    "                    user_comments[p] = defaultdict(list)\n",
    "                    batch_counts[p] = 0\n",
    "                    batch_number[p] += 1\n",
    "    \n",
    "    # 处理所有剩余评论\n",
    "    print(\"\\n=== 各时间段评论总数 ===\")\n",
    "    for period, count in total_counts.items():\n",
    "        print(f\"{period}: {count} 条评论\")\n",
    "        \n",
    "    for period in periods:\n",
    "        if batch_counts[period] > 0:\n",
    "            print(f\"\\n处理剩余的 {batch_counts[period]} 条 {period} 评论...\")\n",
    "            _process_and_save_batch(\n",
    "                period, user_words[period], user_comments[period],\n",
    "                party, batch_number[period], min_users_per_word, preprocessed_dir\n",
    "            )\n",
    "    \n",
    "    print(f\"预处理完成! 所有批次都已保存到 {preprocessed_dir} 目录\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_store_comments(path, party, without_stopwords=True, min_users_per_word=3):\n",
    "    \"\"\"Process comments once and save results for future use\"\"\"\n",
    "    print(f\"Processing file {path}\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # For each period, track comments and user-word usage\n",
    "    periods = [\"before_2016\", \"2017_2020\", \"2021_2024\"]\n",
    "    user_words = {period: defaultdict(set) for period in periods}\n",
    "    user_comments = {period: defaultdict(list) for period in periods}\n",
    "    counts = {period: 0 for period in periods}\n",
    "\n",
    "    print(f\"Preprocessing comments from {path}...\")\n",
    "    with open(path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Skipping unknown file {path}\")\n",
    "            return\n",
    "\n",
    "        for row in tqdm(jsonStream, desc=\"Processing comments\"):\n",
    "            if \"body\" not in row or \"created_utc\" not in row or \"author\" not in row:\n",
    "                continue\n",
    "            author = row[\"author\"]\n",
    "            if author in {\"AutoModerator\", \"election_info_bot\"}:\n",
    "                continue\n",
    "                \n",
    "            text = row[\"body\"]\n",
    "            created_timestamp = row[\"created_utc\"]\n",
    "            year = datetime.datetime.fromtimestamp(int(created_timestamp)).year\n",
    "            \n",
    "            if year <= 2016:\n",
    "                period = \"before_2016\"\n",
    "            elif 2017 <= year <= 2020:\n",
    "                period = \"2017_2020\"\n",
    "            elif 2021 <= year <= 2024:\n",
    "                period = \"2021_2024\"\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            text = preprocess_reddit_text(text)\n",
    "            words = text.split()\n",
    "            if not words:\n",
    "                continue\n",
    "                \n",
    "            tagged_words = nltk.pos_tag(words)\n",
    "            processed_words = []\n",
    "            \n",
    "            for word, tag in tagged_words:\n",
    "                if without_stopwords and word in stop_words:\n",
    "                    continue\n",
    "                if tag.startswith('J'):\n",
    "                    wordnet_pos = 'a'\n",
    "                elif tag.startswith('V'):\n",
    "                    wordnet_pos = 'v'\n",
    "                elif tag.startswith('N'):\n",
    "                    wordnet_pos = 'n'\n",
    "                elif tag.startswith('R'):\n",
    "                    wordnet_pos = 'r'\n",
    "                else:\n",
    "                    wordnet_pos = 'n'\n",
    "                lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "                processed_words.append(lemma)\n",
    "                user_words[period][lemma].add(author)\n",
    "                \n",
    "            if processed_words:\n",
    "                user_comments[period][author].append(processed_words)\n",
    "                counts[period] += 1\n",
    "\n",
    "    print(\"\\n=== Comment Counts by Period ===\")\n",
    "    for period, count in counts.items():\n",
    "        print(f\"{period}: {count} comments\")\n",
    "\n",
    "    # Filter words by user count and rebuild comments for each period\n",
    "    filtered_data = {}\n",
    "    for period in periods:\n",
    "        print(f\"\\nFiltering {period} comments by user count (min {min_users_per_word})...\")\n",
    "        valid_words = {w for w, users in user_words[period].items() if len(users) >= min_users_per_word}\n",
    "        filtered_comments = []\n",
    "        \n",
    "        for comments in user_comments[period].values():\n",
    "            for comment in comments:\n",
    "                filtered = [w for w in comment if w in valid_words]\n",
    "                if filtered:\n",
    "                    filtered_comments.append(filtered)\n",
    "                    \n",
    "        print(f\"{period}: {len(filtered_comments)} comments after filtering words by user count\")\n",
    "        filtered_data[period] = filtered_comments\n",
    "    \n",
    "    # Create directory for preprocessed data\n",
    "    preprocessed_dir = \"preprocessed_data\"\n",
    "    os.makedirs(preprocessed_dir, exist_ok=True)\n",
    "    \n",
    "    # Save filtered comments to disk\n",
    "    for period, comments in filtered_data.items():\n",
    "        output_path = f\"{preprocessed_dir}/{party}_{period}_filtered.pkl\"\n",
    "        print(f\"Saving {len(comments)} filtered comments to {output_path}\")\n",
    "        \n",
    "        with open(output_path, 'wb') as f:\n",
    "            import pickle\n",
    "            pickle.dump(comments, f)\n",
    "    \n",
    "    print(f\"Preprocessing completed for {party}!\")\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def train_model_from_preprocessed(party, period, phrases_min_count=5, word2vec_min_count=5):\n",
    "    \"\"\"Train a model using preprocessed comments\"\"\"\n",
    "    preprocessed_path = f\"preprocessed_data/{party}_{period}_filtered.pkl\"\n",
    "    \n",
    "    if not os.path.exists(preprocessed_path):\n",
    "        print(f\"Error: Preprocessed file {preprocessed_path} not found.\")\n",
    "        return None\n",
    "    \n",
    "    # Load preprocessed comments\n",
    "    print(f\"Loading preprocessed comments from {preprocessed_path}\")\n",
    "    with open(preprocessed_path, 'rb') as f:\n",
    "        import pickle\n",
    "        filtered_comments = pickle.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(filtered_comments)} preprocessed comments\")\n",
    "    \n",
    "    # Extract bigrams\n",
    "    print(f\"Extracting bigrams...\")\n",
    "    phrases = Phrases(filtered_comments, \n",
    "                      min_count=phrases_min_count, \n",
    "                      threshold=0.7,\n",
    "                      scoring='npmi')\n",
    "    bigram_model = Phraser(phrases)\n",
    "    bigrammed_comments = [bigram_model[comment] for comment in filtered_comments]\n",
    "    \n",
    "    # Create and train model\n",
    "    print(f\"Creating new Word2Vec model\")\n",
    "    model = Word2Vec(\n",
    "        vector_size=300,\n",
    "        window=5,\n",
    "        min_count=word2vec_min_count,\n",
    "        workers=16\n",
    "    )\n",
    "    model.build_vocab(bigrammed_comments)\n",
    "    print(f\"Vocabulary size: {len(model.wv.index_to_key)}\")\n",
    "    \n",
    "    print(f\"Training model on {len(bigrammed_comments)} comments\")\n",
    "    model.train(\n",
    "        bigrammed_comments,\n",
    "        total_examples=len(bigrammed_comments),\n",
    "        epochs=5\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    model_dir = \"models/model_v4\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_path = f\"{model_dir}/reddit_word2vec_{phrases_min_count}_{word2vec_min_count}_{party}_{period}.model\"\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Paths to your data\n",
    "    filePathforDemocrats = r\"datasets/democrats_comments.zst\"\n",
    "    filePathforRepublican = r\"datasets/Republican_comments.zst\"\n",
    "    filePathforBackpacking = r\"datasets/backpacking_comments.zst\"\n",
    "    filePathforVagabond = r\"datasets/vagabond_comments.zst\"\n",
    "    filePathforConservative = r\"datasets/Conservative_comments.zst\"\n",
    "    filePathforLiberal = r\"datasets/Liberal_comments.zst\"\n",
    "    \n",
    "    # Step 1: Preprocess once and save results\n",
    "    # You only need to run this once per dataset\n",
    "    preprocess = True  # Set to False if preprocessed data already exists\n",
    "    if preprocess:\n",
    "        print(\"Starting preprocessing pipeline...\")\n",
    "        preprocess_and_store_comments(filePathforDemocrats, \"democrats\", min_users_per_word=3)\n",
    "        preprocess_and_store_comments(filePathforRepublican, \"republican\", min_users_per_word=3)\n",
    "        preprocess_and_store_comments(filePathforConservative, \"conservatives\", min_users_per_word=3)\n",
    "        preprocess_and_store_comments(filePathforLiberal, \"liberal\", min_users_per_word=3)\n",
    "    \n",
    "    # Step 2: Train models using preprocessed data\n",
    "    # You can modify parameters and run this many times without repeating preprocessing\n",
    "    print(\"\\n\\nTraining models using preprocessed data:\")\n",
    "    \n",
    "    # Examples for different parameter combinations\n",
    "    train_model_from_preprocessed(\"democrats\", \"before_2016\", phrases_min_count=10, word2vec_min_count=10)\n",
    "    train_model_from_preprocessed(\"democrats\", \"2017_2020\", phrases_min_count=10, word2vec_min_count=10)\n",
    "    train_model_from_preprocessed(\"democrats\", \"2021_2024\", phrases_min_count=10, word2vec_min_count=10)\n",
    "    \n",
    "    # You could also loop through parameter combinations\n",
    "    # for phrases_min in [5, 10, 15]:\n",
    "    #     for word2vec_min in [5, 10, 15]:\n",
    "    #         train_model_from_preprocessed(\"democrats\", \"before_2016\", \n",
    "    #                                      phrases_min_count=phrases_min, \n",
    "    #                                      word2vec_min_count=word2vec_min)\n",
    "\n",
    "    print(\"Done :>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b166d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'list'>\n",
      "Number of items: 1000000\n",
      "\n",
      "Showing first 5 examples:\n",
      "\n",
      "Example 1:\n",
      "  comment_id: c07p2u0\n",
      "  author: Garak\n",
      "  date: 2009-02-16\n",
      "  timestamp: 1234791099\n",
      "  processed_text: ['allow', 'legend', 'grow', 'ill', 'mythical', 'proportion', 'lie', 'fund', 'acorn', 'nowhere'] ... (total: 49 words)\n",
      "\n",
      "Example 2:\n",
      "  comment_id: c0883zo\n",
      "  author: Garak\n",
      "  date: 2009-03-13\n",
      "  timestamp: 1236991154\n",
      "  processed_text: ['sadden', 'read', 'time', 'yesterday', 'water', 'bill', 'maher', 'recently', 'seem', 'pretty'] ... (total: 15 words)\n",
      "\n",
      "Example 3:\n",
      "  comment_id: c091f56\n",
      "  author: [deleted]\n",
      "  date: 2009-04-22\n",
      "  timestamp: 1240434783\n",
      "  processed_text: ['speaker', 'pelosi', 'culture', 'corruption', 'washington', 'party', 'bad', 'republican']\n",
      "\n",
      "Example 4:\n",
      "  comment_id: c095pfx\n",
      "  author: [deleted]\n",
      "  date: 2009-04-27\n",
      "  timestamp: 1240881225\n",
      "  processed_text: ['congresswoman', 'nancy', 'pelosi', 'call', 'washington', 'culture', 'corruption', 'house', 'speaker', 'nancy'] ... (total: 13 words)\n",
      "\n",
      "Example 5:\n",
      "  comment_id: c09e1na\n",
      "  author: [deleted]\n",
      "  date: 2009-05-06\n",
      "  timestamp: 1241665126\n",
      "  processed_text: ['share', 'place', 'judas', 'benedict', 'arnold', 'man', 'honor', 'trust', 'respect', 'side']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def inspect_pkl_file(file_path, num_examples=5):\n",
    "    \"\"\"\n",
    "    Load a pickle file and print a few example records\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the pickle file\n",
    "        num_examples: Number of examples to show (default 5)\n",
    "    \"\"\"\n",
    "    # Load the pickle file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # Print information about the data structure\n",
    "    print(f\"Data type: {type(data)}\")\n",
    "    \n",
    "    if isinstance(data, list):\n",
    "        print(f\"Number of items: {len(data)}\")\n",
    "        \n",
    "        # Display examples\n",
    "        print(f\"\\nShowing first {min(num_examples, len(data))} examples:\")\n",
    "        for i, item in enumerate(data[:num_examples]):\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            if isinstance(item, dict):\n",
    "                for key, value in item.items():\n",
    "                    # For processed text, just show a few words\n",
    "                    if key == \"processed_text\" and isinstance(value, list) and len(value) > 10:\n",
    "                        print(f\"  {key}: {value[:10]} ... (total: {len(value)} words)\")\n",
    "                    else:\n",
    "                        print(f\"  {key}: {value}\")\n",
    "            else:\n",
    "                print(item)\n",
    "    else:\n",
    "        print(\"Data is not a list. Structure:\", data)\n",
    "\n",
    "# Example usage\n",
    "inspect_pkl_file(\"processed_comments/democrats/democrats_batch1.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

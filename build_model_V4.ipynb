{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a65927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For streaming\n",
    "import sys\n",
    "version = sys.version_info\n",
    "if version.major < 3 or (version.major == 3 and version.minor < 10):\n",
    "    raise RuntimeError(\"This script requires Python 3.10 or higher\")\n",
    "import os\n",
    "from typing import Iterable\n",
    "\n",
    "from fileStreams import getFileJsonStream\n",
    "from utils import FileProgressLog\n",
    "\n",
    "# For processing\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import html\n",
    "import unicodedata\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "    \n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "recursive = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4dc6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file datasets/backpacking_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments: 866545it [07:35, 1901.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Comment Counts by Period ===\n",
      "before_2016: 160382 comments\n",
      "2017_2020: 225690 comments\n",
      "2021_2024: 428375 comments\n",
      "\n",
      "Processing final batch of 160382 comments for before_2016...\n",
      "before_2016: Processing 160121 comments after filtering words by user count\n",
      "Extracting bigrams...\n",
      "Creating new Word2Vec model for before_2016\n",
      "Training model on 160121 comments\n",
      "Interim model saved to models/model_v4/interim/reddit_word2vec_10_10_backpacking_before_2016_interim.model\n",
      "\n",
      "Processing final batch of 225690 comments for 2017_2020...\n",
      "2017_2020: Processing 224936 comments after filtering words by user count\n",
      "Extracting bigrams...\n",
      "Creating new Word2Vec model for 2017_2020\n",
      "Training model on 224936 comments\n",
      "Interim model saved to models/model_v4/interim/reddit_word2vec_10_10_backpacking_2017_2020_interim.model\n",
      "\n",
      "Processing final batch of 428375 comments for 2021_2024...\n",
      "2021_2024: Processing 427362 comments after filtering words by user count\n",
      "Extracting bigrams...\n",
      "Creating new Word2Vec model for 2021_2024\n",
      "Training model on 427362 comments\n",
      "Interim model saved to models/model_v4/interim/reddit_word2vec_10_10_backpacking_2021_2024_interim.model\n",
      "Processing file datasets/vagabond_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments: 513168it [04:19, 1980.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Comment Counts by Period ===\n",
      "before_2016: 39300 comments\n",
      "2017_2020: 156891 comments\n",
      "2021_2024: 292050 comments\n",
      "\n",
      "Processing final batch of 39300 comments for before_2016...\n",
      "before_2016: Processing 39147 comments after filtering words by user count\n",
      "Extracting bigrams...\n",
      "Creating new Word2Vec model for before_2016\n",
      "Training model on 39147 comments\n",
      "Interim model saved to models/model_v4/interim/reddit_word2vec_10_10_vagabond_before_2016_interim.model\n",
      "\n",
      "Processing final batch of 156891 comments for 2017_2020...\n",
      "2017_2020: Processing 156248 comments after filtering words by user count\n",
      "Extracting bigrams...\n",
      "Creating new Word2Vec model for 2017_2020\n",
      "Training model on 156248 comments\n",
      "Interim model saved to models/model_v4/interim/reddit_word2vec_10_10_vagabond_2017_2020_interim.model\n",
      "\n",
      "Processing final batch of 292050 comments for 2021_2024...\n",
      "2021_2024: Processing 290866 comments after filtering words by user count\n",
      "Extracting bigrams...\n",
      "Creating new Word2Vec model for 2021_2024\n",
      "Training model on 290866 comments\n",
      "Interim model saved to models/model_v4/interim/reddit_word2vec_10_10_vagabond_2021_2024_interim.model\n",
      "Done :>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_reddit_text(text):\n",
    "    # Handle HTML entities\n",
    "    text = html.unescape(text)                         # &amp; becomes &, etc.\n",
    "    \n",
    "    # Handle Unicode normalization\n",
    "    text = unicodedata.normalize('NFKD', text)         # converts café with a single combined \"é\" character to \"e\" with an accent\n",
    "    \n",
    "    # Remove all URLs\n",
    "    text = re.sub(r'http\\S+', '', text)                # Remove URLs\n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)        # Images/GIFs\n",
    "    \n",
    "    # Handle Reddit's link format\n",
    "    text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)    # [text](link) becomes text\n",
    "    \n",
    "    # Handle markdown formatting (bold, italics)\n",
    "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)       # **word** becomes word\n",
    "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)           # *word* becomes word\n",
    "    \n",
    "    # Handle subreddit and user references - both with and without leading slash\n",
    "    text = re.sub(r'/r/\\w+', '', text)                 # /r/politics becomes empty\n",
    "    text = re.sub(r'r/\\w+', '', text)                  # r/politics becomes empty\n",
    "    text = re.sub(r'/u/\\w+', '', text)                 # /u/username becomes empty\n",
    "    text = re.sub(r'u/\\w+', '', text)                  # u/username becomes empty\n",
    "    \n",
    "    # Remove time-related terms that create noise\n",
    "    # Time markers\n",
    "    text = re.sub(r'\\b(?:am|pm|AM|PM|a\\.m\\.|p\\.m\\.)\\b', '', text)\n",
    "    \n",
    "    # Days of week - both full and abbreviated forms\n",
    "    days_pattern = r'\\b(?:Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday|'\n",
    "    days_pattern += r'Mon|Tue|Tues|Wed|Thu|Thurs|Fri|Sat|Sun)\\b'\n",
    "    text = re.sub(days_pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Month names - both full and abbreviated forms\n",
    "    months_pattern = r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December|'\n",
    "    months_pattern += r'Jan|Feb|Mar|Apr|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)\\b'\n",
    "    text = re.sub(months_pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Time units that often appear in Reddit comments\n",
    "    time_units = r'\\b(?:second|minute|hour|day|week|month|year)s?\\b'\n",
    "    text = re.sub(time_units, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Basic text cleaning\n",
    "    text = re.sub(\"[^A-Za-z]+\", ' ', text).lower()\n",
    "\n",
    "    # Remove single letters except 'i'\n",
    "    text = re.sub(r'\\b([a-hj-z])\\b', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def build_word2vect_model(path, party, without_stopwords=True, phrases_min_count=5, word2vec_min_count=5, \n",
    "                          batch_size=1000000, save_interim=True):\n",
    "    print(f\"Processing file {path}\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # For each period, track comments and user-word usage\n",
    "    chunks = {\n",
    "        \"before_2016\": [],\n",
    "        \"2017_2020\": [],\n",
    "        \"2021_2024\": [],\n",
    "    }\n",
    "    user_words = {\n",
    "        \"before_2016\": defaultdict(set),\n",
    "        \"2017_2020\": defaultdict(set),\n",
    "        \"2021_2024\": defaultdict(set),\n",
    "    }\n",
    "    user_comments = {\n",
    "        \"before_2016\": defaultdict(list),\n",
    "        \"2017_2020\": defaultdict(list),\n",
    "        \"2021_2024\": defaultdict(list),\n",
    "    }\n",
    "    counts = {period: 0 for period in chunks.keys()}\n",
    "    \n",
    "    # Batch processing counters\n",
    "    batch_counts = {period: 0 for period in chunks.keys()}\n",
    "    models = {period: None for period in chunks.keys()}\n",
    "\n",
    "    with open(path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Skipping unknown file {path}\")\n",
    "            return\n",
    "            \n",
    "        for row in tqdm(jsonStream, desc=\"Processing comments\"):\n",
    "            if \"body\" not in row or \"created_utc\" not in row or \"author\" not in row:\n",
    "                continue\n",
    "            author = row[\"author\"]\n",
    "            if author in {\"AutoModerator\", \"election_info_bot\"}:\n",
    "                continue\n",
    "                \n",
    "            text = row[\"body\"]\n",
    "            created_timestamp = row[\"created_utc\"]\n",
    "            year = datetime.datetime.fromtimestamp(int(created_timestamp)).year\n",
    "            \n",
    "            if year <= 2016:\n",
    "                chunk_key = \"before_2016\"\n",
    "            elif 2017 <= year <= 2020:\n",
    "                chunk_key = \"2017_2020\"\n",
    "            elif 2021 <= year <= 2024:\n",
    "                chunk_key = \"2021_2024\"\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            text = preprocess_reddit_text(text)\n",
    "\n",
    "            words = text.split()\n",
    "            if not words:\n",
    "                continue\n",
    "                \n",
    "            tagged_words = nltk.pos_tag(words)\n",
    "            processed_words = []\n",
    "            \n",
    "            for word, tag in tagged_words:\n",
    "                if without_stopwords and word in stop_words:\n",
    "                    continue\n",
    "                if tag.startswith('J'):\n",
    "                    wordnet_pos = 'a'\n",
    "                elif tag.startswith('V'):\n",
    "                    wordnet_pos = 'v'\n",
    "                elif tag.startswith('N'):\n",
    "                    wordnet_pos = 'n'\n",
    "                elif tag.startswith('R'):\n",
    "                    wordnet_pos = 'r'\n",
    "                else:\n",
    "                    wordnet_pos = 'n'\n",
    "                lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "                processed_words.append(lemma)\n",
    "                user_words[chunk_key][lemma].add(author)\n",
    "                \n",
    "            if processed_words:\n",
    "                user_comments[chunk_key][author].append(processed_words)\n",
    "                counts[chunk_key] += 1\n",
    "                batch_counts[chunk_key] += 1\n",
    "            \n",
    "            # Process batch when we reach the batch size for any period\n",
    "            for period in chunks.keys():\n",
    "                if batch_counts[period] >= batch_size:\n",
    "                    print(f\"\\nReached {batch_size} comments for {period}, processing batch...\")\n",
    "                    process_batch(period, user_words[period], user_comments[period], \n",
    "                                 models[period], phrases_min_count, word2vec_min_count, party)\n",
    "                    \n",
    "                    # Clear batch data to free memory\n",
    "                    user_words[period] = defaultdict(set)\n",
    "                    user_comments[period] = defaultdict(list)\n",
    "                    batch_counts[period] = 0\n",
    "\n",
    "    print(\"\\n=== Final Comment Counts by Period ===\")\n",
    "    for period, count in counts.items():\n",
    "        print(f\"{period}: {count} comments\")\n",
    "    \n",
    "    # Process any remaining comments\n",
    "    for period in chunks.keys():\n",
    "        if batch_counts[period] > 0:\n",
    "            print(f\"\\nProcessing final batch of {batch_counts[period]} comments for {period}...\")\n",
    "            process_batch(period, user_words[period], user_comments[period], \n",
    "                         models[period], phrases_min_count, word2vec_min_count, party)\n",
    "\n",
    "    # Save final models\n",
    "    for period, model in models.items():\n",
    "        if model is not None:\n",
    "            final_path = f\"models/model_v4/reddit_word2vec_{phrases_min_count}_{word2vec_min_count}_{party}_{period}.model\"\n",
    "            model.save(final_path)\n",
    "            print(f\"Final model saved to {final_path}\")\n",
    "\n",
    "\n",
    "def process_batch(period, user_words_dict, user_comments_dict, existing_model, \n",
    "                 phrases_min_count, word2vec_min_count, party):\n",
    "    \"\"\"Process a batch of comments and update the model incrementally\"\"\"\n",
    "    \n",
    "    # Filter words by user count\n",
    "    valid_words = {w for w, users in user_words_dict.items() if len(users) >= 3}\n",
    "    filtered_comments = []\n",
    "    \n",
    "    for comments in user_comments_dict.values():\n",
    "        for comment in comments:\n",
    "            filtered = [w for w in comment if w in valid_words]\n",
    "            if filtered:\n",
    "                filtered_comments.append(filtered)\n",
    "                \n",
    "    print(f\"{period}: Processing {len(filtered_comments)} comments after filtering words by user count\")\n",
    "    \n",
    "    if not filtered_comments:\n",
    "        print(f\"No valid comments for {period} after filtering, skipping batch\")\n",
    "        return existing_model\n",
    "    \n",
    "    # Extract bigrams\n",
    "    print(f\"Extracting bigrams...\")\n",
    "    phrases = Phrases(filtered_comments, \n",
    "                      min_count=phrases_min_count, \n",
    "                      threshold=0.7,\n",
    "                      scoring='npmi')\n",
    "    bigram_model = Phraser(phrases)\n",
    "    bigrammed_comments = [bigram_model[comment] for comment in filtered_comments]\n",
    "    \n",
    "    # Either create a new model or update existing one\n",
    "    if existing_model is None:\n",
    "        print(f\"Creating new Word2Vec model for {period}\")\n",
    "        model = Word2Vec(\n",
    "            vector_size=300,\n",
    "            window=5,\n",
    "            min_count=word2vec_min_count,\n",
    "            workers=16\n",
    "        )\n",
    "        model.build_vocab(bigrammed_comments)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Updating existing Word2Vec model for {period}\")\n",
    "        model = existing_model\n",
    "        model.build_vocab(bigrammed_comments, update=True)\n",
    "    \n",
    "    print(f\"Training model on {len(bigrammed_comments)} comments\")\n",
    "    model.train(\n",
    "        bigrammed_comments,\n",
    "        total_examples=len(bigrammed_comments),\n",
    "        epochs=5\n",
    "    )\n",
    "    \n",
    "    # Save interim model\n",
    "    interim_path = f\"models/model_v4/interim/reddit_word2vec_{phrases_min_count}_{word2vec_min_count}_{party}_{period}_interim.model\"\n",
    "    os.makedirs(os.path.dirname(interim_path), exist_ok=True)\n",
    "    model.save(interim_path)\n",
    "    print(f\"Interim model saved to {interim_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Create output directories\n",
    "    os.makedirs(\"models/model_v4\", exist_ok=True)\n",
    "    os.makedirs(\"models/model_v4/interim\", exist_ok=True)\n",
    "    \n",
    "    filePathforDemocrats = r\"datasets/democrats_comments.zst\"\n",
    "    filePathforRepublican = r\"datasets/Republican_comments.zst\"\n",
    "    filePathforBackpacking = r\"datasets/backpacking_comments.zst\"\n",
    "    filePathforVagabond = r\"datasets/vagabond_comments.zst\"\n",
    "    filePathforConservative = r\"datasets/Conservative_comments.zst\"\n",
    "    filePathforLiberal = r\"datasets/Liberal_comments.zst\"\n",
    "\n",
    "    # Set batch size to 1 million comments\n",
    "    batch_size = 1000000\n",
    "    \n",
    "    # build_word2vect_model(filePathforDemocrats, \"democrats\", \n",
    "    #                       phrases_min_count=10, word2vec_min_count=10,\n",
    "    #                       batch_size=batch_size)\n",
    "    # build_word2vect_model(filePathforConservative, \"conservative\", \n",
    "    #                       phrases_min_count=10, word2vec_min_count=10,\n",
    "    #                       batch_size=batch_size)\n",
    "    # build_word2vect_model(filePathforRepublican, \"republican\", \n",
    "    #                       phrases_min_count=10, word2vec_min_count=10,\n",
    "    #                       batch_size=batch_size)\n",
    "    build_word2vect_model(filePathforBackpacking, \"backpacking\", \n",
    "                          phrases_min_count=10, word2vec_min_count=10,\n",
    "                          batch_size=batch_size)\n",
    "    build_word2vect_model(filePathforVagabond, \"vagabond\", \n",
    "                          phrases_min_count=10, word2vec_min_count=10,\n",
    "                          batch_size=batch_size)\n",
    "    # build_word2vect_model(filePathforLiberal, \"liberal\", \n",
    "    #                       phrases_min_count=10, word2vec_min_count=10,\n",
    "    #                       batch_size=batch_size)\n",
    "\n",
    "    print(\"Done :>\")\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

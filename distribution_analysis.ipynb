{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379cc5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For streaming\n",
    "import sys\n",
    "version = sys.version_info\n",
    "if version.major < 3 or (version.major == 3 and version.minor < 10):\n",
    "    raise RuntimeError(\"This script requires Python 3.10 or higher\")\n",
    "import os\n",
    "from typing import Iterable\n",
    "\n",
    "from fileStreams import getFileJsonStream\n",
    "from utils import FileProgressLog\n",
    "\n",
    "# For processing\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "    \n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "filePathforDemocrats = r\"datasets/democrats_comments.zst\"\n",
    "filePathforRepublican = r\"datasets/Republican_comments.zst\"\n",
    "filePathforBackpacking = r\"datasets/backpacking_comments.zst\"\n",
    "filePathforvagabond = r\"datasets/vagabond_comments.zst\"\n",
    "recursive = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2877a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processFile(path, party, without_stopwords=True):\n",
    "    # Set seeds for reproducibility\n",
    "    random.seed(23)\n",
    "    np.random.seed(23)\n",
    "    \n",
    "    print(f\"Processing file {path}\")\n",
    "    \n",
    "    # Download necessary NLTK resources\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)  # For POS tagging\n",
    "    \n",
    "    # Initialize lemmatizer and stop words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Create empty lists for each time period\n",
    "    chunks = {\n",
    "        \"before_2016\": [],\n",
    "        \"2017_2020\": [],\n",
    "        \"2021_2024\": [],\n",
    "    }\n",
    "    \n",
    "    # Track counts\n",
    "    counts = {period: 0 for period in chunks.keys()}\n",
    "    \n",
    "    with open(path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Skipping unknown file {path}\")\n",
    "            return\n",
    "        for row in tqdm(jsonStream, desc=\"Processing comments\"):\n",
    "            if \"body\" not in row or \"created_utc\" not in row:\n",
    "                continue\n",
    "            \n",
    "            # Get the comment text and timestamp\n",
    "            text = row[\"body\"]\n",
    "            created_timestamp = row[\"created_utc\"]\n",
    "            \n",
    "            # Convert timestamp to year\n",
    "            year = datetime.datetime.fromtimestamp(int(created_timestamp)).year\n",
    "            \n",
    "            # Determine which chunk this comment belongs to\n",
    "            chunk_key = None\n",
    "            if year <= 2016:\n",
    "                chunk_key = \"before_2016\"\n",
    "            elif 2017 <= year <= 2020:\n",
    "                chunk_key = \"2017_2020\"\n",
    "            elif 2021 <= year <= 2024:\n",
    "                chunk_key = \"2021_2024\"\n",
    "            \n",
    "            # Process text\n",
    "            # Remove URLs\n",
    "            txt = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "            \n",
    "            # Remove non-alphanumeric characters and convert to lowercase\n",
    "            txt = re.sub(\"[^A-Za-z0-9']+\", ' ', txt).lower()\n",
    "            \n",
    "            # Replace special characters with spaces\n",
    "            txt = re.sub(r\"['\\-_]\", ' ', txt)\n",
    "            \n",
    "            # Tokenize\n",
    "            words = txt.split()\n",
    "            \n",
    "            # Tag words with parts of speech for better lemmatization\n",
    "            tagged_words = nltk.pos_tag(words)\n",
    "            processed_words = []\n",
    "            \n",
    "            # Skip empty comments\n",
    "            if not words:\n",
    "                continue\n",
    "            \n",
    "            for word, tag in tagged_words:\n",
    "                if without_stopwords:\n",
    "                    if word in stop_words:\n",
    "                        continue\n",
    "                \n",
    "                # Convert Penn Treebank tag to WordNet tag\n",
    "                if tag.startswith('J'):\n",
    "                    wordnet_pos = 'a'  # adjective\n",
    "                elif tag.startswith('V'):\n",
    "                    wordnet_pos = 'v'  # verb\n",
    "                elif tag.startswith('N'):\n",
    "                    wordnet_pos = 'n'  # noun\n",
    "                elif tag.startswith('R'):\n",
    "                    wordnet_pos = 'r'  # adverb\n",
    "                else:\n",
    "                    wordnet_pos = 'n'  # default to noun\n",
    "                    \n",
    "                # Lemmatize with the correct POS\n",
    "                lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "                processed_words.append(lemma)\n",
    "            \n",
    "            # Add to appropriate chunk if it has words\n",
    "            if processed_words:\n",
    "                chunks[chunk_key].append(processed_words)\n",
    "                counts[chunk_key] += 1\n",
    "                \n",
    "    # Print statistics\n",
    "    print(\"\\n=== Comment Counts by Period ===\")\n",
    "    for period, count in counts.items():\n",
    "        print(f\"{period}: {count} comments\")\n",
    "        \n",
    "    # Extract bigrams from each time period\n",
    "    for period, comments in chunks.items():\n",
    "        if len(comments) > 0:\n",
    "            print(f\"\\nExtracting bigrams for {period}...\")\n",
    "            # Build bigram model\n",
    "            phrases = Phrases(comments, min_count=5, threshold=10)\n",
    "            bigram_model = Phraser(phrases)\n",
    "            \n",
    "            # Apply bigram model to create comments with bigrams\n",
    "            bigrammed_comments = [bigram_model[comment] for comment in comments]\n",
    "            chunks[period] = bigrammed_comments\n",
    "        \n",
    "    # Train a Word2Vec model for each time period\n",
    "    for period, comments in chunks.items():\n",
    "        if len(comments) > 0:\n",
    "            print(f\"\\n=== Training Word2Vec for {period} ({len(comments)} comments) ===\")\n",
    "            \n",
    "            # Initialize and train model\n",
    "            model = Word2Vec(\n",
    "                vector_size=300,\n",
    "                window=5,\n",
    "                min_count=5,\n",
    "                workers=16,\n",
    "                seed=23\n",
    "            )\n",
    "            \n",
    "            # Build vocabulary\n",
    "            model.build_vocab(comments)\n",
    "            print(f\"Vocabulary size: {len(model.wv.index_to_key)}\")\n",
    "            \n",
    "            # Train the model\n",
    "            model.train(\n",
    "                comments, \n",
    "                total_examples=len(comments), \n",
    "                epochs=5\n",
    "            )\n",
    "            \n",
    "            # Save the model\n",
    "            model_path = f\"models/models_distribution/reddit_word2vec_{party}_{period}.model\"\n",
    "            model.save(model_path)\n",
    "            print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ba05183",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDone :>\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mprocessFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilePathforDemocrats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdemocrats\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     processFile(filePathforRepublican, \u001b[33m\"\u001b[39m\u001b[33mrepublican\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     processFile(filePathforBackpacking, \u001b[33m\"\u001b[39m\u001b[33mbackpacking\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mprocessFile\u001b[39m\u001b[34m(path, party, without_stopwords)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocessFile\u001b[39m(path, party, without_stopwords=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Set seeds for reproducibility\u001b[39;00m\n\u001b[32m      3\u001b[39m     random.seed(\u001b[32m23\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mnp\u001b[49m.random.seed(\u001b[32m23\u001b[39m)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Download necessary NLTK resources\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    processFile(filePathforDemocrats, \"democrats\")\n",
    "    processFile(filePathforRepublican, \"republican\")\n",
    "    processFile(filePathforBackpacking, \"backpacking\")\n",
    "    processFile(filePathforvagabond, \"vagabond\")    \n",
    "    print(\"Done :>\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472d8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compare_party_embeddings_by_period(dem_model, rep_model, time_period, output_file=None):\n",
    "    \"\"\"Compare two word2vec models globally by aligning their vector spaces\"\"\"\n",
    "    # Find common vocabulary\n",
    "    vocab_dem = set(dem_model.wv.index_to_key)\n",
    "    vocab_rep = set(rep_model.wv.index_to_key)\n",
    "    common_vocab = list(vocab_dem.intersection(vocab_rep))\n",
    "    \n",
    "    # Extract embeddings for common words\n",
    "    vectors_dem = np.array([dem_model.wv[word] for word in common_vocab])\n",
    "    vectors_rep = np.array([rep_model.wv[word] for word in common_vocab])\n",
    "    \n",
    "    # Compute the best rotational alignment (orthogonal Procrustes)\n",
    "    m = vectors_dem.T @ vectors_rep\n",
    "    u, _, vt = np.linalg.svd(m)\n",
    "    rotation = u @ vt\n",
    "    \n",
    "    # Apply rotation to align model2's space with model1's space\n",
    "    vectors_rep_aligned = vectors_rep @ rotation\n",
    "    \n",
    "    # Calculate word by word similarities\n",
    "    similarities = []\n",
    "    for i, word in enumerate(common_vocab):\n",
    "        sim = cosine_similarity(vectors_dem[i].reshape(1, -1), \n",
    "                                vectors_rep_aligned[i].reshape(1, -1))[0][0]\n",
    "        similarities.append((word, sim))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(similarities, columns=['word', 'similarity'])\n",
    "    \n",
    "    # Add context data - most similar words in each model\n",
    "    dem_context = []\n",
    "    rep_context = []\n",
    "    \n",
    "    for word in df['word']:\n",
    "        try:\n",
    "            dem_similar = [w for w, _ in dem_model.wv.most_similar(word, topn=10)]\n",
    "            dem_context.append(\", \".join(dem_similar))\n",
    "        except:\n",
    "            dem_context.append(\"\")\n",
    "        \n",
    "        try:\n",
    "            rep_similar = [w for w, _ in rep_model.wv.most_similar(word, topn=10)]\n",
    "            rep_context.append(\", \".join(rep_similar))\n",
    "        except:\n",
    "            rep_context.append(\"\")\n",
    "    \n",
    "    df['dem_context'] = dem_context\n",
    "    df['rep_context'] = rep_context\n",
    "    df['time_period'] = time_period\n",
    "    \n",
    "    # Sort by similarity (most different words first)\n",
    "    df = df.sort_values('similarity')\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if output_file:\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_democrats_before_2016 = gensim.models.Word2Vec.load(\"models/models_distribution/reddit_word2vec_democrats_before_2016.model\")\n",
    "model_republican_before_2016 = gensim.models.Word2Vec.load(\"models/models_distribution/reddit_word2vec_republican_before_2016.model\")\n",
    "model_democrats_2017_2020 = gensim.models.Word2Vec.load(\"models/models_distribution/reddit_word2vec_democrats_2017_2020.model\")\n",
    "model_republican_2017_2020 = gensim.models.Word2Vec.load(\"models/models_distribution/reddit_word2vec_republican_2017_2020.model\")\n",
    "model_democrats_2021_2024 = gensim.models.Word2Vec.load(\"models/models_distribution/reddit_word2vec_democrats_2021_2024.model\")\n",
    "model_republican_2021_2024 = gensim.models.Word2Vec.load(\"models/models_distribution/reddit_word2vec_republican_2021_2024.model\")\n",
    "\n",
    "# Before 2016\n",
    "df_before_2016 = compare_party_embeddings_by_period(\n",
    "    model_democrats_before_2016, \n",
    "    model_republican_before_2016,\n",
    "    \"before_2016\",\n",
    "    output_file=\"output/output_distribution/party_comparison_before_2016.csv\"\n",
    ")\n",
    "\n",
    "# 2017-2020\n",
    "df_2017_2020 = compare_party_embeddings_by_period(\n",
    "    model_democrats_2017_2020, \n",
    "    model_republican_2017_2020,\n",
    "    \"2017_2020\",\n",
    "    output_file=\"output/output_distribution/party_comparison_2017_2020.csv\"\n",
    ")\n",
    "\n",
    "# 2021-2024\n",
    "df_2021_2024 = compare_party_embeddings_by_period(\n",
    "    model_democrats_2021_2024, \n",
    "    model_republican_2021_2024,\n",
    "    \"2021_2024\",\n",
    "    output_file=\"output/output_distribution/party_comparison_2021_2024.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dcea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV files\n",
    "df_before_2016 = pd.read_csv(\"output/output_distribution/party_comparison_before_2016.csv\")\n",
    "df_2017_2020 = pd.read_csv(\"output/output_distribution/party_comparison_2017_2020.csv\") \n",
    "df_2021_2024 = pd.read_csv(\"output/output_distribution/party_comparison_2021_2024.csv\")\n",
    "\n",
    "\n",
    "\n",
    "def analyze_similarity_distributions():\n",
    "    \"\"\"Analyze how cosine similarity distributions change over time\"\"\"\n",
    "    \n",
    "    # Prepare data\n",
    "    dataframes = [df_before_2016, df_2017_2020, df_2021_2024]\n",
    "    period_names = ['Before 2016', '2017-2020', '2021-2024']\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    # Main distribution plots\n",
    "    for i, (df, period) in enumerate(zip(dataframes, period_names)):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        \n",
    "        # Create histogram\n",
    "        plt.hist(df['similarity'], bins=50, alpha=0.7, density=True, \n",
    "                color=['#1f77b4', '#ff7f0e', '#2ca02c'][i])\n",
    "        \n",
    "        # Add statistics\n",
    "        mean_sim = df['similarity'].mean()\n",
    "        std_sim = df['similarity'].std()\n",
    "        median_sim = df['similarity'].median()\n",
    "        \n",
    "        plt.axvline(mean_sim, color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Mean: {mean_sim:.3f}')\n",
    "        plt.axvline(median_sim, color='purple', linestyle=':', linewidth=2,\n",
    "                   label=f'Median: {median_sim:.3f}')\n",
    "        \n",
    "        plt.title(f'Similarity Distribution: {period}', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Cosine Similarity', fontsize=12)\n",
    "        plt.ylabel('Density', fontsize=12)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Combined comparison plot\n",
    "    plt.subplot(2, 3, 4)\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "    for i, (df, period, color) in enumerate(zip(dataframes, period_names, colors)):\n",
    "        plt.hist(df['similarity'], bins=30, alpha=0.6, density=True, \n",
    "                label=period, color=color)\n",
    "    \n",
    "    plt.title('Similarity Distributions Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Cosine Similarity', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COSINE SIMILARITY DISTRIBUTION ANALYSIS\")\n",
    "    \n",
    "    for df, period in zip(dataframes, period_names):\n",
    "        print(f\"\\n{period} Statistics:\")\n",
    "        print(f\"  Mean similarity: {df['similarity'].mean():.4f}\")\n",
    "        print(f\"  Median similarity: {df['similarity'].median():.4f}\")\n",
    "        print(f\"  Std similarity: {df['similarity'].std():.4f}\")\n",
    "        print(f\"  Min similarity: {df['similarity'].min():.4f}\")\n",
    "        print(f\"  Max similarity: {df['similarity'].max():.4f}\")\n",
    "        print(f\"  Words with similarity < 0.0: {(df['similarity'] < 0.0).sum()}/{len(df)} ({(df['similarity'] < 0.0).mean()*100:.1f}%)\")\n",
    "        print(f\"  Words with similarity < -0.1: {(df['similarity'] < -0.1).sum()}/{len(df)} ({(df['similarity'] < -0.1).mean()*100:.1f}%)\")\n",
    "\n",
    "\n",
    "# Run the analysis\n",
    "similarity_stats = analyze_similarity_distributions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

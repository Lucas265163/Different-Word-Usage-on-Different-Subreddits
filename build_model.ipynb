{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "326f3dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For streaming\n",
    "import sys\n",
    "version = sys.version_info\n",
    "if version.major < 3 or (version.major == 3 and version.minor < 10):\n",
    "    raise RuntimeError(\"This script requires Python 3.10 or higher\")\n",
    "import os\n",
    "from typing import Iterable\n",
    "\n",
    "from fileStreams import getFileJsonStream\n",
    "from utils import FileProgressLog\n",
    "\n",
    "# For processing\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "    \n",
    "import datetime\n",
    "\n",
    "\n",
    "filePathforDemocrats = r\"datasets/democrats_comments.zst\"\n",
    "filePathforRepublican = r\"datasets/Republican_comments.zst\"\n",
    "recursive = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feab659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word2vect_model(path, party, without_stopwords=True):\n",
    "    print(f\"Processing file {path}\")\n",
    "    # Download necessary NLTK resources\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)  # For POS tagging\n",
    "    \n",
    "    # Initialize lemmatizer and stop words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Create empty lists for each time period\n",
    "    chunks = {\n",
    "        \"before_2016\": [],\n",
    "        \"2017_2020\": [],\n",
    "        \"2021_2024\": [],\n",
    "    }\n",
    "    \n",
    "    # Track counts\n",
    "    counts = {period: 0 for period in chunks.keys()}\n",
    "    \n",
    "    with open(path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Skipping unknown file {path}\")\n",
    "            return\n",
    "        for row in tqdm(jsonStream, desc=\"Processing comments\"):\n",
    "            if \"body\" not in row or \"created_utc\" not in row:\n",
    "                continue\n",
    "            \n",
    "            # Get the comment text and timestamp\n",
    "            text = row[\"body\"]\n",
    "            created_timestamp = row[\"created_utc\"]\n",
    "            \n",
    "            # Convert timestamp to year\n",
    "            year = datetime.datetime.fromtimestamp(int(created_timestamp)).year\n",
    "            \n",
    "            # Determine which chunk this comment belongs to\n",
    "            chunk_key = None\n",
    "            if year <= 2016:\n",
    "                chunk_key = \"before_2016\"\n",
    "            elif 2017 <= year <= 2020:\n",
    "                chunk_key = \"2017_2020\"\n",
    "            elif 2021 <= year <= 2024:\n",
    "                chunk_key = \"2021_2024\"\n",
    "            \n",
    "            # Process text\n",
    "            # Remove URLs\n",
    "            txt = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "            \n",
    "            # Remove non-alphanumeric characters and convert to lowercase\n",
    "            txt = re.sub(\"[^A-Za-z0-9']+\", ' ', txt).lower()\n",
    "            \n",
    "            # Replace special characters with spaces\n",
    "            txt = re.sub(r\"['\\-_]\", ' ', txt)\n",
    "            \n",
    "            # Tokenize\n",
    "            words = txt.split()\n",
    "            \n",
    "            # Tag words with parts of speech for better lemmatization\n",
    "            tagged_words = nltk.pos_tag(words)\n",
    "            processed_words = []\n",
    "            \n",
    "            # Skip empty comments\n",
    "            if not words:\n",
    "                continue\n",
    "            \n",
    "            for word, tag in tagged_words:\n",
    "                if without_stopwords:\n",
    "                    if word in stop_words:\n",
    "                        continue\n",
    "                \n",
    "                # Convert Penn Treebank tag to WordNet tag\n",
    "                if tag.startswith('J'):\n",
    "                    wordnet_pos = 'a'  # adjective\n",
    "                elif tag.startswith('V'):\n",
    "                    wordnet_pos = 'v'  # verb\n",
    "                elif tag.startswith('N'):\n",
    "                    wordnet_pos = 'n'  # noun\n",
    "                elif tag.startswith('R'):\n",
    "                    wordnet_pos = 'r'  # adverb\n",
    "                else:\n",
    "                    wordnet_pos = 'n'  # default to noun\n",
    "                    \n",
    "                # Lemmatize with the correct POS\n",
    "                lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "                processed_words.append(lemma)\n",
    "            \n",
    "            # Add to appropriate chunk if it has words\n",
    "            if processed_words:\n",
    "                chunks[chunk_key].append(processed_words)\n",
    "                counts[chunk_key] += 1\n",
    "                \n",
    "    # Print statistics\n",
    "    print(\"\\n=== Comment Counts by Period ===\")\n",
    "    for period, count in counts.items():\n",
    "        print(f\"{period}: {count} comments\")\n",
    "        \n",
    "    # Extract bigrams from each time period\n",
    "    for period, comments in chunks.items():\n",
    "        if len(comments) > 0:\n",
    "            print(f\"\\nExtracting bigrams for {period}...\")\n",
    "            # Build bigram model\n",
    "            phrases = Phrases(comments, min_count=5, threshold=10)\n",
    "            bigram_model = Phraser(phrases)\n",
    "            \n",
    "            # Apply bigram model to create comments with bigrams\n",
    "            bigrammed_comments = [bigram_model[comment] for comment in comments]\n",
    "            chunks[period] = bigrammed_comments\n",
    "        \n",
    "    # Train a Word2Vec model for each time period\n",
    "    for period, comments in chunks.items():\n",
    "        if len(comments) > 0:\n",
    "            print(f\"\\n=== Training Word2Vec for {period} ({len(comments)} comments) ===\")\n",
    "            \n",
    "            # Initialize and train model\n",
    "            model = Word2Vec(\n",
    "                vector_size=300,\n",
    "                window=5,\n",
    "                min_count=5,\n",
    "                workers=16\n",
    "            )\n",
    "            \n",
    "            # Build vocabulary\n",
    "            model.build_vocab(comments)\n",
    "            print(f\"Vocabulary size: {len(model.wv.index_to_key)}\")\n",
    "            \n",
    "            # Train the model\n",
    "            model.train(\n",
    "                comments, \n",
    "                total_examples=len(comments), \n",
    "                epochs=5\n",
    "            )\n",
    "            \n",
    "            # Save the model\n",
    "            model_path = f\"reddit_word2vec_{party}_{period}.model\"\n",
    "            model.save(model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "            \n",
    "            # Show example results\n",
    "            print(\"\\n=== Example Results ===\")\n",
    "            test_words = [\"democracy\", \"president\", \"policy\", \"vote\", \"climate\"]\n",
    "            for word in test_words:\n",
    "                try:\n",
    "                    print(f\"\\nWords similar to '{word}':\")\n",
    "                    similar = model.wv.most_similar(word, topn=3)\n",
    "                    for similar_word, similarity in similar:\n",
    "                        print(f\"  {similar_word}: {similarity:.4f}\")\n",
    "                except KeyError:\n",
    "                    print(f\"  '{word}' not found in {period} vocabulary\")\n",
    "        \n",
    "def main():\n",
    "    build_word2vect_model(filePathforDemocrats, \"democrats_with_stopwords\", without_stopwords=False)\n",
    "    build_word2vect_model(filePathforRepublican, \"republican_with_stopwords\", without_stopwords=False)\n",
    "    \n",
    "    print(\"Done :>\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10273cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your data (replace with your actual file path)\n",
    "df = pd.read_json('datasets/democrats_comments.zst', lines=True, compression='infer')\n",
    "\n",
    "# Count the number of comments per user\n",
    "user_comment_counts = df['author'].value_counts().reset_index()\n",
    "user_comment_counts.columns = ['author', 'comment_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8824ff60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>comment_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>440324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>49681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VegaThePunisher</td>\n",
       "      <td>21638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>backpackwayne</td>\n",
       "      <td>11345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kopskey1</td>\n",
       "      <td>9504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>michaelconfoy</td>\n",
       "      <td>6573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gsteel11</td>\n",
       "      <td>6286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kerryfinchelhillary</td>\n",
       "      <td>6180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>raistlin65</td>\n",
       "      <td>4895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>therecordcorrected</td>\n",
       "      <td>4476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bokono</td>\n",
       "      <td>4383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dolphins3</td>\n",
       "      <td>3668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Btravelen</td>\n",
       "      <td>3179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>walter1950</td>\n",
       "      <td>2942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>behindmyscreen</td>\n",
       "      <td>2743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TechyGuyInIL</td>\n",
       "      <td>2718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>election_info_bot</td>\n",
       "      <td>2640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>wenchette</td>\n",
       "      <td>2638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Raspberries-Are-Evil</td>\n",
       "      <td>2442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>floofnstuff</td>\n",
       "      <td>2439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  author  comment_count\n",
       "0              [deleted]         440324\n",
       "1          AutoModerator          49681\n",
       "2        VegaThePunisher          21638\n",
       "3          backpackwayne          11345\n",
       "4               kopskey1           9504\n",
       "5          michaelconfoy           6573\n",
       "6               Gsteel11           6286\n",
       "7    kerryfinchelhillary           6180\n",
       "8             raistlin65           4895\n",
       "9     therecordcorrected           4476\n",
       "10                bokono           4383\n",
       "11             dolphins3           3668\n",
       "12             Btravelen           3179\n",
       "13            walter1950           2942\n",
       "14        behindmyscreen           2743\n",
       "15          TechyGuyInIL           2718\n",
       "16     election_info_bot           2640\n",
       "17             wenchette           2638\n",
       "18  Raspberries-Are-Evil           2442\n",
       "19           floofnstuff           2439"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the table of user comment frequencies\n",
    "user_comment_counts.head(20)  # Show top 20 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fcec56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users with fewer than 5 comments: 85820\n"
     ]
    }
   ],
   "source": [
    "# Count how many users have fewer than 5 comments\n",
    "num_users_less_than_5 = (user_comment_counts['comment_count'] < 3).sum()\n",
    "print(f'Number of users with fewer than 5 comments: {num_users_less_than_5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a58c842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file datasets/democrats_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments: 2011525it [07:46, 4314.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comment Counts by Period ===\n",
      "before_2016: 0 comments\n",
      "2017_2020: 0 comments\n",
      "2021_2024: 1343920 comments\n",
      "before_2016: 0 comments after filtering words by user count\n",
      "2017_2020: 0 comments after filtering words by user count\n",
      "2021_2024: 1342207 comments after filtering words by user count\n",
      "\n",
      "Extracting bigrams for 2021_2024...\n",
      "\n",
      "=== Training Word2Vec for 2021_2024 (1342207 comments) ===\n",
      "Vocabulary size: 28032\n",
      "Model saved to models/reddit_word2vec_10_10_democrats_2021_2024.model\n",
      "Processing file datasets/Republican_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments: 1405486it [03:54, 6005.26it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comment Counts by Period ===\n",
      "before_2016: 0 comments\n",
      "2017_2020: 0 comments\n",
      "2021_2024: 617471 comments\n",
      "before_2016: 0 comments after filtering words by user count\n",
      "2017_2020: 0 comments after filtering words by user count\n",
      "2021_2024: 616459 comments after filtering words by user count\n",
      "\n",
      "Extracting bigrams for 2021_2024...\n",
      "\n",
      "=== Training Word2Vec for 2021_2024 (616459 comments) ===\n",
      "Vocabulary size: 20147\n",
      "Model saved to models/reddit_word2vec_10_10_republican_2021_2024.model\n",
      "Done :>\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_word2vect_model(path, party, without_stopwords=True, phrases_min_count=5, word2vec_min_count=5):\n",
    "    print(f\"Processing file {path}\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # For each period, track comments and user-word usage\n",
    "    chunks = {\n",
    "        \"before_2016\": [],\n",
    "        \"2017_2020\": [],\n",
    "        \"2021_2024\": [],\n",
    "    }\n",
    "    user_words = {\n",
    "        \"before_2016\": defaultdict(set),\n",
    "        \"2017_2020\": defaultdict(set),\n",
    "        \"2021_2024\": defaultdict(set),\n",
    "    }\n",
    "    user_comments = {\n",
    "        \"before_2016\": defaultdict(list),\n",
    "        \"2017_2020\": defaultdict(list),\n",
    "        \"2021_2024\": defaultdict(list),\n",
    "    }\n",
    "    counts = {period: 0 for period in chunks.keys()}\n",
    "\n",
    "    with open(path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Skipping unknown file {path}\")\n",
    "            return\n",
    "        for row in tqdm(jsonStream, desc=\"Processing comments\"):\n",
    "            if \"body\" not in row or \"created_utc\" not in row or \"author\" not in row:\n",
    "                continue\n",
    "            author = row[\"author\"]\n",
    "            if author in {\"AutoModerator\", \"election_info_bot\"}:\n",
    "                continue\n",
    "            text = row[\"body\"]\n",
    "            created_timestamp = row[\"created_utc\"]\n",
    "            year = datetime.datetime.fromtimestamp(int(created_timestamp)).year\n",
    "            # if year <= 2016:\n",
    "            #     chunk_key = \"before_2016\"\n",
    "            # elif 2017 <= year <= 2020:\n",
    "            #     chunk_key = \"2017_2020\"\n",
    "            # elif 2021 <= year <= 2024:\n",
    "            #     chunk_key = \"2021_2024\"\n",
    "            # else:\n",
    "            #     continue\n",
    "            if 2021 <= year <= 2024:\n",
    "                chunk_key = \"2021_2024\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            txt = re.sub(r'http\\S+', '', text)\n",
    "            txt = re.sub(\"[^A-Za-z']+\", ' ', txt).lower()\n",
    "            txt = re.sub(r\"['\\-]\", ' ', txt)\n",
    "            words = txt.split()\n",
    "            if not words:\n",
    "                continue\n",
    "            tagged_words = nltk.pos_tag(words)\n",
    "            processed_words = []\n",
    "            for word, tag in tagged_words:\n",
    "                if without_stopwords and word in stop_words:\n",
    "                    continue\n",
    "                if tag.startswith('J'):\n",
    "                    wordnet_pos = 'a'\n",
    "                elif tag.startswith('V'):\n",
    "                    wordnet_pos = 'v'\n",
    "                elif tag.startswith('N'):\n",
    "                    wordnet_pos = 'n'\n",
    "                elif tag.startswith('R'):\n",
    "                    wordnet_pos = 'r'\n",
    "                else:\n",
    "                    wordnet_pos = 'n'\n",
    "                lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "                processed_words.append(lemma)\n",
    "                user_words[chunk_key][lemma].add(author)\n",
    "            if processed_words:\n",
    "                user_comments[chunk_key][author].append(processed_words)\n",
    "                counts[chunk_key] += 1\n",
    "\n",
    "    print(\"\\n=== Comment Counts by Period ===\")\n",
    "    for period, count in counts.items():\n",
    "        print(f\"{period}: {count} comments\")\n",
    "\n",
    "    # Filter words by user count and rebuild comments for each period\n",
    "    for period in chunks.keys():\n",
    "        valid_words = {w for w, users in user_words[period].items() if len(users) >= 5}\n",
    "        filtered_comments = []\n",
    "        for comments in user_comments[period].values():\n",
    "            for comment in comments:\n",
    "                filtered = [w for w in comment if w in valid_words]\n",
    "                if filtered:\n",
    "                    filtered_comments.append(filtered)\n",
    "        print(f\"{period}: {len(filtered_comments)} comments after filtering words by user count\")\n",
    "        if filtered_comments:\n",
    "            print(f\"\\nExtracting bigrams for {period}...\")\n",
    "            phrases = Phrases(filtered_comments, \n",
    "                              min_count=phrases_min_count, \n",
    "                              threshold=100)\n",
    "            bigram_model = Phraser(phrases)\n",
    "            bigrammed_comments = [bigram_model[comment] for comment in filtered_comments]\n",
    "            chunks[period] = bigrammed_comments\n",
    "        else:\n",
    "            chunks[period] = []\n",
    "\n",
    "    # Train a Word2Vec model for each time period\n",
    "    for period, comments in chunks.items():\n",
    "        if len(comments) > 0:\n",
    "            print(f\"\\n=== Training Word2Vec for {period} ({len(comments)} comments) ===\")\n",
    "            model = Word2Vec(\n",
    "                vector_size=300,\n",
    "                window=5,\n",
    "                min_count=word2vec_min_count,\n",
    "                workers=16\n",
    "            )\n",
    "            model.build_vocab(comments)\n",
    "            print(f\"Vocabulary size: {len(model.wv.index_to_key)}\")\n",
    "            model.train(\n",
    "                comments,\n",
    "                total_examples=len(comments),\n",
    "                epochs=5\n",
    "            )\n",
    "            model_path = f\"models/reddit_word2vec_{phrases_min_count}_{word2vec_min_count}_filterd_{party}_{period}.model\"\n",
    "            model.save(model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def main():\n",
    "    build_word2vect_model(filePathforDemocrats, \"democrats\", without_stopwords=False, \n",
    "                          phrases_min_count=5, word2vec_min_count=5)\n",
    "    build_word2vect_model(filePathforRepublican, \"republican\", without_stopwords=False, \n",
    "                          phrases_min_count=5, word2vec_min_count=5)\n",
    "    build_word2vect_model(filePathforDemocrats, \"democrats\", without_stopwords=False, \n",
    "                          phrases_min_count=10, word2vec_min_count=10)\n",
    "    build_word2vect_model(filePathforRepublican, \"republican\", without_stopwords=False, \n",
    "                          phrases_min_count=10, word2vec_min_count=10)\n",
    "    build_word2vect_model(filePathforDemocrats, \"democrats\", without_stopwords=False, \n",
    "                          phrases_min_count=20, word2vec_min_count=20)\n",
    "    build_word2vect_model(filePathforRepublican, \"republican\", without_stopwords=False, \n",
    "                          phrases_min_count=20, word2vec_min_count=20)\n",
    "    build_word2vect_model(filePathforDemocrats, \"democrats\", without_stopwords=False, \n",
    "                          phrases_min_count=50, word2vec_min_count=50)\n",
    "    build_word2vect_model(filePathforRepublican, \"republican\", without_stopwords=False, \n",
    "                          phrases_min_count=50, word2vec_min_count=50)\n",
    "    print(\"Done :>\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

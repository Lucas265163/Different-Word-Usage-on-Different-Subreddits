{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d0c47d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading global bigram model from models/bigram/political_bigram_1.phr\n",
      "Loaded 1000000 comments from processed_comments_1/democrats\\democrats_batch1.pkl\n",
      "Loaded 933011 comments from processed_comments_1/democrats\\democrats_batch2.pkl\n",
      "Processing chunk of 1000000 comments for 2021_2024\n",
      "2021_2024 vocabulary size: 28457\n",
      "Processing final 127161 comments for before_2016\n",
      "before_2016 vocabulary size: 11534\n",
      "Processing final 472651 comments for 2017_2020\n",
      "2017_2020 vocabulary size: 19525\n",
      "Processing final 333199 comments for 2021_2024\n",
      "2021_2024 vocabulary size: 28704\n",
      "Model saved to models/chunk_1\n",
      "Completed building models for democrats\n",
      "Loading global bigram model from models/bigram/political_bigram_1.phr\n",
      "Loaded 1000000 comments from processed_comments_1/republican\\republican_batch1.pkl\n",
      "Loaded 290701 comments from processed_comments_1/republican\\republican_batch2.pkl\n",
      "Processing final 263564 comments for before_2016\n",
      "before_2016 vocabulary size: 17932\n",
      "Processing final 414712 comments for 2017_2020\n",
      "2017_2020 vocabulary size: 19067\n",
      "Processing final 612425 comments for 2021_2024\n",
      "2021_2024 vocabulary size: 21754\n",
      "Model saved to models/chunk_1\n",
      "Completed building models for republican\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "def get_date_from_comment(comment):\n",
    "    \"\"\"Extract date from a comment dictionary\"\"\"\n",
    "    try:\n",
    "        return datetime.datetime.strptime(comment[\"date\"], \"%Y-%m-%d\").date()\n",
    "    except (KeyError, ValueError):\n",
    "        try:\n",
    "            return datetime.datetime.fromtimestamp(int(comment[\"timestamp\"])).date()\n",
    "        except (KeyError, ValueError):\n",
    "            return None\n",
    "\n",
    "def get_period(date):\n",
    "    \"\"\"Determine which time period a date belongs to\"\"\"\n",
    "    if date is None:\n",
    "        return None\n",
    "    year = date.year\n",
    "    if year <= 2016:\n",
    "        return \"before_2016\"\n",
    "    elif 2017 <= year <= 2020:\n",
    "        return \"2017_2020\"\n",
    "    elif 2021 <= year <= 2024:\n",
    "        return \"2021_2024\"\n",
    "    return None\n",
    "\n",
    "def build_bigram_model(comments):\n",
    "    \"\"\"Build a bigram model for the given comments\"\"\"\n",
    "    sentences = []\n",
    "    for comment in comments:\n",
    "        if \"processed_text\" in comment:\n",
    "            sentences.append(comment[\"processed_text\"])\n",
    "    phrases = Phrases(sentences, min_count=10, threshold=10)\n",
    "    return Phraser(phrases)\n",
    "\n",
    "def apply_bigrams(comments, bigram_model):\n",
    "    \"\"\"Apply bigram model to comments\"\"\"\n",
    "    processed = []\n",
    "    for comment in comments:\n",
    "        if \"processed_text\" in comment:\n",
    "            processed.append(bigram_model[comment[\"processed_text\"]])\n",
    "    return processed\n",
    "\n",
    "def create_or_update_model(period, comments, vector_size, window, min_count, workers, sg, epochs, existing_model=None):\n",
    "    \"\"\"Create a new model or update an existing one\"\"\"\n",
    "    if existing_model is None:\n",
    "        model = Word2Vec(\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=min_count,\n",
    "            workers=workers,\n",
    "            sg=sg,\n",
    "            seed=23\n",
    "        )\n",
    "        model.build_vocab(comments)\n",
    "        print(f\"{period} vocabulary size: {len(model.wv.index_to_key)}\")\n",
    "    else:\n",
    "        model = existing_model\n",
    "        model.build_vocab(comments, update=True)\n",
    "        print(f\"{period} vocabulary size: {len(model.wv.index_to_key)}\")\n",
    "    model.train(comments, total_examples=len(comments), epochs=epochs)\n",
    "    return model\n",
    "\n",
    "def save_model(model, subreddit, period, model_dir, is_interim=False):\n",
    "    \"\"\"Save model to disk\"\"\"\n",
    "    if is_interim:\n",
    "        path = f\"{model_dir}/interim/{subreddit}_{period}_interim.model\"\n",
    "    else:\n",
    "        path = f\"{model_dir}/{subreddit}_{period}.model\"\n",
    "    model.save(path)\n",
    "\n",
    "def build_models_for_subreddit(\n",
    "    subreddit,\n",
    "    base_data_dir,\n",
    "    model_dir,\n",
    "    vector_size=300,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    epochs=5,\n",
    "    workers=16,\n",
    "    sg=0,\n",
    "    min_comments_to_train=10000,\n",
    "    chunk_size=1000000,\n",
    "    global_bigram_path=None\n",
    "):\n",
    "\n",
    "    time_periods = [\"before_2016\", \"2017_2020\", \"2021_2024\"]\n",
    "    models = {period: None for period in time_periods}\n",
    "    bigram_models = {period: None for period in time_periods}\n",
    "    \n",
    "    # Load global bigram model if exists\n",
    "    global_bigram_path = global_bigram_path\n",
    "    if os.path.exists(global_bigram_path):\n",
    "        print(f\"Loading global bigram model from {global_bigram_path}\")\n",
    "        global_bigram_model = Phraser.load(global_bigram_path)\n",
    "    else:\n",
    "        print(f\"Global bigram model not found at {global_bigram_path}, will train on each chunk.\")\n",
    "        global_bigram_model = None\n",
    "        return\n",
    "\n",
    "    # Find all pickle files\n",
    "    pattern = f\"{base_data_dir}/{subreddit}/{subreddit}_batch*.pkl\"\n",
    "    pickle_files = sorted(glob.glob(pattern))\n",
    "    if not pickle_files:\n",
    "        print(f\"No pickle files found for {subreddit} in {base_data_dir}/{subreddit}/\")\n",
    "        return\n",
    "\n",
    "    comments_by_period = {period: [] for period in time_periods}\n",
    "\n",
    "    for file_path in pickle_files:\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                comments = pickle.load(f)\n",
    "            print(f\"Loaded {len(comments)} comments from {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for comment in comments:\n",
    "            date = get_date_from_comment(comment)\n",
    "            period = get_period(date)\n",
    "            if period:\n",
    "                comments_by_period[period].append(comment)\n",
    "\n",
    "        for period in time_periods:\n",
    "            period_comments = comments_by_period[period]\n",
    "            while len(period_comments) >= chunk_size:\n",
    "                print(f\"Processing chunk of {chunk_size} comments for {period}\")\n",
    "                chunk = period_comments[:chunk_size]\n",
    "                period_comments = period_comments[chunk_size:]\n",
    "\n",
    "                # Use global bigram model if exists, otherwise train on each chunk\n",
    "                if global_bigram_model is not None:\n",
    "                    bigram_model = global_bigram_model\n",
    "                else:\n",
    "                    bigram_model = build_bigram_model(chunk)\n",
    "                bigram_models[period] = bigram_model\n",
    "                processed_chunk = apply_bigrams(chunk, bigram_model)\n",
    "\n",
    "                if len(processed_chunk) > min_comments_to_train:\n",
    "                    model = create_or_update_model(\n",
    "                        period, processed_chunk, vector_size, window, min_count, workers, sg, epochs, models[period]\n",
    "                    )\n",
    "                    models[period] = model\n",
    "                    save_model(model, subreddit, period, model_dir, is_interim=True)\n",
    "            comments_by_period[period] = period_comments\n",
    "\n",
    "    # Process any remaining comments\n",
    "    for period, remaining_comments in comments_by_period.items():\n",
    "        if len(remaining_comments) > min_comments_to_train:\n",
    "            print(f\"Processing final {len(remaining_comments)} comments for {period}\")\n",
    "            if global_bigram_model is not None:\n",
    "                bigram_model = global_bigram_model\n",
    "            else:\n",
    "                bigram_model = build_bigram_model(remaining_comments)\n",
    "            bigram_models[period] = bigram_model\n",
    "            processed_chunk = apply_bigrams(remaining_comments, bigram_model)\n",
    "            model = create_or_update_model(\n",
    "                period, processed_chunk, vector_size, window, min_count, workers, sg, epochs, models[period]\n",
    "            )\n",
    "            models[period] = model\n",
    "            save_model(model, subreddit, period, model_dir, is_interim=False)\n",
    "        else:\n",
    "            print(f\"Skipping final {len(remaining_comments)} comments for {period} (less than minimum required)\")\n",
    "\n",
    "    # Save final models\n",
    "    for period, model in models.items():\n",
    "        if model is not None:\n",
    "            save_model(model, subreddit, period, model_dir, is_interim=False)\n",
    "    print(f\"Model saved to {model_dir}\")\n",
    "    print(f\"Completed building models for {subreddit}\")\n",
    "\n",
    "def main():\n",
    "    model_dir = \"models/chunk_1\"\n",
    "    global_bigram_path = \"models/bigram/political_bigram_1.phr\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(f\"{model_dir}/interim\", exist_ok=True)\n",
    "    random.seed(23)\n",
    "    np.random.seed(23)\n",
    "    subreddits = [\"democrats\", \"republican\"]\n",
    "    for subreddit in subreddits:\n",
    "        build_models_for_subreddit(\n",
    "            subreddit,\n",
    "            base_data_dir=\"processed_comments_1\",\n",
    "            model_dir=model_dir,\n",
    "            vector_size=300,\n",
    "            window=5,\n",
    "            min_count=10,\n",
    "            epochs=5,\n",
    "            workers=16,\n",
    "            sg=0,\n",
    "            min_comments_to_train=10000,\n",
    "            chunk_size=1000000,\n",
    "            global_bigram_path=global_bigram_path\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Changes made:\n",
    "# Using global bigram model, set min_count=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35b65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For streaming\n",
    "import sys\n",
    "version = sys.version_info\n",
    "if version.major < 3 or (version.major == 3 and version.minor < 10):\n",
    "    raise RuntimeError(\"This script requires Python 3.10 or higher\")\n",
    "import os\n",
    "from typing import Iterable\n",
    "\n",
    "from fileStreams import getFileJsonStream\n",
    "from utils import FileProgressLog\n",
    "\n",
    "# For processing\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "    \n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import html\n",
    "import unicodedata\n",
    "\n",
    "# # Download necessary NLTK resources\n",
    "# nltk.download('stopwords', quiet=True)\n",
    "# nltk.download('wordnet', quiet=True)\n",
    "# nltk.download('punkt', quiet=True)\n",
    "# nltk.download('averaged_perceptron_tagger', quiet=True)  # For POS tagging\n",
    "\n",
    "recursive = False\n",
    "\n",
    "\n",
    "def processFile(path, party, output_dir, without_stopwords=True):\n",
    "    print(f\"Processing file {path}\")\n",
    "    # Initialize lemmatizer and stop words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Create empty lists for each time period\n",
    "    chunks = {\n",
    "        \"before_2016\": [],\n",
    "        \"2017_2020\": [],\n",
    "        \"2021_2024\": [],\n",
    "    }\n",
    "    \n",
    "    # Track counts\n",
    "    counts = {period: 0 for period in chunks.keys()}\n",
    "    \n",
    "    POS_CACHE = {}\n",
    "    LEMMA_CACHE = {}\n",
    "    \n",
    "    with open(path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Skipping unknown file {path}\")\n",
    "            return\n",
    "        for row in tqdm(jsonStream, desc=\"Processing comments\"):\n",
    "            if \"body\" not in row or \"created_utc\" not in row:\n",
    "                continue\n",
    "            \n",
    "            # Get the comment text and timestamp\n",
    "            text = row[\"body\"]\n",
    "            created_timestamp = row[\"created_utc\"]\n",
    "            author = row[\"author\"]\n",
    "            if author in {\"AutoModerator\", \"election_info_bot\"}:\n",
    "                continue\n",
    "            \n",
    "            # Convert timestamp to year\n",
    "            year = datetime.datetime.fromtimestamp(int(created_timestamp)).year\n",
    "            \n",
    "            # Determine which chunk this comment belongs to\n",
    "            chunk_key = None\n",
    "            if year <= 2016:\n",
    "                chunk_key = \"before_2016\"\n",
    "            elif 2017 <= year <= 2020:\n",
    "                chunk_key = \"2017_2020\"\n",
    "            elif 2021 <= year <= 2024:\n",
    "                chunk_key = \"2021_2024\"\n",
    "            \n",
    "            # Process text\n",
    "            # Handle HTML entities\n",
    "            text = html.unescape(text)\n",
    "            \n",
    "            # Unicode normalization\n",
    "            text = unicodedata.normalize('NFKD', text)\n",
    "            \n",
    "            # Remove URLs and Markdown formatting\n",
    "            text = re.sub(r'http\\S+', '', text)\n",
    "            text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)\n",
    "            text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)\n",
    "            text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)\n",
    "            text = re.sub(r'\\*(.*?)\\*', r'\\1', text)\n",
    "            \n",
    "            # Remove subreddit and user references\n",
    "            text = re.sub(r'/r/\\w+', '', text)\n",
    "            text = re.sub(r'r/\\w+', '', text)\n",
    "            text = re.sub(r'/u/\\w+', '', text)\n",
    "            text = re.sub(r'u/\\w+', '', text)\n",
    "            \n",
    "            # Basic text cleaning\n",
    "            text = re.sub(\"[^A-Za-z]+\", ' ', text).lower()\n",
    "            \n",
    "            # Remove single letters (except 'i')\n",
    "            text = re.sub(r'\\b([a-hj-z])\\b', '', text)\n",
    "            \n",
    "            # Tokenize\n",
    "            words = text.split()\n",
    "            \n",
    "            # Skip empty comments\n",
    "            if not words:\n",
    "                continue\n",
    "            \n",
    "            # Filter stop words\n",
    "            if without_stopwords:\n",
    "                words_to_tag = []\n",
    "                for word in words:\n",
    "                    if word not in stop_words:\n",
    "                        words_to_tag.append(word)\n",
    "            else:\n",
    "                words_to_tag = words[:]\n",
    "                \n",
    "            if not words_to_tag:\n",
    "                continue\n",
    "            \n",
    "            # POS Cache\n",
    "            uncached_words = []\n",
    "            for word in words_to_tag:\n",
    "                if word not in POS_CACHE:\n",
    "                    uncached_words.append(word)\n",
    "\n",
    "            if uncached_words:\n",
    "                tagged_uncached = nltk.pos_tag(uncached_words)\n",
    "                for word, tag in tagged_uncached:\n",
    "                    POS_CACHE[word] = tag\n",
    "\n",
    "            processed_words = []\n",
    "            for word in words_to_tag:\n",
    "                tag = POS_CACHE[word]\n",
    "                # Convert to wordnet_pos\n",
    "                if tag.startswith('J'):\n",
    "                    wordnet_pos = 'a'\n",
    "                elif tag.startswith('V'):\n",
    "                    wordnet_pos = 'v'\n",
    "                elif tag.startswith('N'):\n",
    "                    wordnet_pos = 'n'\n",
    "                elif tag.startswith('R'):\n",
    "                    wordnet_pos = 'r'\n",
    "                else:\n",
    "                    wordnet_pos = 'n'\n",
    "\n",
    "                lemma_key = (word, wordnet_pos)\n",
    "                if lemma_key in LEMMA_CACHE:\n",
    "                    lemma = LEMMA_CACHE[lemma_key]\n",
    "                else:\n",
    "                    lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "                    LEMMA_CACHE[lemma_key] = lemma\n",
    "\n",
    "                processed_words.append(lemma)\n",
    "\n",
    "            if processed_words:\n",
    "                chunks[chunk_key].append(processed_words)\n",
    "                counts[chunk_key] += 1\n",
    "                \n",
    "    # Print statistics\n",
    "    print(\"\\n=== Comment Counts by Period ===\")\n",
    "    for period, count in counts.items():\n",
    "        print(f\"{period}: {count} comments\")\n",
    "        \n",
    "    # Extract bigrams from each time period\n",
    "    for period, comments in chunks.items():\n",
    "        if len(comments) > 0:\n",
    "            # Build bigram model\n",
    "            phrases = Phrases(comments, min_count=5, threshold=10)\n",
    "            bigram_model = Phraser(phrases)\n",
    "            \n",
    "            # Apply bigram model to create comments with bigrams\n",
    "            bigrammed_comments = [bigram_model[comment] for comment in comments]\n",
    "            chunks[period] = bigrammed_comments\n",
    "        \n",
    "    # Train a Word2Vec model for each time period\n",
    "    for period, comments in chunks.items():\n",
    "        if len(comments) > 0:\n",
    "            # Initialize and train model\n",
    "            model = Word2Vec(\n",
    "                vector_size=300,\n",
    "                window=5,\n",
    "                min_count=5,\n",
    "                workers=16,\n",
    "                seed=23\n",
    "            )\n",
    "            \n",
    "            # Build vocabulary\n",
    "            model.build_vocab(comments)\n",
    "            print(f\"{period} vocabulary size: {len(model.wv.index_to_key)}\")\n",
    "            \n",
    "            # Train the model\n",
    "            model.train(\n",
    "                comments, \n",
    "                total_examples=len(comments), \n",
    "                epochs=5\n",
    "            )\n",
    "            \n",
    "            # Save the model\n",
    "            model_path = f\"{output_dir}/{party}_{period}.model\"\n",
    "            model.save(model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "            \n",
    "        \n",
    "def main():\n",
    "    filePathforDemocrats = r\"datasets/democrats_comments.zst\"\n",
    "    filePathforRepublican = r\"datasets/Republican_comments.zst\"\n",
    "    filePathforConservative = r\"datasets/Conservative_comments.zst\"\n",
    "    filePathforLiberal = r\"datasets/Liberal_comments.zst\"\n",
    "    filePathforVagabond = r\"datasets/vagabond_comments.zst\"\n",
    "    filePathforbackpacking = r\"datasets/backpacking_comments.zst\"\n",
    "\n",
    "\n",
    "    random.seed(23)\n",
    "    np.random.seed(23)\n",
    "    output_dir = \"models/final_full\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # processFile(filePathforDemocrats, \"democrats\", output_dir)\n",
    "    # processFile(filePathforRepublican, \"republican\", output_dir)\n",
    "    processFile(filePathforConservative, \"conservative\", output_dir)\n",
    "    processFile(filePathforLiberal, \"liberal\", output_dir)\n",
    "    processFile(filePathforVagabond, \"vagabond\", output_dir)\n",
    "    processFile(filePathforbackpacking, \"backpacking\", output_dir)\n",
    "    print(\"Done :>\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

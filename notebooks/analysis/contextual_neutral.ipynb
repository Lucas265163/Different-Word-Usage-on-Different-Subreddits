{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f993c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[democrats] before_2016: 4889 targets\n",
      "[democrats] 2017_2020: 11138 targets\n",
      "[democrats] 2021_2024: 13324 targets\n",
      "[republican] before_2016: 4889 targets\n",
      "[republican] 2017_2020: 11138 targets\n",
      "[republican] 2021_2024: 13324 targets\n",
      "No overlapping words for before_2016; CSV skipped.\n",
      "No overlapping words for 2017_2020; CSV skipped.\n",
      "No overlapping words for 2021_2024; CSV skipped.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phraser\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ------------------------------------------------------------------\n",
    "PERIOD_DEFINITIONS = {\n",
    "    \"before_2016\": range(2011, 2017),\n",
    "    \"2017_2020\": range(2017, 2021),\n",
    "    \"2021_2024\": range(2021, 2025),\n",
    "}\n",
    "SUBREDDITS = (\"democrats\", \"republican\")\n",
    "BASE_DATA_DIR = Path(\"processed_comments_2\")\n",
    "BIGRAM_MODEL_PATH = Path(\"../../models/bigram/political_bigram_1.phr\")\n",
    "NEUTRAL_MODEL_PATH = Path(\"../../models/neutral/neutral_2.model\")\n",
    "FREQ_FILE_DEM = Path(\"../../output/word_frequency/word_freq_yearly/democrats_withstopwords_year.csv\")\n",
    "FREQ_FILE_REP = Path(\"../../output/word_frequency/word_freq_yearly/republican_withstopwords_year.csv\")\n",
    "OUTPUT_DIR = Path(\"../../output/contextual/neutral\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "DATE_FIELDS = (\"year\", \"created_year\", \"created_utc\", \"created\")\n",
    "CONTEXT_WINDOW = 5\n",
    "EPS = 1e-8\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ------------------------------------------------------------------\n",
    "def ensure_path(path: Path) -> Path:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {path.resolve()}\")\n",
    "    return path\n",
    "\n",
    "def load_bigram_model(path: Path) -> Phraser:\n",
    "    return Phraser.load(str(ensure_path(path)))\n",
    "\n",
    "def load_neutral_model(path: Path) -> Word2Vec:\n",
    "    return Word2Vec.load(str(ensure_path(path)))\n",
    "\n",
    "def period_years(period: str) -> list[str]:\n",
    "    return [str(y) for y in PERIOD_DEFINITIONS.get(period, [])]\n",
    "\n",
    "def select_target_words(freq_file_dem: Path, freq_file_rep: Path, period: str) -> set[str]:\n",
    "    years = period_years(period)\n",
    "    if not years:\n",
    "        return set()\n",
    "    df_dem = pd.read_csv(ensure_path(freq_file_dem), index_col=0).reindex(columns=years).fillna(0)\n",
    "    df_rep = pd.read_csv(ensure_path(freq_file_rep), index_col=0).reindex(columns=years).fillna(0)\n",
    "    mask_dem = (df_dem >= 1).all(axis=1) & (df_dem.sum(axis=1) > 10)\n",
    "    mask_rep = (df_rep >= 1).all(axis=1) & (df_rep.sum(axis=1) > 10)\n",
    "    return set(df_dem.index[mask_dem]) & set(df_rep.index[mask_rep])\n",
    "\n",
    "def comment_year(comment: dict) -> int | None:\n",
    "    for field in DATE_FIELDS:\n",
    "        value = comment.get(field)\n",
    "        if value is None:\n",
    "            continue\n",
    "        if isinstance(value, (int, float)):\n",
    "            return datetime.utcfromtimestamp(value).year\n",
    "        if isinstance(value, str):\n",
    "            try:\n",
    "                return datetime.fromisoformat(value[:19]).year\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "def year_to_period(year: int | None) -> str | None:\n",
    "    if year is None:\n",
    "        return None\n",
    "    for period, years in PERIOD_DEFINITIONS.items():\n",
    "        if year in years:\n",
    "            return period\n",
    "    return None\n",
    "\n",
    "def iter_comments(base_dir: Path, subreddit: str):\n",
    "    pattern = base_dir / subreddit / f\"{subreddit}_batch*.pkl\"\n",
    "    for file_path in sorted(glob.glob(str(pattern))):\n",
    "        with open(file_path, \"rb\") as fh:\n",
    "            comments = pickle.load(fh)\n",
    "        for comment in comments:\n",
    "            yield comment\n",
    "\n",
    "def extract_context_vectors(\n",
    "    base_dir: Path,\n",
    "    subreddit: str,\n",
    "    bigram_model: Phraser,\n",
    "    target_words: set[str],\n",
    "    period: str,\n",
    "    embedding_model: Word2Vec,\n",
    ") -> dict[str, list[np.ndarray]]:\n",
    "    context_vectors: dict[str, list[np.ndarray]] = defaultdict(list)\n",
    "    vocab = embedding_model.wv.key_to_index\n",
    "\n",
    "    for comment in iter_comments(base_dir, subreddit):\n",
    "        tokens = comment.get(\"processed_text\")\n",
    "        if not tokens or year_to_period(comment_year(comment)) != period:\n",
    "            continue\n",
    "        bigram_tokens = bigram_model[tokens]\n",
    "        for idx, token in enumerate(bigram_tokens):\n",
    "            if token not in target_words:\n",
    "                continue\n",
    "            left = max(0, idx - CONTEXT_WINDOW)\n",
    "            right = min(len(bigram_tokens), idx + CONTEXT_WINDOW + 1)\n",
    "            window = [\n",
    "                bigram_tokens[j]\n",
    "                for j in range(left, right)\n",
    "                if j != idx and bigram_tokens[j] in vocab\n",
    "            ]\n",
    "            if not window:\n",
    "                continue\n",
    "            centroid = np.mean([embedding_model.wv[w] for w in window], axis=0)\n",
    "            context_vectors[token].append(centroid)\n",
    "    return context_vectors\n",
    "\n",
    "def summarize_context_vectors(context_vectors: dict[str, list[np.ndarray]]) -> dict[str, dict]:\n",
    "    stats = {}\n",
    "    for word, vectors in context_vectors.items():\n",
    "        if not vectors:\n",
    "            continue\n",
    "        arr = np.vstack(vectors)\n",
    "        stats[word] = {\n",
    "            \"centroid\": arr.mean(axis=0),\n",
    "            \"variance\": float(arr.var(axis=0).mean()),\n",
    "            \"count\": len(vectors),\n",
    "        }\n",
    "    return stats\n",
    "\n",
    "def polarization_measure(stats_left: dict, stats_right: dict) -> dict[str, dict]:\n",
    "    results = {}\n",
    "    for word in set(stats_left).intersection(stats_right):\n",
    "        c1, c2 = stats_left[word][\"centroid\"], stats_right[word][\"centroid\"]\n",
    "        v1, v2 = stats_left[word][\"variance\"], stats_right[word][\"variance\"]\n",
    "        cos_dist = cosine(c1, c2) if np.any(c1) and np.any(c2) else 0.0\n",
    "        kl_div = np.log((v2 + EPS) / (v1 + EPS)) + (v1 + np.sum((c1 - c2) ** 2)) / (v2 + EPS) - 1\n",
    "        results[word] = {\n",
    "            \"word\": word,\n",
    "            \"cosine\": float(cos_dist),\n",
    "            \"kl\": float(kl_div),\n",
    "            \"count_left\": stats_left[word][\"count\"],\n",
    "            \"count_right\": stats_right[word][\"count\"],\n",
    "        }\n",
    "    return results\n",
    "\n",
    "def save_period_results(period: str, polarization: dict[str, dict]) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(polarization.values()).sort_values(\"cosine\", ascending=False)\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUTPUT_DIR / f\"polarization_{period}.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Pipeline\n",
    "# ------------------------------------------------------------------\n",
    "bigram_model = load_bigram_model(BIGRAM_MODEL_PATH)\n",
    "neutral_model = load_neutral_model(NEUTRAL_MODEL_PATH)\n",
    "targets_per_period = {\n",
    "    period: select_target_words(FREQ_FILE_DEM, FREQ_FILE_REP, period)\n",
    "    for period in PERIOD_DEFINITIONS\n",
    "}\n",
    "\n",
    "all_stats = {subreddit: {} for subreddit in SUBREDDITS}\n",
    "for subreddit in SUBREDDITS:\n",
    "    for period, targets in targets_per_period.items():\n",
    "        if not targets:\n",
    "            print(f\"No shared targets for {period}; skipping {subreddit}.\")\n",
    "            all_stats[subreddit][period] = {}\n",
    "            continue\n",
    "        print(f\"[{subreddit}] {period}: {len(targets)} targets\")\n",
    "        vectors = extract_context_vectors(\n",
    "            base_dir=BASE_DATA_DIR,\n",
    "            subreddit=subreddit,\n",
    "            bigram_model=bigram_model,\n",
    "            target_words=targets,\n",
    "            period=period,\n",
    "            embedding_model=neutral_model,\n",
    "        )\n",
    "        all_stats[subreddit][period] = summarize_context_vectors(vectors)\n",
    "\n",
    "for period in PERIOD_DEFINITIONS:\n",
    "    stats_dem = all_stats[\"democrats\"].get(period, {})\n",
    "    stats_rep = all_stats[\"republican\"].get(period, {})\n",
    "    polarization = polarization_measure(stats_dem, stats_rep)\n",
    "    if not polarization:\n",
    "        print(f\"No overlapping words for {period}; CSV skipped.\")\n",
    "        continue\n",
    "    save_period_results(period, polarization)\n",
    "    print(f\"Saved {period} results to {OUTPUT_DIR / f'polarization_{period}.csv'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

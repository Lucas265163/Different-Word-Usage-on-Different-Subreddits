{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19bb7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling conservative...\n",
      "Found existing bigdocs\\conservative.txt, skipping aggregation.\n",
      "Handling liberal...\n",
      "Found existing bigdocs\\liberal.txt, skipping aggregation.\n",
      "Handling republican...\n",
      "Found existing bigdocs\\republican.txt, skipping aggregation.\n",
      "Handling democrats...\n",
      "Found existing bigdocs\\democrats.txt, skipping aggregation.\n",
      "Handling books...\n",
      "Found existing bigdocs\\books.txt, skipping aggregation.\n",
      "Handling cooking...\n",
      "Found existing bigdocs\\cooking.txt, skipping aggregation.\n",
      "Handling gaming...\n",
      "Found existing bigdocs\\gaming.txt, skipping aggregation.\n",
      "Handling movies...\n",
      "Found existing bigdocs\\movies.txt, skipping aggregation.\n",
      "Handling personalfinance...\n",
      "Found existing bigdocs\\personalfinance.txt, skipping aggregation.\n",
      "Handling travel...\n",
      "Found existing bigdocs\\travel.txt, skipping aggregation.\n",
      "Handling technology...\n",
      "Found existing bigdocs\\technology.txt, skipping aggregation.\n",
      "bigdocs\\conservative.txt has 213976957 tokens.\n",
      "bigdocs\\liberal.txt has 7990796 tokens.\n",
      "bigdocs\\republican.txt has 18907583 tokens.\n",
      "bigdocs\\democrats.txt has 22672234 tokens.\n",
      "bigdocs\\books.txt has 223171883 tokens.\n",
      "bigdocs\\cooking.txt has 160287837 tokens.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from gensim.models.phrases import Phraser\n",
    "# from collections import Counter\n",
    "\n",
    "political = [\"conservative\", \"liberal\", \"republican\", \"democrats\"]\n",
    "non_political = [\"books\", \"cooking\", \"gaming\", \"movies\", \"personalfinance\", \"travel\", \"technology\"]\n",
    "\n",
    "# Load global bigram model\n",
    "bigram_model_path = \"../../models/bigram/all_bigram_1.phr\"\n",
    "bigram = Phraser.load(bigram_model_path)\n",
    "\n",
    "def save_or_load_subreddit_doc(subreddit, base_dir=\"../../processed_comments/processed_comments_1\", out_dir=\"bigdocs\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = os.path.join(out_dir, f\"{subreddit}.txt\")\n",
    "    if os.path.exists(out_path):\n",
    "        print(f\"Found existing {out_path}, skipping aggregation.\")\n",
    "        return out_path\n",
    "    pattern = os.path.join(base_dir, subreddit, f\"{subreddit}_batch*.pkl\")\n",
    "    files = sorted(glob(pattern))\n",
    "    with open(out_path, \"w\", encoding=\"utf8\") as fout:\n",
    "        for file in files:\n",
    "            with open(file, \"rb\") as f:\n",
    "                batch = pickle.load(f)\n",
    "                for comment in batch:\n",
    "                    tokens = comment.get(\"processed_text\", [])\n",
    "                    bigram_tokens = bigram[tokens]\n",
    "                    fout.write(\" \".join(bigram_tokens) + \" \")\n",
    "    return out_path\n",
    "\n",
    "subreddits = political + non_political\n",
    "doc_files = []\n",
    "for sub in subreddits:\n",
    "    print(f\"Handling {sub}...\")\n",
    "    doc_files.append(save_or_load_subreddit_doc(sub))\n",
    "\n",
    "# total_counter = Counter()\n",
    "# for doc_file in doc_files:\n",
    "#     with open(doc_file, encoding=\"utf8\") as f:\n",
    "#         tokens = f.read().split()\n",
    "#         print(f\"{doc_file} has {len(tokens)} tokens.\")\n",
    "#         total_counter.update(tokens)\n",
    "\n",
    "# min_total_freq = 100 \n",
    "# vocab = [word for word, freq in total_counter.items() if freq >= min_total_freq]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=20000, min_df=5, input='filename')\n",
    "tfidf_matrix = vectorizer.fit_transform(doc_files)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Calculate mean TF-IDF for political and non-political subreddits\n",
    "political_idx = [subreddits.index(s) for s in political]\n",
    "non_political_idx = [subreddits.index(s) for s in non_political]\n",
    "\n",
    "mean_tfidf_political = tfidf_matrix[political_idx].mean(axis=0).A1\n",
    "mean_tfidf_nonpolitical = tfidf_matrix[non_political_idx].mean(axis=0).A1\n",
    "\n",
    "diff = mean_tfidf_political - mean_tfidf_nonpolitical\n",
    "df = pd.DataFrame({\"word\": words, \"tfidf_diff\": diff})\n",
    "df = df.sort_values(\"tfidf_diff\", ascending=False)\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "df.to_csv(\"output/political_words_tfidf_diff.csv\", index=False)\n",
    "\n",
    "print(df.head(50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

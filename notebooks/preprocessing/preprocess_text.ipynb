{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed98bc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cyyua\\DS\\word_embedding\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import html\n",
    "import unicodedata\n",
    "import pickle\n",
    "\n",
    "TARGET_SIZE = 1_000_000\n",
    "OUTPUT_DIR = \"../../processed_comments/wikipedia_processed\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "source_ds = load_from_disk(\"../../datasets/wikipedia-20231101-en\")\n",
    "sample_size = min(TARGET_SIZE, len(source_ds))\n",
    "sampled_ds = source_ds.shuffle(seed=23).select(range(sample_size))\n",
    "\n",
    "# Initialize global resources once\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "# POS tag cache to avoid redundant tagging\n",
    "POS_CACHE = {}\n",
    "# Lemma cache to avoid redundant lemmatization\n",
    "LEMMA_CACHE = {}\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Convert NLTK POS tag to WordNet POS tag\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # verb\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'  # noun\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # adverb\n",
    "    else:\n",
    "        return 'n'  # default as noun\n",
    "\n",
    "def preprocess_words(text, lemmatize=True, without_stopwords=False):\n",
    "    \"\"\"Preprocess Reddit text content with optimized NLTK operations\"\"\"\n",
    "    # Handle HTML entities\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Unicode normalization\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Remove URLs and Markdown formatting\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)\n",
    "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)\n",
    "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)\n",
    "    \n",
    "    # Basic text cleaning\n",
    "    text = re.sub(\"[^A-Za-z]+\", ' ', text).lower()\n",
    "    \n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "    \n",
    "    # Lemmatization first\n",
    "    if lemmatize:\n",
    "        # POS tagging (with cache)\n",
    "        uncached_words = [w for w in words if w not in POS_CACHE]\n",
    "        if uncached_words:\n",
    "            tagged_uncached = nltk.pos_tag(uncached_words)\n",
    "            for word, tag in tagged_uncached:\n",
    "                POS_CACHE[word] = tag\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            tag = POS_CACHE[word]\n",
    "            wordnet_pos = get_wordnet_pos(tag)\n",
    "            lemma_key = (word, wordnet_pos)\n",
    "            if lemma_key in LEMMA_CACHE:\n",
    "                lemma = LEMMA_CACHE[lemma_key]\n",
    "            else:\n",
    "                lemma = LEMMATIZER.lemmatize(word, pos=wordnet_pos)\n",
    "                LEMMA_CACHE[lemma_key] = lemma\n",
    "            processed_words.append(lemma)\n",
    "    else:\n",
    "        processed_words = words\n",
    "\n",
    "    # Remove stopwords after lemmatization\n",
    "    if without_stopwords:\n",
    "        processed_words = [w for w in processed_words if w not in STOP_WORDS]\n",
    "\n",
    "    return processed_words\n",
    "\n",
    "corpus = [preprocess_words(text) for text in sampled_ds[\"text\"]]\n",
    "with open(f'{OUTPUT_DIR}/processed_corpus_stopwords.pkl', \"wb\") as f:\n",
    "    pickle.dump(corpus, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

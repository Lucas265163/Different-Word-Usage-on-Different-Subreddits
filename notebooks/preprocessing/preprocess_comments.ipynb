{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a8624a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing subreddit: democrats\n",
      "Processing file: datasets/democrats_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing democrats comments: 1035748it [00:33, 36947.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 1...\n",
      "Saved 1000000 comments to processed_comments_2/democrats/democrats_batch1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing democrats comments: 2011525it [01:02, 32007.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving remaining 941514 comments...\n",
      "Saved 941514 comments to processed_comments_2/democrats/democrats_batch2.pkl\n",
      "\n",
      "Completed processing democrats comments!\n",
      "Total comments saved: 1941514\n",
      "\n",
      "Processing subreddit: republican\n",
      "Processing file: datasets/Republican_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing republican comments: 1071593it [00:33, 37639.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing republican comments: 1082696it [00:36, 8237.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/republican/republican_batch1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing republican comments: 1405486it [00:44, 31463.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving remaining 296609 comments...\n",
      "Saved 296609 comments to processed_comments_2/republican/republican_batch2.pkl\n",
      "\n",
      "Completed processing republican comments!\n",
      "Total comments saved: 1296609\n",
      "\n",
      "Processing subreddit: conservative\n",
      "Processing file: datasets/Conservative_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 1006762it [00:35, 36803.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 1014581it [00:38, 5820.53it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 2017516it [01:12, 33795.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 2...\n",
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch2.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 3027286it [01:46, 38016.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 3031119it [01:48, 4961.89it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch3.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 4035096it [02:19, 31260.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 4040739it [02:22, 5201.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch4.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 5041290it [02:56, 35423.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 5048101it [02:58, 5489.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch5.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 6056257it [03:30, 38948.72it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 6...\n",
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch6.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 7062498it [04:02, 35291.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 7...\n",
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch7.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 8074131it [04:32, 36454.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 8081473it [04:34, 6967.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch8.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 9083618it [05:03, 40876.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 9091379it [05:05, 7210.58it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch9.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 10094490it [05:36, 33497.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 10100945it [05:38, 5879.85it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch10.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 11103686it [06:09, 37609.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 11111381it [06:11, 7087.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch11.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 12109823it [06:41, 37346.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 12...\n",
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch12.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 13115643it [07:14, 38794.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 13...\n",
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch13.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 14120897it [07:44, 51417.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 14...\n",
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch14.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 15125693it [08:14, 45443.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 15134435it [08:16, 8667.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch15.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 16132434it [08:44, 37425.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 16139613it [08:46, 7006.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch16.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 17140972it [09:13, 44113.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 17...\n",
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch17.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 18144421it [09:42, 48903.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 18...\n",
      "Saved 1000000 comments to processed_comments_2/conservative/conservative_batch18.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 18984143it [10:02, 31485.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving remaining 832387 comments...\n",
      "Saved 832387 comments to processed_comments_2/conservative/conservative_batch19.pkl\n",
      "\n",
      "Completed processing conservative comments!\n",
      "Total comments saved: 18832387\n",
      "\n",
      "Processing subreddit: liberal\n",
      "Processing file: datasets/Liberal_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing liberal comments: 497079it [00:18, 27418.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving remaining 492131 comments...\n",
      "Saved 492131 comments to processed_comments_2/liberal/liberal_batch1.pkl\n",
      "\n",
      "Completed processing liberal comments!\n",
      "Total comments saved: 492131\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "import html\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "# Add scripts directory to Python path\n",
    "scripts_dir = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"..\", \"..\", \"scripts\"))\n",
    "if scripts_dir not in sys.path:\n",
    "    sys.path.insert(0, scripts_dir)\n",
    "\n",
    "from file_streams import getFileJsonStream\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "# Initialize global resources once\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "# POS tag cache to avoid redundant tagging\n",
    "POS_CACHE = {}\n",
    "# Lemma cache to avoid redundant lemmatization\n",
    "LEMMA_CACHE = {}\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Convert NLTK POS tag to WordNet POS tag\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # verb\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'  # noun\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # adverb\n",
    "    else:\n",
    "        return 'n'  # default as noun\n",
    "\n",
    "def preprocess_text(text, lemmatize=True, without_stopwords=True):\n",
    "    \"\"\"Preprocess Reddit text content with optimized NLTK operations\"\"\"\n",
    "    # Handle HTML entities\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Unicode normalization\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Remove URLs and Markdown formatting\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)\n",
    "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)\n",
    "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)\n",
    "    \n",
    "    # Remove subreddit and user references\n",
    "    text = re.sub(r'/r/\\w+', '', text)\n",
    "    text = re.sub(r'r/\\w+', '', text)\n",
    "    text = re.sub(r'/u/\\w+', '', text)\n",
    "    text = re.sub(r'u/\\w+', '', text)\n",
    "    \n",
    "    # Basic text cleaning\n",
    "    text = re.sub(\"[^A-Za-z]+\", ' ', text).lower()\n",
    "    \n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "    \n",
    "    # Lemmatization first\n",
    "    if lemmatize:\n",
    "        # POS tagging (with cache)\n",
    "        uncached_words = [w for w in words if w not in POS_CACHE]\n",
    "        if uncached_words:\n",
    "            tagged_uncached = nltk.pos_tag(uncached_words)\n",
    "            for word, tag in tagged_uncached:\n",
    "                POS_CACHE[word] = tag\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            tag = POS_CACHE[word]\n",
    "            wordnet_pos = get_wordnet_pos(tag)\n",
    "            lemma_key = (word, wordnet_pos)\n",
    "            if lemma_key in LEMMA_CACHE:\n",
    "                lemma = LEMMA_CACHE[lemma_key]\n",
    "            else:\n",
    "                lemma = LEMMATIZER.lemmatize(word, pos=wordnet_pos)\n",
    "                LEMMA_CACHE[lemma_key] = lemma\n",
    "            processed_words.append(lemma)\n",
    "    else:\n",
    "        processed_words = words\n",
    "\n",
    "    # Remove stopwords after lemmatization\n",
    "    if without_stopwords:\n",
    "        processed_words = [w for w in processed_words if w not in STOP_WORDS]\n",
    "\n",
    "    # Remove all words with length <= 2 and > 15\n",
    "    processed_words = [w for w in processed_words if len(w) > 2 and len(w) <= 15]\n",
    "\n",
    "    return processed_words\n",
    "\n",
    "\n",
    "def process_and_save_comments(path, subreddit, output_dir, without_stopwords=True, batch_size=1000000):\n",
    "    \"\"\"Process comments and save in batches\"\"\"\n",
    "    print(f\"Processing file: {path}\")\n",
    "    \n",
    "    # Batch processing counters\n",
    "    batch_count = 0\n",
    "    batch_number = 1\n",
    "    total_count = 0\n",
    "    \n",
    "    # Create data structure for comments\n",
    "    comments_batch = []\n",
    "\n",
    "    with open(path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Unable to read file {path}\")\n",
    "            return\n",
    "        \n",
    "        for row in tqdm(jsonStream, desc=f\"Processing {subreddit} comments\"):\n",
    "            if \"body\" not in row or \"created_utc\" not in row or \"author\" not in row or \"id\" not in row:\n",
    "                continue\n",
    "                \n",
    "            author = row[\"author\"]\n",
    "            if author in {\"AutoModerator\", \"election_info_bot\"}:\n",
    "                continue\n",
    "            \n",
    "            comment_id = row[\"id\"]\n",
    "            text = row[\"body\"]\n",
    "            created_timestamp = row[\"created_utc\"]\n",
    "            date = datetime.datetime.fromtimestamp(int(created_timestamp))\n",
    "            \n",
    "            # Process text with optimized functions\n",
    "            processed_words = preprocess_text(text, lemmatize=True, without_stopwords=without_stopwords)\n",
    "            \n",
    "            if processed_words:\n",
    "                # Save processed comment with metadata\n",
    "                comment_data = {\n",
    "                    \"comment_id\": comment_id,\n",
    "                    \"author\": author,\n",
    "                    \"date\": date.strftime(\"%Y-%m-%d\"),\n",
    "                    \"timestamp\": created_timestamp,\n",
    "                    \"processed_text\": processed_words,  # Original order preserved\n",
    "                    \"original\": text\n",
    "                }\n",
    "                \n",
    "                comments_batch.append(comment_data)\n",
    "                batch_count += 1\n",
    "                \n",
    "            # Check if we need to save the current batch\n",
    "            if batch_count >= batch_size:\n",
    "                print(f\"\\nReached {batch_size} comments, saving batch {batch_number}...\")\n",
    "                \n",
    "                # Save batch directly without filtering\n",
    "                save_path = f\"{output_dir}/{subreddit}_batch{batch_number}.pkl\"\n",
    "                with open(save_path, \"wb\") as out_file:\n",
    "                    pickle.dump(comments_batch, out_file)\n",
    "                \n",
    "                print(f\"Saved {len(comments_batch)} comments to {save_path}\")\n",
    "                \n",
    "                # Reset batch data\n",
    "                comments_batch = []\n",
    "                batch_count = 0\n",
    "                batch_number += 1\n",
    "                total_count += batch_size\n",
    "    \n",
    "    # Process any remaining comments\n",
    "    if batch_count > 0:\n",
    "        print(f\"\\nSaving remaining {batch_count} comments...\")\n",
    "        \n",
    "        # Save batch\n",
    "        save_path = f\"{output_dir}/{subreddit}_batch{batch_number}.pkl\"\n",
    "        with open(save_path, \"wb\") as out_file:\n",
    "            pickle.dump(comments_batch, out_file)\n",
    "        \n",
    "        print(f\"Saved {len(comments_batch)} comments to {save_path}\")\n",
    "        total_count += batch_count\n",
    "    \n",
    "    print(f\"\\nCompleted processing {subreddit} comments!\")\n",
    "    print(f\"Total comments saved: {total_count}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    random.seed(23)\n",
    "    np.random.seed(23)\n",
    "    \n",
    "    # Define data file paths\n",
    "    files = {\n",
    "        \"democrats\": r\"datasets/democrats_comments.zst\",\n",
    "        \"republican\": r\"datasets/Republican_comments.zst\",\n",
    "        \"conservative\": r\"datasets/Conservative_comments.zst\",\n",
    "        \"liberal\": r\"datasets/Liberal_comments.zst\"\n",
    "    }\n",
    "    \n",
    "    # List of subreddits to process (process all by default)\n",
    "    subreddits_to_process = list(files.keys())\n",
    "\n",
    "    for subreddit in subreddits_to_process:\n",
    "        output_dir = f\"processed_comments_2/{subreddit}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"\\nProcessing subreddit: {subreddit}\")\n",
    "        process_and_save_comments(\n",
    "            files[subreddit],\n",
    "            subreddit,\n",
    "            output_dir,\n",
    "            without_stopwords=False,\n",
    "            batch_size=1000000\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86c821f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For streaming\n",
    "import sys\n",
    "version = sys.version_info\n",
    "if version.major < 3 or (version.major == 3 and version.minor < 10):\n",
    "    raise RuntimeError(\"This script requires Python 3.10 or higher\")\n",
    "import os\n",
    "from typing import Iterable\n",
    "\n",
    "from fileStreams import getFileJsonStream\n",
    "from utils import FileProgressLog\n",
    "\n",
    "# For processing\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import html\n",
    "import unicodedata\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "    \n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "filePathforDemocrats = r\"datasets/democrats_comments.zst\"\n",
    "filePathforRepublican = r\"datasets/Republican_comments.zst\"\n",
    "recursive = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9f9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file datasets/democrats_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments: 2011525it [07:20, 4564.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comment Counts by Period ===\n",
      "before_2016: 0 comments\n",
      "2017_2020: 0 comments\n",
      "2021_2024: 1341626 comments\n",
      "before_2016: 0 comments after filtering words by user count\n",
      "2017_2020: 0 comments after filtering words by user count\n",
      "2021_2024: 1339874 comments after filtering words by user count\n",
      "\n",
      "Extracting bigrams for 2021_2024...\n",
      "\n",
      "=== Training Word2Vec for 2021_2024 (1339874 comments) ===\n",
      "Vocabulary size: 26047\n",
      "Model saved to models/reddit_word2vec_10_10_filtered_processed_democrats_2021_2024.model\n",
      "Done :>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def build_word2vect_model(path, party, without_stopwords=True, phrases_min_count=5, word2vec_min_count=5):\n",
    "    print(f\"Processing file {path}\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # For each period, track comments and user-word usage\n",
    "    chunks = {\n",
    "        \"before_2016\": [],\n",
    "        \"2017_2020\": [],\n",
    "        \"2021_2024\": [],\n",
    "    }\n",
    "    user_words = {\n",
    "        \"before_2016\": defaultdict(set),\n",
    "        \"2017_2020\": defaultdict(set),\n",
    "        \"2021_2024\": defaultdict(set),\n",
    "    }\n",
    "    user_comments = {\n",
    "        \"before_2016\": defaultdict(list),\n",
    "        \"2017_2020\": defaultdict(list),\n",
    "        \"2021_2024\": defaultdict(list),\n",
    "    }\n",
    "    counts = {period: 0 for period in chunks.keys()}\n",
    "\n",
    "    with open(path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Skipping unknown file {path}\")\n",
    "            return\n",
    "        for row in tqdm(jsonStream, desc=\"Processing comments\"):\n",
    "            if \"body\" not in row or \"created_utc\" not in row or \"author\" not in row:\n",
    "                continue\n",
    "            author = row[\"author\"]\n",
    "            if author in {\"AutoModerator\", \"election_info_bot\"}:\n",
    "                continue\n",
    "            text = row[\"body\"]\n",
    "            created_timestamp = row[\"created_utc\"]\n",
    "            year = datetime.datetime.fromtimestamp(int(created_timestamp)).year\n",
    "            # if year <= 2016:\n",
    "            #     chunk_key = \"before_2016\"\n",
    "            # elif 2017 <= year <= 2020:\n",
    "            #     chunk_key = \"2017_2020\"\n",
    "            # elif 2021 <= year <= 2024:\n",
    "            #     chunk_key = \"2021_2024\"\n",
    "            # else:\n",
    "            #     continue\n",
    "            if 2021 <= year <= 2024:\n",
    "                chunk_key = \"2021_2024\"\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Handle HTML entities\n",
    "            text = html.unescape(text)                         # &amp; becomes &, etc.\n",
    "            \n",
    "            # Handle Unicode normalization\n",
    "            text = unicodedata.normalize('NFKD', text)         # converts café with a single combined \"é\" character to \"e\" with an accent\n",
    "            \n",
    "            # Remove all URLs\n",
    "            text = re.sub(r'http\\S+', '', text)                # Remove URLs\n",
    "            text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)        # Images/GIFs\n",
    "            \n",
    "            # Handle Reddit's link format\n",
    "            text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)    # [text](link) becomes text\n",
    "            \n",
    "            # Handle markdown formatting (bold, italics)\n",
    "            text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)       # **word** becomes word\n",
    "            text = re.sub(r'\\*(.*?)\\*', r'\\1', text)           # *word* becomes word\n",
    "            \n",
    "            # Handle subreddit and user references\n",
    "            text = re.sub(r'/r/(\\w+)', r'subreddit_\\1', text)  # /r/politics becomes subreddit_politics\n",
    "            text = re.sub(r'/u/(\\w+)', r'user_\\1', text)       # /u/username becomes user_username\n",
    "            \n",
    "            text = re.sub(\"[^A-Za-z']+\", ' ', text).lower()\n",
    "\n",
    "            words = text.split()\n",
    "            if not words:\n",
    "                continue\n",
    "            tagged_words = nltk.pos_tag(words)\n",
    "            processed_words = []\n",
    "            for word, tag in tagged_words:\n",
    "                if without_stopwords and word in stop_words:\n",
    "                    continue\n",
    "                if tag.startswith('J'):\n",
    "                    wordnet_pos = 'a'\n",
    "                elif tag.startswith('V'):\n",
    "                    wordnet_pos = 'v'\n",
    "                elif tag.startswith('N'):\n",
    "                    wordnet_pos = 'n'\n",
    "                elif tag.startswith('R'):\n",
    "                    wordnet_pos = 'r'\n",
    "                else:\n",
    "                    wordnet_pos = 'n'\n",
    "                lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "                processed_words.append(lemma)\n",
    "                user_words[chunk_key][lemma].add(author)\n",
    "            if processed_words:\n",
    "                user_comments[chunk_key][author].append(processed_words)\n",
    "                counts[chunk_key] += 1\n",
    "\n",
    "    print(\"\\n=== Comment Counts by Period ===\")\n",
    "    for period, count in counts.items():\n",
    "        print(f\"{period}: {count} comments\")\n",
    "\n",
    "    # Filter words by user count and rebuild comments for each period\n",
    "    for period in chunks.keys():\n",
    "        valid_words = {w for w, users in user_words[period].items() if len(users) >= 5}\n",
    "        filtered_comments = []\n",
    "        for comments in user_comments[period].values():\n",
    "            for comment in comments:\n",
    "                filtered = [w for w in comment if w in valid_words]\n",
    "                if filtered:\n",
    "                    filtered_comments.append(filtered)\n",
    "        print(f\"{period}: {len(filtered_comments)} comments after filtering words by user count\")\n",
    "        if filtered_comments:\n",
    "            print(f\"\\nExtracting bigrams for {period}...\")\n",
    "            phrases = Phrases(filtered_comments, \n",
    "                              min_count=phrases_min_count, \n",
    "                              threshold=0.7,\n",
    "                              scoring='npmi')\n",
    "            bigram_model = Phraser(phrases)\n",
    "            bigrammed_comments = [bigram_model[comment] for comment in filtered_comments]\n",
    "            chunks[period] = bigrammed_comments\n",
    "        else:\n",
    "            chunks[period] = []\n",
    "\n",
    "    # Train a Word2Vec model for each time period\n",
    "    for period, comments in chunks.items():\n",
    "        if len(comments) > 0:\n",
    "            print(f\"\\n=== Training Word2Vec for {period} ({len(comments)} comments) ===\")\n",
    "            model = Word2Vec(\n",
    "                vector_size=300,\n",
    "                window=5,\n",
    "                min_count=word2vec_min_count,\n",
    "                workers=16\n",
    "            )\n",
    "            model.build_vocab(comments)\n",
    "            print(f\"Vocabulary size: {len(model.wv.index_to_key)}\")\n",
    "            model.train(\n",
    "                comments,\n",
    "                total_examples=len(comments),\n",
    "                epochs=5\n",
    "            )\n",
    "            model_path = f\"models/model_v3/reddit_word2vec_{phrases_min_count}_{word2vec_min_count}_{party}_{period}.model\"\n",
    "            model.save(model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def main():\n",
    "    # build_word2vect_model(filePathforDemocrats, \"democrats\", without_stopwords=False, \n",
    "    #                       phrases_min_count=5, word2vec_min_count=5)\n",
    "    # build_word2vect_model(filePathforRepublican, \"republican\", without_stopwords=False, \n",
    "    #                       phrases_min_count=5, word2vec_min_count=5)\n",
    "    build_word2vect_model(filePathforDemocrats, \"democrats\", without_stopwords=False, \n",
    "                          phrases_min_count=10, word2vec_min_count=10)\n",
    "    build_word2vect_model(filePathforRepublican, \"republican\", without_stopwords=False, \n",
    "                          phrases_min_count=10, word2vec_min_count=10)\n",
    "    # build_word2vect_model(filePathforDemocrats, \"democrats\", without_stopwords=False, \n",
    "    #                       phrases_min_count=20, word2vec_min_count=20)\n",
    "    # build_word2vect_model(filePathforRepublican, \"republican\", without_stopwords=False, \n",
    "    #                       phrases_min_count=20, word2vec_min_count=20)\n",
    "    # build_word2vect_model(filePathforDemocrats, \"democrats\", without_stopwords=False, \n",
    "    #                       phrases_min_count=50, word2vec_min_count=50)\n",
    "    # build_word2vect_model(filePathforRepublican, \"republican\", without_stopwords=False, \n",
    "    #                       phrases_min_count=50, word2vec_min_count=50)\n",
    "    print(\"Done :>\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

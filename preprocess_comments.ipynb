{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cddb4c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: datasets/Conservative_comments.zst\n",
      "Starting to process conservative comments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 1008333it [00:36, 35491.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 1018898it [00:39, 7922.46it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments/conservative/conservative_batch1.pkl\n",
      "POS tag cache size: 125996 words\n",
      "Lemma cache size: 125996 word-POS pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 2024567it [01:11, 36607.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 2...\n",
      "Saved 1000000 comments to processed_comments/conservative/conservative_batch2.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 2031742it [01:14, 6605.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tag cache size: 173667 words\n",
      "Lemma cache size: 173667 word-POS pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 3034414it [01:44, 39619.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 3...\n",
      "Saved 1000000 comments to processed_comments/conservative/conservative_batch3.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 3042178it [01:46, 7506.74it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tag cache size: 213252 words\n",
      "Lemma cache size: 213252 word-POS pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 4044368it [02:16, 31628.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 4...\n",
      "Saved 1000000 comments to processed_comments/conservative/conservative_batch4.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 4054791it [02:18, 8846.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tag cache size: 247173 words\n",
      "Lemma cache size: 247173 word-POS pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 5052754it [02:49, 36698.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 5...\n",
      "Saved 1000000 comments to processed_comments/conservative/conservative_batch5.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 5063536it [02:51, 9049.56it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tag cache size: 277406 words\n",
      "Lemma cache size: 277406 word-POS pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 6067061it [03:20, 41557.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 6075606it [03:22, 8220.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments/conservative/conservative_batch6.pkl\n",
      "POS tag cache size: 303053 words\n",
      "Lemma cache size: 303053 word-POS pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 7080552it [03:49, 59488.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 7...\n",
      "Saved 1000000 comments to processed_comments/conservative/conservative_batch7.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 7099140it [03:51, 16321.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tag cache size: 324278 words\n",
      "Lemma cache size: 324278 word-POS pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 8095393it [04:17, 38532.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 8...\n",
      "Saved 1000000 comments to processed_comments/conservative/conservative_batch8.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 8102885it [04:19, 7470.08it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tag cache size: 340768 words\n",
      "Lemma cache size: 340768 word-POS pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 9103313it [04:48, 37776.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 9110552it [04:50, 7198.98it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments/conservative/conservative_batch9.pkl\n",
      "POS tag cache size: 357637 words\n",
      "Lemma cache size: 357637 word-POS pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 10118194it [05:19, 35580.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 10...\n",
      "Saved 1000000 comments to processed_comments/conservative/conservative_batch10.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 10125299it [05:21, 6699.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tag cache size: 376309 words\n",
      "Lemma cache size: 376309 word-POS pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 11127583it [05:51, 37623.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 11135448it [05:53, 7675.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments/conservative/conservative_batch11.pkl\n",
      "POS tag cache size: 394619 words\n",
      "Lemma cache size: 394619 word-POS pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 12136369it [06:24, 32666.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 12...\n",
      "Saved 1000000 comments to processed_comments/conservative/conservative_batch12.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 12143399it [06:26, 6998.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tag cache size: 412637 words\n",
      "Lemma cache size: 412637 word-POS pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 13147081it [06:56, 38547.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 13...\n",
      "Saved 1000000 comments to processed_comments/conservative/conservative_batch13.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 13158340it [06:58, 9718.60it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tag cache size: 431323 words\n",
      "Lemma cache size: 431323 word-POS pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 14156847it [07:26, 45618.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 14165336it [07:28, 9135.06it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments/conservative/conservative_batch14.pkl\n",
      "POS tag cache size: 445631 words\n",
      "Lemma cache size: 445631 word-POS pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 15166380it [07:55, 37957.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 15...\n",
      "Saved 1000000 comments to processed_comments/conservative/conservative_batch15.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 15173395it [07:57, 7838.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tag cache size: 458541 words\n",
      "Lemma cache size: 458541 word-POS pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 16172298it [08:26, 36419.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 16179933it [08:28, 7693.87it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments/conservative/conservative_batch16.pkl\n",
      "POS tag cache size: 472757 words\n",
      "Lemma cache size: 472757 word-POS pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 17182992it [08:55, 44591.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 17...\n",
      "Saved 1000000 comments to processed_comments/conservative/conservative_batch17.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 17195445it [08:57, 11421.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tag cache size: 486615 words\n",
      "Lemma cache size: 486615 word-POS pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 18186702it [09:21, 47274.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reached 1000000 comments, saving batch 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 18195560it [09:23, 9904.60it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000000 comments to processed_comments/conservative/conservative_batch18.pkl\n",
      "POS tag cache size: 497034 words\n",
      "Lemma cache size: 497034 word-POS pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conservative comments: 18984143it [09:40, 32681.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving remaining 788076 comments...\n",
      "Saved 788076 comments to processed_comments/conservative/conservative_batch19.pkl\n",
      "\n",
      "Completed processing conservative comments!\n",
      "Total comments saved: 18788076\n",
      "Final POS tag cache size: 502625 words\n",
      "Final lemma cache size: 502625 word-POS pairs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "import html\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from fileStreams import getFileJsonStream\n",
    "from collections import defaultdict\n",
    "\n",
    "# Pre-download required NLTK resources once at the module level\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# Initialize global resources once\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "# POS tag cache to avoid redundant tagging\n",
    "POS_CACHE = {}\n",
    "# Lemma cache to avoid redundant lemmatization\n",
    "LEMMA_CACHE = {}\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Convert NLTK POS tag to WordNet POS tag\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # verb\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'  # noun\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # adverb\n",
    "    else:\n",
    "        return 'n'  # default as noun\n",
    "\n",
    "def preprocess_text(text, lemmatize=True, without_stopwords=True):\n",
    "    \"\"\"Preprocess Reddit text content with optimized NLTK operations\"\"\"\n",
    "    # Handle HTML entities\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Unicode normalization\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Remove URLs and Markdown formatting\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)\n",
    "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)\n",
    "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)\n",
    "    \n",
    "    # Remove subreddit and user references\n",
    "    text = re.sub(r'/r/\\w+', '', text)\n",
    "    text = re.sub(r'r/\\w+', '', text)\n",
    "    text = re.sub(r'/u/\\w+', '', text)\n",
    "    text = re.sub(r'u/\\w+', '', text)\n",
    "    \n",
    "    # Basic text cleaning\n",
    "    text = re.sub(\"[^A-Za-z]+\", ' ', text).lower()\n",
    "    \n",
    "    # Remove single letters (except 'i')\n",
    "    text = re.sub(r'\\b([a-hj-z])\\b', '', text)\n",
    "    \n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "        \n",
    "    # Lemmatization\n",
    "    if lemmatize:\n",
    "        stop_words = STOP_WORDS if without_stopwords else set()\n",
    "        \n",
    "        # Process words in a single batch for better performance\n",
    "        words_to_tag = [word for word in words if not (without_stopwords and word in stop_words)]\n",
    "        \n",
    "        if not words_to_tag:\n",
    "            return []\n",
    "            \n",
    "        # First check our caches\n",
    "        uncached_words = [word for word in words_to_tag if word not in POS_CACHE]\n",
    "        \n",
    "        # Only perform POS tagging on words not in cache\n",
    "        if uncached_words:\n",
    "            tagged_uncached = nltk.pos_tag(uncached_words)\n",
    "            # Update cache with new tags\n",
    "            for word, tag in tagged_uncached:\n",
    "                POS_CACHE[word] = tag\n",
    "        \n",
    "        processed_words = []\n",
    "        \n",
    "        # Process each word with cached information\n",
    "        for word in words_to_tag:\n",
    "            tag = POS_CACHE[word]\n",
    "            wordnet_pos = get_wordnet_pos(tag)\n",
    "            \n",
    "            # Check lemma cache first\n",
    "            lemma_key = (word, wordnet_pos)\n",
    "            if lemma_key in LEMMA_CACHE:\n",
    "                lemma = LEMMA_CACHE[lemma_key]\n",
    "            else:\n",
    "                lemma = LEMMATIZER.lemmatize(word, pos=wordnet_pos)\n",
    "                LEMMA_CACHE[lemma_key] = lemma\n",
    "                \n",
    "            processed_words.append(lemma)\n",
    "                \n",
    "        return processed_words\n",
    "    \n",
    "    return words\n",
    "\n",
    "def process_and_save_comments(path, subreddit, without_stopwords=True, batch_size=1000000):\n",
    "    \"\"\"Process comments and save in batches\"\"\"\n",
    "    print(f\"Processing file: {path}\")\n",
    "    \n",
    "    # Batch processing counters\n",
    "    batch_count = 0\n",
    "    batch_number = 1\n",
    "    total_count = 0\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = f\"processed_comments/{subreddit}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create data structure for comments without author filtering\n",
    "    comments_batch = []\n",
    "    \n",
    "    print(f\"Starting to process {subreddit} comments...\")\n",
    "    with open(path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Unable to read file {path}\")\n",
    "            return\n",
    "        \n",
    "        for row in tqdm(jsonStream, desc=f\"Processing {subreddit} comments\"):\n",
    "            if \"body\" not in row or \"created_utc\" not in row or \"author\" not in row or \"id\" not in row:\n",
    "                continue\n",
    "                \n",
    "            author = row[\"author\"]\n",
    "            if author in {\"AutoModerator\", \"election_info_bot\"}:\n",
    "                continue\n",
    "            \n",
    "            comment_id = row[\"id\"]\n",
    "            text = row[\"body\"]\n",
    "            created_timestamp = row[\"created_utc\"]\n",
    "            date = datetime.datetime.fromtimestamp(int(created_timestamp))\n",
    "            \n",
    "            # Process text with optimized functions\n",
    "            processed_words = preprocess_text(text, lemmatize=True, without_stopwords=without_stopwords)\n",
    "            \n",
    "            if processed_words:\n",
    "                # Save processed comment with metadata\n",
    "                comment_data = {\n",
    "                    \"comment_id\": comment_id,\n",
    "                    \"author\": author,\n",
    "                    \"date\": date.strftime(\"%Y-%m-%d\"),\n",
    "                    \"timestamp\": created_timestamp,\n",
    "                    \"processed_text\": processed_words,  # Original order preserved\n",
    "                    \"original\": text\n",
    "                }\n",
    "                \n",
    "                comments_batch.append(comment_data)\n",
    "                batch_count += 1\n",
    "                \n",
    "            # Check if we need to save the current batch\n",
    "            if batch_count >= batch_size:\n",
    "                print(f\"\\nReached {batch_size} comments, saving batch {batch_number}...\")\n",
    "                \n",
    "                # Save batch directly without filtering\n",
    "                save_path = f\"{output_dir}/{subreddit}_batch{batch_number}.pkl\"\n",
    "                with open(save_path, \"wb\") as out_file:\n",
    "                    pickle.dump(comments_batch, out_file)\n",
    "                \n",
    "                print(f\"Saved {len(comments_batch)} comments to {save_path}\")\n",
    "                \n",
    "                # Reset batch data\n",
    "                comments_batch = []\n",
    "                batch_count = 0\n",
    "                batch_number += 1\n",
    "                total_count += batch_size\n",
    "                \n",
    "                # Report cache stats\n",
    "                print(f\"POS tag cache size: {len(POS_CACHE)} words\")\n",
    "                print(f\"Lemma cache size: {len(LEMMA_CACHE)} word-POS pairs\")\n",
    "    \n",
    "    # Process any remaining comments\n",
    "    if batch_count > 0:\n",
    "        print(f\"\\nSaving remaining {batch_count} comments...\")\n",
    "        \n",
    "        # Save batch\n",
    "        save_path = f\"{output_dir}/{subreddit}_batch{batch_number}.pkl\"\n",
    "        with open(save_path, \"wb\") as out_file:\n",
    "            pickle.dump(comments_batch, out_file)\n",
    "        \n",
    "        print(f\"Saved {len(comments_batch)} comments to {save_path}\")\n",
    "        total_count += batch_count\n",
    "    \n",
    "    print(f\"\\nCompleted processing {subreddit} comments!\")\n",
    "    print(f\"Total comments saved: {total_count}\")\n",
    "    print(f\"Final POS tag cache size: {len(POS_CACHE)} words\")\n",
    "    print(f\"Final lemma cache size: {len(LEMMA_CACHE)} word-POS pairs\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    # Define data file paths\n",
    "    files = {\n",
    "        \"democrats\": r\"datasets/democrats_comments.zst\",\n",
    "        \"republican\": r\"datasets/Republican_comments.zst\",\n",
    "        \"conservative\": r\"datasets/Conservative_comments.zst\",\n",
    "        \"liberal\": r\"datasets/Liberal_comments.zst\",\n",
    "        \"backpacking\": r\"datasets/backpacking_comments.zst\",\n",
    "        \"vagabond\": r\"datasets/vagabond_comments.zst\"\n",
    "    }\n",
    "    \n",
    "    # Choose subreddit to process\n",
    "    subreddit_to_process = \"conservative\"  # Change this to process other subreddits\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(f\"processed_comments/{subreddit_to_process}\", exist_ok=True)\n",
    "\n",
    "    if subreddit_to_process in files:\n",
    "        process_and_save_comments(  \n",
    "            files[subreddit_to_process],\n",
    "            subreddit_to_process,\n",
    "            without_stopwords=True,\n",
    "            batch_size=1000000\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Subreddit not found: {subreddit_to_process}\")\n",
    "        print(f\"Available subreddits: {', '.join(files.keys())}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6287775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'list'>\n",
      "Number of items: 1000000\n",
      "\n",
      "Showing first 5 examples:\n",
      "\n",
      "Example 1:\n",
      "  comment_id: c07p2u0\n",
      "  author: Garak\n",
      "  date: 2009-02-16\n",
      "  timestamp: 1234791099\n",
      "  processed_text: ['allow', 'legend', 'grow', 'ill', 'mythical', 'proportion', 'lie', 'fund', 'acorn', 'nowhere'] ... (total: 49 words)\n",
      "  original: &gt;  And they have allowed its legend to grow to ill and mythical proportions: lies about funding for ACORN, which is nowhere mentioned in the bill. Gross and unanswered misrepresentations by McCain about the \"honey bee insurance\" provision.\n",
      "\n",
      "This is a great point. Democrats have this habit of letting the Republicans not only control the conversation, but reduce it to a fourth-grade level. \"Honey bee insurance!\" (Insurance for livestock producers in general, including honeybees because they are [very important and are have had a rough few years](http://en.wikipedia.org/wiki/Colony_Collapse_Disorder).) \"$30M for mice!\" (Wetlands restoration.) \"Fruit fly research!\" (Genetics research.) \n",
      "\n",
      "\n",
      "Example 2:\n",
      "  comment_id: c0883zo\n",
      "  author: Garak\n",
      "  date: 2009-03-13\n",
      "  timestamp: 1236991154\n",
      "  processed_text: ['sadden', 'read', 'time', 'yesterday', 'water', 'bill', 'maher', 'recently', 'seem', 'pretty'] ... (total: 15 words)\n",
      "  original: I was saddened to read this in the Times yesterday. Waters was on Bill Maher recently and she seemed to be pretty smart. Seemed like a good lady.\n",
      "\n",
      "Example 3:\n",
      "  comment_id: c091f56\n",
      "  author: [deleted]\n",
      "  date: 2009-04-22\n",
      "  timestamp: 1240434783\n",
      "  processed_text: ['speaker', 'pelosi', 'culture', 'corruption', 'washington', 'party', 'bad', 'republican']\n",
      "  original: What was that from Speaker Pelosi about the \"\"Culture of Corruption\" in Washington D.C.?\n",
      "Your party is just as bad as the Republicans.\n",
      "\n",
      "Example 4:\n",
      "  comment_id: c095pfx\n",
      "  author: [deleted]\n",
      "  date: 2009-04-27\n",
      "  timestamp: 1240881225\n",
      "  processed_text: ['congresswoman', 'nancy', 'pelosi', 'call', 'washington', 'culture', 'corruption', 'house', 'speaker', 'nancy'] ... (total: 13 words)\n",
      "  original: Congresswoman Nancy Pelosi called Washington D.C. a \"Culture of Corruption\". House Speaker Nancy Pelosi now has an opportunity to do something about it. When?\n",
      "\n",
      "\n",
      "Example 5:\n",
      "  comment_id: c09e1na\n",
      "  author: [deleted]\n",
      "  date: 2009-05-06\n",
      "  timestamp: 1241665126\n",
      "  processed_text: ['share', 'place', 'judas', 'benedict', 'arnold', 'man', 'honor', 'trust', 'respect', 'side']\n",
      "  original: He shares the same place as Judas and Benedict Arnold. A man of no honor, trust or respect from \"both sides\" now.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def inspect_pkl_file(file_path, num_examples=5):\n",
    "    \"\"\"\n",
    "    Load a pickle file and print a few example records\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the pickle file\n",
    "        num_examples: Number of examples to show (default 5)\n",
    "    \"\"\"\n",
    "    # Load the pickle file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # Print information about the data structure\n",
    "    print(f\"Data type: {type(data)}\")\n",
    "    \n",
    "    if isinstance(data, list):\n",
    "        print(f\"Number of items: {len(data)}\")\n",
    "        \n",
    "        # Display examples\n",
    "        print(f\"\\nShowing first {min(num_examples, len(data))} examples:\")\n",
    "        for i, item in enumerate(data[:num_examples]):\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            if isinstance(item, dict):\n",
    "                for key, value in item.items():\n",
    "                    # For processed text, just show a few words\n",
    "                    if key == \"processed_text\" and isinstance(value, list) and len(value) > 10:\n",
    "                        print(f\"  {key}: {value[:10]} ... (total: {len(value)} words)\")\n",
    "                    else:\n",
    "                        print(f\"  {key}: {value}\")\n",
    "            else:\n",
    "                print(item)\n",
    "    else:\n",
    "        print(\"Data is not a list. Structure:\", data)\n",
    "\n",
    "# Example usage\n",
    "inspect_pkl_file(\"processed_comments/democrats/democrats_batch1.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

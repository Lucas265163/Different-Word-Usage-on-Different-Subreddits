{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38f47a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For streaming\n",
    "import sys\n",
    "version = sys.version_info\n",
    "if version.major < 3 or (version.major == 3 and version.minor < 10):\n",
    "    raise RuntimeError(\"This script requires Python 3.10 or higher\")\n",
    "import os\n",
    "from typing import Iterable\n",
    "\n",
    "from fileStreams import getFileJsonStream\n",
    "from utils import FileProgressLog\n",
    "\n",
    "# For processing\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "    \n",
    "import datetime\n",
    "\n",
    "\n",
    "filePathforDemocrats = r\"..\\..\\datasets\\Reddit\\reddit\\subreddits24\\democrats_comments.zst\"\n",
    "filePathforRepublican = r\"..\\..\\datasets\\Reddit\\reddit\\subreddits24\\Republican_comments.zst\"\n",
    "recursive = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbe7a38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file ..\\..\\datasets\\Reddit\\reddit\\subreddits24\\democrats_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments: 2011525it [13:12, 2537.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comment Counts by Period ===\n",
      "before_2016: 129718 comments\n",
      "2017_2020: 492295 comments\n",
      "2021_2024: 1379685 comments\n",
      "\n",
      "Extracting bigrams for before_2016...\n",
      "\n",
      "Extracting bigrams for 2017_2020...\n",
      "\n",
      "Extracting bigrams for 2021_2024...\n",
      "\n",
      "=== Training Word2Vec for before_2016 (129718 comments) ===\n",
      "Vocabulary size: 21814\n",
      "Model saved to reddit_word2vec_democrats_with_stopwords_before_2016.model\n",
      "\n",
      "=== Example Results ===\n",
      "\n",
      "Words similar to 'democracy':\n",
      "  capitalism: 0.6880\n",
      "  society: 0.6871\n",
      "  nation: 0.6373\n",
      "\n",
      "Words similar to 'president':\n",
      "  potus: 0.6785\n",
      "  presidency: 0.6247\n",
      "  leader: 0.5840\n",
      "\n",
      "Words similar to 'policy':\n",
      "  economic_policy: 0.6963\n",
      "  platform: 0.6567\n",
      "  agenda: 0.6520\n",
      "\n",
      "Words similar to 'vote':\n",
      "  voting: 0.6867\n",
      "  support: 0.5513\n",
      "  voter: 0.5493\n",
      "\n",
      "Words similar to 'climate':\n",
      "  structural: 0.6787\n",
      "  austerity: 0.6605\n",
      "  technology: 0.6572\n",
      "\n",
      "=== Training Word2Vec for 2017_2020 (492295 comments) ===\n",
      "Vocabulary size: 34115\n",
      "Model saved to reddit_word2vec_democrats_with_stopwords_2017_2020.model\n",
      "\n",
      "=== Example Results ===\n",
      "\n",
      "Words similar to 'democracy':\n",
      "  republic: 0.7943\n",
      "  institution: 0.6403\n",
      "  democratic_republic: 0.6224\n",
      "\n",
      "Words similar to 'president':\n",
      "  potus: 0.7653\n",
      "  presidential_candidate: 0.5691\n",
      "  leader: 0.5603\n",
      "\n",
      "Words similar to 'policy':\n",
      "  economic_policy: 0.6587\n",
      "  policy_proposal: 0.5717\n",
      "  immigration_policy: 0.5697\n",
      "\n",
      "Words similar to 'vote':\n",
      "  voting: 0.6583\n",
      "  voted: 0.5569\n",
      "  ballot: 0.5510\n",
      "\n",
      "Words similar to 'climate':\n",
      "  environmental: 0.7109\n",
      "  climate_change: 0.6774\n",
      "  development: 0.6508\n",
      "\n",
      "=== Training Word2Vec for 2021_2024 (1379685 comments) ===\n",
      "Vocabulary size: 52714\n",
      "Model saved to reddit_word2vec_democrats_with_stopwords_2021_2024.model\n",
      "\n",
      "=== Example Results ===\n",
      "\n",
      "Words similar to 'democracy':\n",
      "  republic: 0.6877\n",
      "  democratic_republic: 0.6641\n",
      "  fascism: 0.5457\n",
      "\n",
      "Words similar to 'president':\n",
      "  potus: 0.8460\n",
      "  pres: 0.6394\n",
      "  vice_president: 0.5972\n",
      "\n",
      "Words similar to 'policy':\n",
      "  economic_policy: 0.7167\n",
      "  policy_proposal: 0.5710\n",
      "  agenda: 0.5689\n",
      "\n",
      "Words similar to 'vote':\n",
      "  voting: 0.6485\n",
      "  voted: 0.5682\n",
      "  stay_home: 0.5123\n",
      "\n",
      "Words similar to 'climate':\n",
      "  climate_change: 0.6577\n",
      "  environment: 0.5882\n",
      "  environmental: 0.5465\n",
      "Processing file ..\\..\\datasets\\Reddit\\reddit\\subreddits24\\Republican_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments: 1405486it [11:45, 1993.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comment Counts by Period ===\n",
      "before_2016: 265370 comments\n",
      "2017_2020: 452285 comments\n",
      "2021_2024: 680869 comments\n",
      "\n",
      "Extracting bigrams for before_2016...\n",
      "\n",
      "Extracting bigrams for 2017_2020...\n",
      "\n",
      "Extracting bigrams for 2021_2024...\n",
      "\n",
      "=== Training Word2Vec for before_2016 (265370 comments) ===\n",
      "Vocabulary size: 30882\n",
      "Model saved to reddit_word2vec_republican_with_stopwords_before_2016.model\n",
      "\n",
      "=== Example Results ===\n",
      "\n",
      "Words similar to 'democracy':\n",
      "  republic: 0.7965\n",
      "  tyranny: 0.7113\n",
      "  dictatorship: 0.6685\n",
      "\n",
      "Words similar to 'president':\n",
      "  potus: 0.7824\n",
      "  presidency: 0.6026\n",
      "  vice_president: 0.5950\n",
      "\n",
      "Words similar to 'policy':\n",
      "  economic_policy: 0.6946\n",
      "  fiscal_policy: 0.6153\n",
      "  policy_proposal: 0.5841\n",
      "\n",
      "Words similar to 'vote':\n",
      "  voting: 0.7103\n",
      "  ballot: 0.5388\n",
      "  stay_home: 0.5329\n",
      "\n",
      "Words similar to 'climate':\n",
      "  global_climate: 0.6726\n",
      "  paradigm: 0.6414\n",
      "  warming: 0.6055\n",
      "\n",
      "=== Training Word2Vec for 2017_2020 (452285 comments) ===\n",
      "Vocabulary size: 32958\n",
      "Model saved to reddit_word2vec_republican_with_stopwords_2017_2020.model\n",
      "\n",
      "=== Example Results ===\n",
      "\n",
      "Words similar to 'democracy':\n",
      "  republic: 0.7112\n",
      "  democratic_republic: 0.6007\n",
      "  society: 0.5874\n",
      "\n",
      "Words similar to 'president':\n",
      "  potus: 0.7460\n",
      "  presidential_candidate: 0.6236\n",
      "  pres: 0.6174\n",
      "\n",
      "Words similar to 'policy':\n",
      "  economic_policy: 0.6719\n",
      "  foreign_policy: 0.5876\n",
      "  proposal: 0.5562\n",
      "\n",
      "Words similar to 'vote':\n",
      "  voting: 0.5928\n",
      "  ballot: 0.5759\n",
      "  voter: 0.5033\n",
      "\n",
      "Words similar to 'climate':\n",
      "  atmosphere: 0.5760\n",
      "  climate_change: 0.5534\n",
      "  economic: 0.5512\n",
      "\n",
      "=== Training Word2Vec for 2021_2024 (680869 comments) ===\n",
      "Vocabulary size: 37396\n",
      "Model saved to reddit_word2vec_republican_with_stopwords_2021_2024.model\n",
      "\n",
      "=== Example Results ===\n",
      "\n",
      "Words similar to 'democracy':\n",
      "  republic: 0.7549\n",
      "  constitutional_republic: 0.6406\n",
      "  dictatorship: 0.5629\n",
      "\n",
      "Words similar to 'president':\n",
      "  potus: 0.7423\n",
      "  vice_president: 0.6741\n",
      "  vp: 0.6709\n",
      "\n",
      "Words similar to 'policy':\n",
      "  economic_policy: 0.6552\n",
      "  foreign_policy: 0.6157\n",
      "  immigration_policy: 0.5796\n",
      "\n",
      "Words similar to 'vote':\n",
      "  voting: 0.3891\n",
      "  ballot: 0.3598\n",
      "  elect: 0.3584\n",
      "\n",
      "Words similar to 'climate':\n",
      "  climate_change: 0.6089\n",
      "  societal: 0.5904\n",
      "  environmental: 0.5855\n",
      "Done :>\n"
     ]
    }
   ],
   "source": [
    "def processFile(path, party, without_stopwords=True):\n",
    "    print(f\"Processing file {path}\")\n",
    "    \n",
    "    # Download necessary NLTK resources\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)  # For POS tagging\n",
    "    \n",
    "    # Initialize lemmatizer and stop words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Create empty lists for each time period\n",
    "    chunks = {\n",
    "        \"before_2016\": [],\n",
    "        \"2017_2020\": [],\n",
    "        \"2021_2024\": [],\n",
    "    }\n",
    "    \n",
    "    # Track counts\n",
    "    counts = {period: 0 for period in chunks.keys()}\n",
    "    \n",
    "    with open(path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Skipping unknown file {path}\")\n",
    "            return\n",
    "        for row in tqdm(jsonStream, desc=\"Processing comments\"):\n",
    "            if \"body\" not in row or \"created_utc\" not in row:\n",
    "                continue\n",
    "            \n",
    "            # Get the comment text and timestamp\n",
    "            text = row[\"body\"]\n",
    "            created_timestamp = row[\"created_utc\"]\n",
    "            \n",
    "            # Convert timestamp to year\n",
    "            year = datetime.datetime.fromtimestamp(int(created_timestamp)).year\n",
    "            \n",
    "            # Determine which chunk this comment belongs to\n",
    "            chunk_key = None\n",
    "            if year <= 2016:\n",
    "                chunk_key = \"before_2016\"\n",
    "            elif 2017 <= year <= 2020:\n",
    "                chunk_key = \"2017_2020\"\n",
    "            elif 2021 <= year <= 2024:\n",
    "                chunk_key = \"2021_2024\"\n",
    "            \n",
    "            # Process text\n",
    "            # Remove URLs\n",
    "            txt = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "            \n",
    "            # Remove non-alphanumeric characters and convert to lowercase\n",
    "            txt = re.sub(\"[^A-Za-z0-9']+\", ' ', txt).lower()\n",
    "            \n",
    "            # Replace special characters with spaces\n",
    "            txt = re.sub(r\"['\\-_]\", ' ', txt)\n",
    "            \n",
    "            # Tokenize\n",
    "            words = txt.split()\n",
    "            \n",
    "            # Tag words with parts of speech for better lemmatization\n",
    "            tagged_words = nltk.pos_tag(words)\n",
    "            processed_words = []\n",
    "            \n",
    "            # Skip empty comments\n",
    "            if not words:\n",
    "                continue\n",
    "            \n",
    "            for word, tag in tagged_words:\n",
    "                if without_stopwords:\n",
    "                    if word in stop_words:\n",
    "                        continue\n",
    "                \n",
    "                # Convert Penn Treebank tag to WordNet tag\n",
    "                if tag.startswith('J'):\n",
    "                    wordnet_pos = 'a'  # adjective\n",
    "                elif tag.startswith('V'):\n",
    "                    wordnet_pos = 'v'  # verb\n",
    "                elif tag.startswith('N'):\n",
    "                    wordnet_pos = 'n'  # noun\n",
    "                elif tag.startswith('R'):\n",
    "                    wordnet_pos = 'r'  # adverb\n",
    "                else:\n",
    "                    wordnet_pos = 'n'  # default to noun\n",
    "                    \n",
    "                # Lemmatize with the correct POS\n",
    "                lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "                processed_words.append(lemma)\n",
    "            \n",
    "            # Add to appropriate chunk if it has words\n",
    "            if processed_words:\n",
    "                chunks[chunk_key].append(processed_words)\n",
    "                counts[chunk_key] += 1\n",
    "                \n",
    "    # Print statistics\n",
    "    print(\"\\n=== Comment Counts by Period ===\")\n",
    "    for period, count in counts.items():\n",
    "        print(f\"{period}: {count} comments\")\n",
    "        \n",
    "    # Extract bigrams from each time period\n",
    "    for period, comments in chunks.items():\n",
    "        if len(comments) > 0:\n",
    "            print(f\"\\nExtracting bigrams for {period}...\")\n",
    "            # Build bigram model\n",
    "            phrases = Phrases(comments, min_count=5, threshold=10)\n",
    "            bigram_model = Phraser(phrases)\n",
    "            \n",
    "            # Apply bigram model to create comments with bigrams\n",
    "            bigrammed_comments = [bigram_model[comment] for comment in comments]\n",
    "            chunks[period] = bigrammed_comments\n",
    "        \n",
    "    # Train a Word2Vec model for each time period\n",
    "    for period, comments in chunks.items():\n",
    "        if len(comments) > 0:\n",
    "            print(f\"\\n=== Training Word2Vec for {period} ({len(comments)} comments) ===\")\n",
    "            \n",
    "            # Initialize and train model\n",
    "            model = Word2Vec(\n",
    "                vector_size=300,\n",
    "                window=5,\n",
    "                min_count=5,\n",
    "                workers=16\n",
    "            )\n",
    "            \n",
    "            # Build vocabulary\n",
    "            model.build_vocab(comments)\n",
    "            print(f\"Vocabulary size: {len(model.wv.index_to_key)}\")\n",
    "            \n",
    "            # Train the model\n",
    "            model.train(\n",
    "                comments, \n",
    "                total_examples=len(comments), \n",
    "                epochs=5\n",
    "            )\n",
    "            \n",
    "            # Save the model\n",
    "            model_path = f\"reddit_word2vec_{party}_{period}.model\"\n",
    "            model.save(model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "            \n",
    "            # Show example results\n",
    "            print(\"\\n=== Example Results ===\")\n",
    "            test_words = [\"democracy\", \"president\", \"policy\", \"vote\", \"climate\"]\n",
    "            for word in test_words:\n",
    "                try:\n",
    "                    print(f\"\\nWords similar to '{word}':\")\n",
    "                    similar = model.wv.most_similar(word, topn=3)\n",
    "                    for similar_word, similarity in similar:\n",
    "                        print(f\"  {similar_word}: {similarity:.4f}\")\n",
    "                except KeyError:\n",
    "                    print(f\"  '{word}' not found in {period} vocabulary\")\n",
    "        \n",
    "def main():\n",
    "    processFile(filePathforDemocrats, \"democrats_with_stopwords\", without_stopwords=False)\n",
    "    processFile(filePathforRepublican, \"republican_with_stopwords\", without_stopwords=False)\n",
    "    \n",
    "    print(\"Done :>\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b3d7f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing entire dataset from ..\\..\\datasets\\Reddit\\reddit\\subreddits24\\democrats_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments: 2011525it [14:54, 2249.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments processed: 2011525\n",
      "\n",
      "Extracting bigrams...\n",
      "\n",
      "=== Training Word2Vec model on entire dataset ===\n",
      "Vocabulary size: 80120\n",
      "Model saved to reddit_word2vec_democrats_all_periods.model\n",
      "\n",
      "=== Example Results ===\n",
      "\n",
      "Words similar to 'democracy':\n",
      "  republic: 0.7175\n",
      "  democratic_republic: 0.6618\n",
      "  representative_democracy: 0.6310\n",
      "  fascism: 0.5854\n",
      "  democratic_institution: 0.5639\n",
      "  autocracy: 0.5592\n",
      "  dictatorship: 0.5455\n",
      "  american_experiment: 0.5292\n",
      "  oligarchy: 0.5245\n",
      "  tyranny: 0.5199\n",
      "\n",
      "Words similar to 'president':\n",
      "  potus: 0.7967\n",
      "  vice_president: 0.5898\n",
      "  presidency: 0.5754\n",
      "  prez: 0.5734\n",
      "  pres: 0.5542\n",
      "  presidential_candidate: 0.5444\n",
      "  commander_chief: 0.5376\n",
      "  second_term: 0.5131\n",
      "  presidential: 0.5073\n",
      "  vp: 0.5063\n",
      "\n",
      "Words similar to 'policy':\n",
      "  economic_policy: 0.7203\n",
      "  policy_proposal: 0.6181\n",
      "  immigration_policy: 0.6073\n",
      "  agenda: 0.5583\n",
      "  proposal: 0.5234\n",
      "  legislation: 0.4938\n",
      "  platform: 0.4934\n",
      "  fiscal_policy: 0.4893\n",
      "  domestic_policy: 0.4774\n",
      "  environmental_policy: 0.4755\n",
      "\n",
      "Words similar to 'trump':\n",
      "  donald_trump: 0.6079\n",
      "  trumps: 0.5868\n",
      "  djt: 0.5572\n",
      "  dt: 0.5233\n",
      "  biden: 0.5217\n",
      "  tfg: 0.4934\n",
      "  drumpf: 0.4899\n",
      "  harris: 0.4730\n",
      "  kamala: 0.4408\n",
      "  donald: 0.4359\n",
      "\n",
      "Words similar to 'biden':\n",
      "  joe_biden: 0.6918\n",
      "  bidens: 0.6802\n",
      "  joe: 0.6388\n",
      "  harris: 0.5796\n",
      "  trump: 0.5217\n",
      "  kamala: 0.5174\n",
      "  hrc: 0.4832\n",
      "  clinton: 0.4711\n",
      "  bernie: 0.4688\n",
      "  vp_harris: 0.4685\n",
      "Processing entire dataset from ..\\..\\datasets\\Reddit\\reddit\\subreddits24\\Republican_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments: 1405486it [13:25, 1745.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments processed: 1405486\n",
      "\n",
      "Extracting bigrams...\n",
      "\n",
      "=== Training Word2Vec model on entire dataset ===\n",
      "Vocabulary size: 70486\n",
      "Model saved to reddit_word2vec_republican_all_periods.model\n",
      "\n",
      "=== Example Results ===\n",
      "\n",
      "Words similar to 'democracy':\n",
      "  republic: 0.7347\n",
      "  constitutional_republic: 0.6772\n",
      "  democratic_republic: 0.6603\n",
      "  dictatorship: 0.6052\n",
      "  democratic_process: 0.6036\n",
      "  representative_democracy: 0.6028\n",
      "  direct_democracy: 0.5999\n",
      "  pure_democracy: 0.5857\n",
      "  monarchy: 0.5649\n",
      "  mob_rule: 0.5537\n",
      "\n",
      "Words similar to 'president':\n",
      "  potus: 0.7545\n",
      "  pres: 0.6406\n",
      "  presidency: 0.6232\n",
      "  vice_president: 0.5959\n",
      "  former_president: 0.5942\n",
      "  commander_chief: 0.5740\n",
      "  vp: 0.5697\n",
      "  presidential: 0.5693\n",
      "  administration: 0.5362\n",
      "  presidential_candidate: 0.5287\n",
      "\n",
      "Words similar to 'policy':\n",
      "  economic_policy: 0.6785\n",
      "  immigration_policy: 0.5804\n",
      "  fiscal_policy: 0.5629\n",
      "  foreign_policy: 0.5073\n",
      "  policy_proposal: 0.4965\n",
      "  agenda: 0.4739\n",
      "  legislation: 0.4579\n",
      "  proposal: 0.4391\n",
      "  stance: 0.4347\n",
      "  initiative: 0.4343\n",
      "\n",
      "Words similar to 'trump':\n",
      "  biden: 0.5959\n",
      "  trumps: 0.5927\n",
      "  djt: 0.5554\n",
      "  donald_trump: 0.5506\n",
      "  biden_harris: 0.5213\n",
      "  obama: 0.5175\n",
      "  clinton: 0.5144\n",
      "  romney: 0.5141\n",
      "  hillary: 0.5106\n",
      "  harris: 0.5078\n",
      "\n",
      "Words similar to 'biden':\n",
      "  joe_biden: 0.6334\n",
      "  joe: 0.6308\n",
      "  bidens: 0.6296\n",
      "  biden_harris: 0.6000\n",
      "  trump: 0.5959\n",
      "  harris: 0.5918\n",
      "  kamala: 0.5756\n",
      "  clinton: 0.5293\n",
      "  obama: 0.5176\n",
      "  hillary: 0.5021\n",
      "Done processing both datasets :)\n"
     ]
    }
   ],
   "source": [
    "def process_entire_dataset(path, party):\n",
    "    print(f\"Processing entire dataset from {path}\")\n",
    "    \n",
    "    # Download necessary NLTK resources\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)  # For POS tagging\n",
    "    \n",
    "    # Initialize lemmatizer and stop words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Storage for all processed comments\n",
    "    all_comments = []\n",
    "    total_comments = 0\n",
    "    \n",
    "    # Process the dataset\n",
    "    with open(path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Skipping unknown file {path}\")\n",
    "            return\n",
    "        \n",
    "        for row in tqdm(jsonStream, desc=\"Processing comments\"):\n",
    "            total_comments += 1\n",
    "            if \"body\" not in row:\n",
    "                continue\n",
    "            \n",
    "            # Get the comment text\n",
    "            text = row[\"body\"]\n",
    "            \n",
    "            # Process text\n",
    "            # Remove URLs\n",
    "            txt = re.sub(r'http\\S+', '', str(text))\n",
    "            \n",
    "            # Keep alphanumeric + apostrophes, replace with spaces, lowercase\n",
    "            txt = re.sub(r\"[^A-Za-z0-9']+\", ' ', txt).lower()\n",
    "            \n",
    "            # Replace special characters with spaces\n",
    "            txt = re.sub(r\"['\\-_]\", ' ', txt)\n",
    "            \n",
    "            # Tokenize\n",
    "            words = txt.split()\n",
    "            \n",
    "            # Skip empty comments\n",
    "            if not words:\n",
    "                continue\n",
    "                \n",
    "            # Tag words with parts of speech for better lemmatization\n",
    "            tagged_words = nltk.pos_tag(words)\n",
    "            processed_words = []\n",
    "            \n",
    "            for word, tag in tagged_words:\n",
    "                if word in stop_words:\n",
    "                    continue\n",
    "                \n",
    "                # Convert Penn Treebank tag to WordNet tag\n",
    "                if tag.startswith('J'):\n",
    "                    wordnet_pos = 'a'  # adjective\n",
    "                elif tag.startswith('V'):\n",
    "                    wordnet_pos = 'v'  # verb\n",
    "                elif tag.startswith('N'):\n",
    "                    wordnet_pos = 'n'  # noun\n",
    "                elif tag.startswith('R'):\n",
    "                    wordnet_pos = 'r'  # adverb\n",
    "                else:\n",
    "                    wordnet_pos = 'n'  # default to noun\n",
    "                    \n",
    "                # Lemmatize with the correct POS\n",
    "                lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "                processed_words.append(lemma)\n",
    "            \n",
    "            # Add to comments if it has words\n",
    "            if processed_words:\n",
    "                all_comments.append(processed_words)\n",
    "    \n",
    "    print(f\"Total comments processed: {total_comments}\")\n",
    "    \n",
    "    # Extract bigrams from the dataset\n",
    "    print(\"\\nExtracting bigrams...\")\n",
    "    phrases = Phrases(all_comments, min_count=5, threshold=10)\n",
    "    bigram_model = Phraser(phrases)\n",
    "    \n",
    "    # Apply bigram model to create comments with bigrams\n",
    "    bigrammed_comments = [bigram_model[comment] for comment in all_comments]\n",
    "    \n",
    "    # Train Word2Vec model\n",
    "    print(\"\\n=== Training Word2Vec model on entire dataset ===\")\n",
    "    model = Word2Vec(\n",
    "        vector_size=300,\n",
    "        window=5,\n",
    "        min_count=5,\n",
    "        workers=16\n",
    "    )\n",
    "    \n",
    "    # Build vocabulary\n",
    "    model.build_vocab(bigrammed_comments)\n",
    "    print(f\"Vocabulary size: {len(model.wv.index_to_key)}\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.train(\n",
    "        bigrammed_comments, \n",
    "        total_examples=len(bigrammed_comments), \n",
    "        epochs=5\n",
    "    )\n",
    "    \n",
    "    # Save the model\n",
    "    model_path = f\"reddit_word2vec_{party}_all_periods.model\"\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    # Show example results\n",
    "    print(\"\\n=== Example Results ===\")\n",
    "    test_words = [\"democracy\", \"president\", \"policy\", \"trump\", \"biden\"]\n",
    "    for word in test_words:\n",
    "        try:\n",
    "            print(f\"\\nWords similar to '{word}':\")\n",
    "            similar = model.wv.most_similar(word, topn=10)\n",
    "            for similar_word, similarity in similar:\n",
    "                print(f\"  {similar_word}: {similarity:.4f}\")\n",
    "        except KeyError:\n",
    "            print(f\"  '{word}' not found in vocabulary\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main_entire_dataset():\n",
    "    process_entire_dataset(filePathforDemocrats, \"democrats\")\n",
    "    process_entire_dataset(filePathforRepublican, \"republican\")\n",
    "    \n",
    "    print(\"Done processing both datasets :)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_entire_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

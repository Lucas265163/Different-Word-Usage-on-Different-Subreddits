{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb4c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: datasets/Republican_comments.zst\n",
      "Starting to process republican comments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing republican comments: 157671it [00:07, 21837.92it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "import html\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from fileStreams import getFileJsonStream\n",
    "from collections import defaultdict\n",
    "\n",
    "# Pre-download required NLTK resources once at the module level\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# Initialize global resources once\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "# POS tag cache to avoid redundant tagging\n",
    "POS_CACHE = {}\n",
    "# Lemma cache to avoid redundant lemmatization\n",
    "LEMMA_CACHE = {}\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Convert NLTK POS tag to WordNet POS tag\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # verb\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'  # noun\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # adverb\n",
    "    else:\n",
    "        return 'n'  # default as noun\n",
    "\n",
    "def preprocess_text(text, lemmatize=True, without_stopwords=True):\n",
    "    \"\"\"Preprocess Reddit text content with optimized NLTK operations\"\"\"\n",
    "    # Handle HTML entities\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Unicode normalization\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Remove URLs and Markdown formatting\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)\n",
    "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)\n",
    "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)\n",
    "    \n",
    "    # Remove subreddit and user references\n",
    "    text = re.sub(r'/r/\\w+', '', text)\n",
    "    text = re.sub(r'r/\\w+', '', text)\n",
    "    text = re.sub(r'/u/\\w+', '', text)\n",
    "    text = re.sub(r'u/\\w+', '', text)\n",
    "    \n",
    "    # Basic text cleaning\n",
    "    text = re.sub(\"[^A-Za-z]+\", ' ', text).lower()\n",
    "    \n",
    "    # Remove single letters (except 'i')\n",
    "    text = re.sub(r'\\b([a-hj-z])\\b', '', text)\n",
    "    \n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "        \n",
    "    # Lemmatization\n",
    "    if lemmatize:\n",
    "        stop_words = STOP_WORDS if without_stopwords else set()\n",
    "        \n",
    "        # Process words in a single batch for better performance\n",
    "        words_to_tag = [word for word in words if not (without_stopwords and word in stop_words)]\n",
    "        \n",
    "        if not words_to_tag:\n",
    "            return []\n",
    "            \n",
    "        # First check our caches\n",
    "        uncached_words = [word for word in words_to_tag if word not in POS_CACHE]\n",
    "        \n",
    "        # Only perform POS tagging on words not in cache\n",
    "        if uncached_words:\n",
    "            tagged_uncached = nltk.pos_tag(uncached_words)\n",
    "            # Update cache with new tags\n",
    "            for word, tag in tagged_uncached:\n",
    "                POS_CACHE[word] = tag\n",
    "        \n",
    "        processed_words = []\n",
    "        \n",
    "        # Process each word with cached information\n",
    "        for word in words_to_tag:\n",
    "            tag = POS_CACHE[word]\n",
    "            wordnet_pos = get_wordnet_pos(tag)\n",
    "            \n",
    "            # Check lemma cache first\n",
    "            lemma_key = (word, wordnet_pos)\n",
    "            if lemma_key in LEMMA_CACHE:\n",
    "                lemma = LEMMA_CACHE[lemma_key]\n",
    "            else:\n",
    "                lemma = LEMMATIZER.lemmatize(word, pos=wordnet_pos)\n",
    "                LEMMA_CACHE[lemma_key] = lemma\n",
    "                \n",
    "            processed_words.append(lemma)\n",
    "                \n",
    "        return processed_words\n",
    "    \n",
    "    return words\n",
    "\n",
    "def process_and_save_comments(path, subreddit, without_stopwords=True, batch_size=1000000):\n",
    "    \"\"\"Process comments and save in batches\"\"\"\n",
    "    print(f\"Processing file: {path}\")\n",
    "    \n",
    "    # Batch processing counters\n",
    "    batch_count = 0\n",
    "    batch_number = 1\n",
    "    total_count = 0\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = f\"processed_comments/{subreddit}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create data structure for comments without author filtering\n",
    "    comments_batch = []\n",
    "    \n",
    "    print(f\"Starting to process {subreddit} comments...\")\n",
    "    with open(path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Unable to read file {path}\")\n",
    "            return\n",
    "        \n",
    "        for row in tqdm(jsonStream, desc=f\"Processing {subreddit} comments\"):\n",
    "            if \"body\" not in row or \"created_utc\" not in row or \"author\" not in row or \"id\" not in row:\n",
    "                continue\n",
    "                \n",
    "            author = row[\"author\"]\n",
    "            if author in {\"AutoModerator\", \"election_info_bot\"}:\n",
    "                continue\n",
    "            \n",
    "            comment_id = row[\"id\"]\n",
    "            text = row[\"body\"]\n",
    "            created_timestamp = row[\"created_utc\"]\n",
    "            date = datetime.datetime.fromtimestamp(int(created_timestamp))\n",
    "            \n",
    "            # Process text with optimized functions\n",
    "            processed_words = preprocess_text(text, lemmatize=True, without_stopwords=without_stopwords)\n",
    "            \n",
    "            if processed_words:\n",
    "                # Save processed comment with metadata\n",
    "                comment_data = {\n",
    "                    \"comment_id\": comment_id,\n",
    "                    \"author\": author,\n",
    "                    \"date\": date.strftime(\"%Y-%m-%d\"),\n",
    "                    \"timestamp\": created_timestamp,\n",
    "                    \"processed_text\": processed_words,  # Original order preserved\n",
    "                    \"original\": text\n",
    "                }\n",
    "                \n",
    "                comments_batch.append(comment_data)\n",
    "                batch_count += 1\n",
    "                \n",
    "            # Check if we need to save the current batch\n",
    "            if batch_count >= batch_size:\n",
    "                print(f\"\\nReached {batch_size} comments, saving batch {batch_number}...\")\n",
    "                \n",
    "                # Save batch directly without filtering\n",
    "                save_path = f\"{output_dir}/{subreddit}_batch{batch_number}.pkl\"\n",
    "                with open(save_path, \"wb\") as out_file:\n",
    "                    pickle.dump(comments_batch, out_file)\n",
    "                \n",
    "                print(f\"Saved {len(comments_batch)} comments to {save_path}\")\n",
    "                \n",
    "                # Reset batch data\n",
    "                comments_batch = []\n",
    "                batch_count = 0\n",
    "                batch_number += 1\n",
    "                total_count += batch_size\n",
    "                \n",
    "                # Report cache stats\n",
    "                print(f\"POS tag cache size: {len(POS_CACHE)} words\")\n",
    "                print(f\"Lemma cache size: {len(LEMMA_CACHE)} word-POS pairs\")\n",
    "    \n",
    "    # Process any remaining comments\n",
    "    if batch_count > 0:\n",
    "        print(f\"\\nSaving remaining {batch_count} comments...\")\n",
    "        \n",
    "        # Save batch\n",
    "        save_path = f\"{output_dir}/{subreddit}_batch{batch_number}.pkl\"\n",
    "        with open(save_path, \"wb\") as out_file:\n",
    "            pickle.dump(comments_batch, out_file)\n",
    "        \n",
    "        print(f\"Saved {len(comments_batch)} comments to {save_path}\")\n",
    "        total_count += batch_count\n",
    "    \n",
    "    print(f\"\\nCompleted processing {subreddit} comments!\")\n",
    "    print(f\"Total comments saved: {total_count}\")\n",
    "    print(f\"Final POS tag cache size: {len(POS_CACHE)} words\")\n",
    "    print(f\"Final lemma cache size: {len(LEMMA_CACHE)} word-POS pairs\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    # Define data file paths\n",
    "    files = {\n",
    "        \"democrats\": r\"datasets/democrats_comments.zst\",\n",
    "        \"republican\": r\"datasets/Republican_comments.zst\",\n",
    "        \"conservative\": r\"datasets/Conservative_comments.zst\",\n",
    "        \"liberal\": r\"datasets/Liberal_comments.zst\",\n",
    "        \"backpacking\": r\"datasets/backpacking_comments.zst\",\n",
    "        \"vagabond\": r\"datasets/vagabond_comments.zst\"\n",
    "    }\n",
    "    \n",
    "    # Choose subreddit to process\n",
    "    subreddit_to_process = \"conservative\"  # Change this to process other subreddits\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(f\"processed_comments/{subreddit_to_process}\", exist_ok=True)\n",
    "\n",
    "    if subreddit_to_process in files:\n",
    "        process_and_save_comments(  \n",
    "            files[subreddit_to_process],\n",
    "            subreddit_to_process,\n",
    "            without_stopwords=True,\n",
    "            batch_size=1000000\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Subreddit not found: {subreddit_to_process}\")\n",
    "        print(f\"Available subreddits: {', '.join(files.keys())}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6287775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'list'>\n",
      "Number of items: 1000000\n",
      "\n",
      "Showing first 5 examples:\n",
      "\n",
      "Example 1:\n",
      "  comment_id: c07p2u0\n",
      "  author: Garak\n",
      "  date: 2009-02-16\n",
      "  timestamp: 1234791099\n",
      "  processed_text: ['allow', 'legend', 'grow', 'ill', 'mythical', 'proportion', 'lie', 'fund', 'acorn', 'nowhere'] ... (total: 49 words)\n",
      "  original: &gt;  And they have allowed its legend to grow to ill and mythical proportions: lies about funding for ACORN, which is nowhere mentioned in the bill. Gross and unanswered misrepresentations by McCain about the \"honey bee insurance\" provision.\n",
      "\n",
      "This is a great point. Democrats have this habit of letting the Republicans not only control the conversation, but reduce it to a fourth-grade level. \"Honey bee insurance!\" (Insurance for livestock producers in general, including honeybees because they are [very important and are have had a rough few years](http://en.wikipedia.org/wiki/Colony_Collapse_Disorder).) \"$30M for mice!\" (Wetlands restoration.) \"Fruit fly research!\" (Genetics research.) \n",
      "\n",
      "\n",
      "Example 2:\n",
      "  comment_id: c0883zo\n",
      "  author: Garak\n",
      "  date: 2009-03-13\n",
      "  timestamp: 1236991154\n",
      "  processed_text: ['sadden', 'read', 'time', 'yesterday', 'water', 'bill', 'maher', 'recently', 'seem', 'pretty'] ... (total: 15 words)\n",
      "  original: I was saddened to read this in the Times yesterday. Waters was on Bill Maher recently and she seemed to be pretty smart. Seemed like a good lady.\n",
      "\n",
      "Example 3:\n",
      "  comment_id: c091f56\n",
      "  author: [deleted]\n",
      "  date: 2009-04-22\n",
      "  timestamp: 1240434783\n",
      "  processed_text: ['speaker', 'pelosi', 'culture', 'corruption', 'washington', 'party', 'bad', 'republican']\n",
      "  original: What was that from Speaker Pelosi about the \"\"Culture of Corruption\" in Washington D.C.?\n",
      "Your party is just as bad as the Republicans.\n",
      "\n",
      "Example 4:\n",
      "  comment_id: c095pfx\n",
      "  author: [deleted]\n",
      "  date: 2009-04-27\n",
      "  timestamp: 1240881225\n",
      "  processed_text: ['congresswoman', 'nancy', 'pelosi', 'call', 'washington', 'culture', 'corruption', 'house', 'speaker', 'nancy'] ... (total: 13 words)\n",
      "  original: Congresswoman Nancy Pelosi called Washington D.C. a \"Culture of Corruption\". House Speaker Nancy Pelosi now has an opportunity to do something about it. When?\n",
      "\n",
      "\n",
      "Example 5:\n",
      "  comment_id: c09e1na\n",
      "  author: [deleted]\n",
      "  date: 2009-05-06\n",
      "  timestamp: 1241665126\n",
      "  processed_text: ['share', 'place', 'judas', 'benedict', 'arnold', 'man', 'honor', 'trust', 'respect', 'side']\n",
      "  original: He shares the same place as Judas and Benedict Arnold. A man of no honor, trust or respect from \"both sides\" now.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def inspect_pkl_file(file_path, num_examples=5):\n",
    "    \"\"\"\n",
    "    Load a pickle file and print a few example records\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the pickle file\n",
    "        num_examples: Number of examples to show (default 5)\n",
    "    \"\"\"\n",
    "    # Load the pickle file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # Print information about the data structure\n",
    "    print(f\"Data type: {type(data)}\")\n",
    "    \n",
    "    if isinstance(data, list):\n",
    "        print(f\"Number of items: {len(data)}\")\n",
    "        \n",
    "        # Display examples\n",
    "        print(f\"\\nShowing first {min(num_examples, len(data))} examples:\")\n",
    "        for i, item in enumerate(data[:num_examples]):\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            if isinstance(item, dict):\n",
    "                for key, value in item.items():\n",
    "                    # For processed text, just show a few words\n",
    "                    if key == \"processed_text\" and isinstance(value, list) and len(value) > 10:\n",
    "                        print(f\"  {key}: {value[:10]} ... (total: {len(value)} words)\")\n",
    "                    else:\n",
    "                        print(f\"  {key}: {value}\")\n",
    "            else:\n",
    "                print(item)\n",
    "    else:\n",
    "        print(\"Data is not a list. Structure:\", data)\n",
    "\n",
    "# Example usage\n",
    "inspect_pkl_file(\"processed_comments/democrats/democrats_batch1.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

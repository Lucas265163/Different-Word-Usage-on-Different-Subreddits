{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326f3dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For streaming\n",
    "import sys\n",
    "version = sys.version_info\n",
    "if version.major < 3 or (version.major == 3 and version.minor < 10):\n",
    "    raise RuntimeError(\"This script requires Python 3.10 or higher\")\n",
    "import os\n",
    "from typing import Iterable\n",
    "\n",
    "from fileStreams import getFileJsonStream\n",
    "from utils import FileProgressLog\n",
    "\n",
    "# For processing\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "    \n",
    "import datetime\n",
    "\n",
    "\n",
    "filePathforDemocrats = r\"datasets/democrats_comments.zst\"\n",
    "filePathforRepublican = r\"datasets/Republican_comments.zst\"\n",
    "recursive = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10273cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your data (replace with your actual file path)\n",
    "df = pd.read_json('datasets/republican_comments.zst', lines=True, compression='infer')\n",
    "\n",
    "# Count the number of comments per user\n",
    "user_comment_counts = df['author'].value_counts().reset_index()\n",
    "user_comment_counts.columns = ['author', 'comment_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8824ff60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>comment_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>227727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>97588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RedBaronsBrother</td>\n",
       "      <td>22257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IBiteYou</td>\n",
       "      <td>20383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>keypuncher</td>\n",
       "      <td>13879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tampammm</td>\n",
       "      <td>6354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The_seph_i_am</td>\n",
       "      <td>3676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wethedownvoted</td>\n",
       "      <td>3192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stevano</td>\n",
       "      <td>2877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3-10</td>\n",
       "      <td>2851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M_i_c_K</td>\n",
       "      <td>2637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MicahWeeks</td>\n",
       "      <td>2468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Comprehensive-Tell13</td>\n",
       "      <td>2423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MikeyPh</td>\n",
       "      <td>2373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SurburbanCowboy</td>\n",
       "      <td>2074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Coast_watcher</td>\n",
       "      <td>1876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mbarnar</td>\n",
       "      <td>1735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Morgue724</td>\n",
       "      <td>1670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>justusethatname</td>\n",
       "      <td>1557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Yosoff</td>\n",
       "      <td>1549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  author  comment_count\n",
       "0              [deleted]         227727\n",
       "1          AutoModerator          97588\n",
       "2       RedBaronsBrother          22257\n",
       "3               IBiteYou          20383\n",
       "4             keypuncher          13879\n",
       "5               Tampammm           6354\n",
       "6          The_seph_i_am           3676\n",
       "7         wethedownvoted           3192\n",
       "8                stevano           2877\n",
       "9                   3-10           2851\n",
       "10               M_i_c_K           2637\n",
       "11            MicahWeeks           2468\n",
       "12  Comprehensive-Tell13           2423\n",
       "13               MikeyPh           2373\n",
       "14       SurburbanCowboy           2074\n",
       "15         Coast_watcher           1876\n",
       "16               mbarnar           1735\n",
       "17             Morgue724           1670\n",
       "18       justusethatname           1557\n",
       "19                Yosoff           1549"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the table of user comment frequencies\n",
    "user_comment_counts.head(20)  # Show top 20 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fcec56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users with fewer than 5 comments: 85820\n"
     ]
    }
   ],
   "source": [
    "# Count how many users have fewer than 5 comments\n",
    "num_users_less_than_5 = (user_comment_counts['comment_count'] < 3).sum()\n",
    "print(f'Number of users with fewer than 5 comments: {num_users_less_than_5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a58c842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file datasets/democrats_comments.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments: 2011525it [07:31, 4455.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comment Counts by Period ===\n",
      "before_2016: 0 comments\n",
      "2017_2020: 0 comments\n",
      "2021_2024: 1343406 comments\n",
      "before_2016: 0 comments after filtering words by user count\n",
      "2017_2020: 0 comments after filtering words by user count\n",
      "2021_2024: 1341722 comments after filtering words by user count\n",
      "\n",
      "Extracting bigrams for 2021_2024...\n",
      "\n",
      "=== Training Word2Vec for 2021_2024 (1341722 comments) ===\n",
      "Vocabulary size: 27107\n",
      "Model saved to models/reddit_word2vec_10_10_filterd_democrats_2021_2024.model\n",
      "Done :>\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_word2vect_model(path, party, without_stopwords=True, phrases_min_count=5, word2vec_min_count=5):\n",
    "    print(f\"Processing file {path}\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # For each period, track comments and user-word usage\n",
    "    chunks = {\n",
    "        \"before_2016\": [],\n",
    "        \"2017_2020\": [],\n",
    "        \"2021_2024\": [],\n",
    "    }\n",
    "    user_words = {\n",
    "        \"before_2016\": defaultdict(set),\n",
    "        \"2017_2020\": defaultdict(set),\n",
    "        \"2021_2024\": defaultdict(set),\n",
    "    }\n",
    "    user_comments = {\n",
    "        \"before_2016\": defaultdict(list),\n",
    "        \"2017_2020\": defaultdict(list),\n",
    "        \"2021_2024\": defaultdict(list),\n",
    "    }\n",
    "    counts = {period: 0 for period in chunks.keys()}\n",
    "\n",
    "    with open(path, \"rb\") as f:\n",
    "        jsonStream = getFileJsonStream(path, f)\n",
    "        if jsonStream is None:\n",
    "            print(f\"Skipping unknown file {path}\")\n",
    "            return\n",
    "        for row in tqdm(jsonStream, desc=\"Processing comments\"):\n",
    "            if \"body\" not in row or \"created_utc\" not in row or \"author\" not in row:\n",
    "                continue\n",
    "            author = row[\"author\"]\n",
    "            if author in {\"AutoModerator\", \"election_info_bot\"}:\n",
    "                continue\n",
    "            text = row[\"body\"]\n",
    "            created_timestamp = row[\"created_utc\"]\n",
    "            year = datetime.datetime.fromtimestamp(int(created_timestamp)).year\n",
    "            # if year <= 2016:\n",
    "            #     chunk_key = \"before_2016\"\n",
    "            # elif 2017 <= year <= 2020:\n",
    "            #     chunk_key = \"2017_2020\"\n",
    "            # elif 2021 <= year <= 2024:\n",
    "            #     chunk_key = \"2021_2024\"\n",
    "            # else:\n",
    "            #     continue\n",
    "            if 2021 <= year <= 2024:\n",
    "                chunk_key = \"2021_2024\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            txt = re.sub(r'http\\S+', '', text)\n",
    "            txt = re.sub(\"[^A-Za-z']+\", ' ', txt).lower()\n",
    "            txt = re.sub(r\"['\\-_]\", ' ', txt)\n",
    "            words = txt.split()\n",
    "            if not words:\n",
    "                continue\n",
    "            tagged_words = nltk.pos_tag(words)\n",
    "            processed_words = []\n",
    "            for word, tag in tagged_words:\n",
    "                if without_stopwords and word in stop_words:\n",
    "                    continue\n",
    "                if tag.startswith('J'):\n",
    "                    wordnet_pos = 'a'\n",
    "                elif tag.startswith('V'):\n",
    "                    wordnet_pos = 'v'\n",
    "                elif tag.startswith('N'):\n",
    "                    wordnet_pos = 'n'\n",
    "                elif tag.startswith('R'):\n",
    "                    wordnet_pos = 'r'\n",
    "                else:\n",
    "                    wordnet_pos = 'n'\n",
    "                lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "                processed_words.append(lemma)\n",
    "                user_words[chunk_key][lemma].add(author)\n",
    "            if processed_words:\n",
    "                user_comments[chunk_key][author].append(processed_words)\n",
    "                counts[chunk_key] += 1\n",
    "\n",
    "    print(\"\\n=== Comment Counts by Period ===\")\n",
    "    for period, count in counts.items():\n",
    "        print(f\"{period}: {count} comments\")\n",
    "\n",
    "    # Filter words by user count and rebuild comments for each period\n",
    "    for period in chunks.keys():\n",
    "        valid_words = {w for w, users in user_words[period].items() if len(users) >= 5}\n",
    "        filtered_comments = []\n",
    "        for comments in user_comments[period].values():\n",
    "            for comment in comments:\n",
    "                filtered = [w for w in comment if w in valid_words]\n",
    "                if filtered:\n",
    "                    filtered_comments.append(filtered)\n",
    "        print(f\"{period}: {len(filtered_comments)} comments after filtering words by user count\")\n",
    "        if filtered_comments:\n",
    "            print(f\"\\nExtracting bigrams for {period}...\")\n",
    "            phrases = Phrases(filtered_comments, \n",
    "                              min_count=phrases_min_count, \n",
    "                              threshold=100)\n",
    "            bigram_model = Phraser(phrases)\n",
    "            bigrammed_comments = [bigram_model[comment] for comment in filtered_comments]\n",
    "            chunks[period] = bigrammed_comments\n",
    "        else:\n",
    "            chunks[period] = []\n",
    "\n",
    "    # Train a Word2Vec model for each time period\n",
    "    for period, comments in chunks.items():\n",
    "        if len(comments) > 0:\n",
    "            print(f\"\\n=== Training Word2Vec for {period} ({len(comments)} comments) ===\")\n",
    "            model = Word2Vec(\n",
    "                vector_size=300,\n",
    "                window=5,\n",
    "                min_count=word2vec_min_count,\n",
    "                workers=16\n",
    "            )\n",
    "            model.build_vocab(comments)\n",
    "            print(f\"Vocabulary size: {len(model.wv.index_to_key)}\")\n",
    "            model.train(\n",
    "                comments,\n",
    "                total_examples=len(comments),\n",
    "                epochs=5\n",
    "            )\n",
    "            model_path = f\"models/reddit_word2vec_{phrases_min_count}_{word2vec_min_count}_filterd_{party}_{period}.model\"\n",
    "            model.save(model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "\n",
    "def main():\n",
    "    # build_word2vect_model(filePathforDemocrats, \"democrats\", without_stopwords=False, \n",
    "    #                       phrases_min_count=5, word2vec_min_count=5)\n",
    "    # build_word2vect_model(filePathforRepublican, \"republican\", without_stopwords=False, \n",
    "    #                       phrases_min_count=5, word2vec_min_count=5)\n",
    "    build_word2vect_model(filePathforDemocrats, \"democrats\", without_stopwords=False, \n",
    "                          phrases_min_count=10, word2vec_min_count=10)\n",
    "    # build_word2vect_model(filePathforRepublican, \"republican\", without_stopwords=False, \n",
    "    #                       phrases_min_count=10, word2vec_min_count=10)\n",
    "    # build_word2vect_model(filePathforDemocrats, \"democrats\", without_stopwords=False, \n",
    "    #                       phrases_min_count=20, word2vec_min_count=20)\n",
    "    # build_word2vect_model(filePathforRepublican, \"republican\", without_stopwords=False, \n",
    "    #                       phrases_min_count=20, word2vec_min_count=20)\n",
    "    # build_word2vect_model(filePathforDemocrats, \"democrats\", without_stopwords=False, \n",
    "    #                       phrases_min_count=50, word2vec_min_count=50)\n",
    "    # build_word2vect_model(filePathforRepublican, \"republican\", without_stopwords=False, \n",
    "    #                       phrases_min_count=50, word2vec_min_count=50)\n",
    "    print(\"Done :>\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
